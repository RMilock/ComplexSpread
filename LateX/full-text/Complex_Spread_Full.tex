\documentclass[a4paper,10pt,twoside]{book} %%{book}

\usepackage[top=5cm,bottom=5cm,inner=5cm,outer=3cm]{geometry}
\usepackage{graphicx, caption, subcaption} %per poter inserire le figure
\usepackage{csquotes} %handle " and ""
\MakeOuterQuote{"}  %handle opening/closing quotation
\usepackage{amssymb,amsmath,amsthm,amsfonts,bm, yhmath}
\usepackage{bookmark}% http://ctan.org/pkg/bookmark
\usepackage{appendix}
\usepackage{xspace}
\usepackage{tabularx}
\usepackage{indentfirst}
%\usepackage{subfigure}
%\usepackage[small]{caption}
\usepackage{eucal}
\usepackage{eso-pic}
\usepackage{url}
\usepackage{booktabs}
\usepackage{afterpage}
\usepackage{parskip}
\usepackage{listings}
\usepackage{fancyhdr}
\usepackage{textcomp}
\usepackage{cite}
\usepackage{multirow}
%\usepackage[utf8]{inputenc}   %per riuscire a scrivere gli accenti, but w/ this an error with csquote is raised. So, use \usep[english]{babel} only.
\usepackage[english]{babel}   %per riuscire a scrivere gli accenti
\usepackage{setspace}
\usepackage{etoc}
\usepackage{etoolbox}
\usepackage{setspace}
\usepackage{verbatim}
\usepackage{color}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{tocbibind}
\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{braket}
\usepackage{multirow}
\usepackage{float}
\usepackage{units}
\usepackage{siunitx}
\usepackage{bigints}

\usepackage{pgfplots} % to make plot inside LateX
\pgfplotsset{compat = 1.16}

\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{mathrsfs}
\usepackage{bbold}
\usepackage{mathtools}
\usepackage[style=ddmmyyyy]{datetime2}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\pagestyle{fancy} 

\graphicspath{{../images/}}

\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\Huge}

\definecolor{brown}{rgb}{0.28, 0.02, 0.03}
\hypersetup{
colorlinks,
citecolor=blue,
filecolor=blue,
linkcolor=blue,
urlcolor=blue
}

%\makeatletter
%\renewcommand\tableofcontents{%
%    \section*{\Huge{\contentsname}}%
%    \@starttoc{toc}%
%}
%\makeatother

%\NewDocumentCommand{\evalat}{sO{\big}mm}{%
%  \IfBooleanTF{#1}
%   {\mleft. #3 \mright|_{#4}}
%   {#3#2|_{#4}}%
%}

\begin{document}
\frontmatter


\newgeometry{top=2cm,bottom=2cm,left=2cm,right=2cm}
%%old for {book}: \frontmatter
%below: textsc = font "small caps"
\begin{titlepage}
\vspace{5mm}
\begin{figure}[hbtp]
\centering
\includegraphics[scale=.13]{../images/unipd_logo.png}
\end{figure}
\vspace{5mm}
\begin{center}
{{\huge{\textbf{\LARGE UNIVERSIT\`A DEGLI STUDI DI PADOVA}}}\\}
\vspace{20mm}
{\Large{\bf Dipartimento di Fisica e Astronomia "Galileo Galilei"}} \\
\vspace{5mm}
{\Large{\textsc{\bf Master Degree in Physics (LM-17)}}}\\  
\vspace{20mm}
{\Large{\textsc{\bf Final Dissertation}}}\\
\vspace{30mm}
{\Large{\textsc{\bf Modelling COVID-19 In a Network}}}\\
\vspace{45mm}
\end{center}

\begin{spacing}{2}
\begin{tabular}{lccccccccccl}
	{\Large{\bf Candidate}} &&&&&&&&&& {\Large{\bf Thesis supervisor}}\\
	{\Large{\bf Riccardo Milocco}} &&&&&&&&&& {\Large{\bf Prof./Dr. Marco Baiesi}}\\
\end{tabular}
\end{spacing}
\vspace{15 mm}

\begin{center}
{\Large{\bf Academic Year 2020/2021}}
\end{center}
\end{titlepage}

\restoregeometry

\clearpage{\pagestyle{empty}\cleardoublepage}

\pagestyle{empty}

\vspace*{\fill}
\tableofcontents
\vspace*{\fill}

	

%-----------------------------------------------------------------------

%\newgeometry{top=4cm,left=4cm}
\newgeometry{left = 4cm, bottom = 2cm, right = 2cm, top=4cm}
\chapter*{Abstract}
The usual simplified description of epidemic dynamics predicts an exponential growth. This is due to the mean field character of the dynamical equations. However, a recent paper (Thurner S, Klimek P and Hanel R 2020 Proc. Nat. Acad. Sci. 117, 22684) \cite{Thurner::NetBasedExpl} showed
that in a network with fixed connectivity, the nodes become infected at a rate that increases linearly rather than exponentially.
Experimental data for COVID-19 seem to validate this approach. In this thesis we plan to study this model by tuning its parameters.
In particular, we monitor the effect induced by a significant presence of hubs in the network.

\chapter*{Motivations}
Last Update: \today
"The COVID-19 pandemics has led to a dramatic loss of human life worldwide and presents an unprecedented challenge to public health, food systems and the world of work"\cite{Chriscaden::2021_ImpactCOVID19}. In this scenario, monitoring and forecasting the diffusion of the virus is an essential tool for policymakers to handle the health-care resources and restrictions among individuals. To this purpose, many academics, e.g. \href{https://web.unipd.it/covid19/en/}{UniPD against COVID-19}\footnote{https://web.unipd.it/covid19/en/}, have been trying to tackle the COVID-19 spreading as well as its side-effects, which have to be considered on the "path to normality". This drastic shift with respect to the "path to herd immunity" comes from a recent study \cite{GU::2021_SitePathToNormality} which shed light on the fact that "herd immunity", due to delay on vaccinations, could probably not be reached for the end of 2021. On the other hand, due to few practical points \cite{Nature:18.3.2021_NoHerdImmunity}, it is also probabile that the long-term prospects include COVID-19 becoming an endemic disease, much like influenza. 
Therefore, the recovery of the pre-COVID-19 situation could be slow even in the presence of effective vaccines.

So, an interesting path to cover is to describe the evolution of the COVID-19 pandemic, e.g. with the SIR model \cite{pizzuti::2020_ItalyCOVIDnetwork}, on different network topologies in order to simulate the different containment policies, i.e. "lock-downs".

%%old for {book}: 
\mainmatter

\newcommand{\changefont}{%
    \fontsize{12}{12}
	%\selectfont
}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{\changefont \slshape \nouppercase{\rightmark}} %section
\fancyhead[RE,LO]{\changefont \slshape \nouppercase{\leftmark}} %chapter
\fancyfoot[C]{\thepage}

\chapter[Introduction]{Introduction}
%\include{cap_Introduction}

\section{Complexity}
Since its beginning, Physics has been driven by a "Unification Principle"\footnote{from Newton with the "gravity"-relationship between an apple and the Moon; passing to Einstein, whom the effects of gravity are equal to the ones of an accelerated rocket for; ending to the \textit{Grand Unified Theory} as an unification of of general relativity with the particle physics},which aims at describing the reality as a "reduction" and simplification of its constituents and interactions. Nevertheless, this optimistic gaze, is (amazingly) challenged by the complexity that emerges in the world we are living in. This complexity could have broader origins but also different spatial scales: from ants' nest to storms, from the markets to human/animal migrations, from the epidemics to ecosystems. Therefore, Nature is capable of obeying simple laws but, on the other hand, to straightforwardly generate complex phenomena. A paradigmatic example is the flocking of birds: the core of the flock assumes different shapes due to a collective interaction among the birds; while an alone bird would proceed in a defined direction.
These facts were enough to establish the new branch of \textit{Physics Of the Complex Systems} which ultimate goal is to describe the emergence of the "complex" phenomena from the simple law of physics, e.g. "gravity model" \cite{GravityModelsandEmpiricalTrade}.
Having understood that \textit{"More is different"} \cite{Anderson:1972_MoreIsDifferent} and its ubiquity, it is natural to demand a formal definition of "Complexity". Nevertheless, quoting the Nobel Price Murray Gell-Mann \cite{Gell-Man:1987_S&C}, "a variety of different measures would be required to capture all our intuitive ideas about what is meant by complexity and by its opposite, simplicity" . In fact, from computational complexity to other information measures, all such quantities are context-dependent and subjective, since they depend on many aspects, such as the detail level (coarse graining) of the entity to describe. Thus, a rough but clear definition could be drawn for the Complex System Society site \cite{CSS:2021_compsystdef}:

\begin{definition}[\textit{"Complexity"}]
Complex systems are systems where the collective behavior of their parts entails emergence of properties that can hardly, if not at all, be inferred from properties of the parts.
\end{definition}

In other words, the non-linear interactions among the parts of which a complex system is composed let emerge a behavior which could not be understood simply by applying the fundamental laws to its constituents.

The first representation of this weaved system is by considering entities and connections among them. Thus, a powerful tool to describe it is the Network Science.
In particular, the purpose of this thesis is to study the epidemic spreading of the COVID-19 over different topologies of the underlying social network of individuals. This phenomenon is regarded as a complex system through the analysis of the SIR model, which grasps the emergent features rather than the microscopic behavior of the disease.

\subsection{Models Ecosystem}
Infectious epidemics are responsible for a significant health and economic burden on society. New diseases appear frequently and old diseases persist. Therefore, many mathematical models have been used recently to guide policy makers to mitigate the impact of new infectious diseases (e.g. Polio, H1N1 influenza and Ebola) or established ones (such as HIV, cholera and seasonal influenza).
On the other hand, with the phrase \textit{All models are wrong; some models are useful} \cite{Box::2005_StatDesign}, George Box wanted to highlight that no model, no matter how complicated\footnote{Instead, the complexity of a model could challenge its possibility to be predictive in many scenarios. A related topic is the "over-fitted model" in statistical learning or "Occam's razor" in Philosophy}, is perfect. Thus, a way to evaluate a "successful" model has to be recovered.

Inside the "ecosystems" of models, there are the simple models which allow to analytically explain how the primary mechanisms influence important characteristics (e.g. epidemic threshold, epidemic size\footnote{how large it will be}, how long it will last). To be more accurate, it is also possible to develop models that incorporate much more details about both the disease and individual-level interactions \cite{Kiss::MathOfEpiOnNet}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width = .4\linewidth]{perfect_model.png}
	\caption{A caricature of the relation between model realism, complexity and the
	insight the model provides \cite{Kiss::MathOfEpiOnNet}.}
	\label{fig:perfect_model}
\end{figure}	

As suggested by the \autoref{fig:perfect_model}\footnote{\textit{"Est modus in rebus"} \label{cit:GM}} (\cite{Kiss::Ch1MathOfEpiOnNet}), models provide maximum insight where the right balance of realism (circle filled with dots, e.g. regarding epidemiology Graph-Neural-Network to predict $R_t$ \cite{Davahli::USA_predicting_COVID19}) and simplicity (circle with vertical lines e.g. on epidemics the deterministic SIR model ) is met\footnote{The equilibrium point may depend on the specific posed question} (central circle with oblique lines).

In this middle region, there could be found several works that use network analysis as the underlying structure for a phenomenon(\cite{Thurner::NetBasedExpl} \cite{VespignaniSatorras2001Epidemic} \cite{pizzuti::2020_ItalyCOVIDnetwork}); while other, more involved, that incorporate human mobility data in several hybrid models \cite{ZEROUAL::DL_COVID19, Stubinger::Incidence_Diff_Countries} in order to predict its diffusion. For the sake of simplicity, the present thesis is going to deepen the paper of \cite{Thurner::NetBasedExpl}. In particular, Thurner et al. \cite{Thurner::NetBasedExpl}, by focusing on the COVID-19 spreading in Austria and in the USA, are able to find a scenario, i.e. a combination of the SIR spreading parameters\footnote{the infection rate $\beta$, recover rate $\mu$, the long-range attachment probability $p$}, such that the outbreak is (quasi-)linear for a value of the average number of contact $D$. This behavior of the SIR model is qualitatively comparable with the curves reported by the JHUniversity reported below. 
So, inspired by \cite{Thurner::NetBasedExpl}, the aim of this project is to analyze how the SIR model behaves by changing its "inner" epidemic parameters but also the topology of the underlying social network, i.e. the susceptible people.

\section{A Technical Introduction}
\label{sec:ATechIntro}
At $8 \, \textnormal{May} \, 2020$, none of the affected COVID-19 states have reached "herd immunity" but still they reached the "epidemic peak" due to the containment restrictions on social contacts. One paradigmatic example is the case of Austria, for which the total infected cases at the first peak were $0.16\%$ of the total population, remarkably low with respect to the SARS-COV-2 "herd-immunity" level that are supposed to be at $0.8\%$ (\cite{Zingano:2021_HI_hom_pop}) of the entire (susceptible) population. A graphical representation of the cases could be found in \autoref{fig:USA-AUT-ITAtotalcasesOWID}, \autoref{fig:USA-AUT-ITAtotalcasesOWID} produced by \href{https://ourworldindata.org/coronavirus/country/austria}{Austria New Daily Infected}.
\begin{figure}[tbp]
	\centering
	\includegraphics[width = \textwidth]{Introduction/COVID-RealStates.png}
	\caption{Normalized number of total cases until 8.5.2020. In the legend, there are reported the percentage of "$max / totp$" where "$max$" is the total cases at the end, while "$totp$" is the total population for each state according to \cite{PopulationEstimate}}
	\label{fig:USA-AUT-ITAtotalcasesOWID}
\end{figure}
The most striking observation is that the number of total cases (\autoref{fig:USA-AUT-ITAtotalcasesOWID}), after early-stages when Non-Pharmaceutical-Interventions (NPIs), i.e. lockdown, were at work, exhibits a (quasi-)linear growth for an extended time interval in contrast with the "S-shaped" logistic curve predicted by the standard compartmental models. The extension of the linear regime depends on the onset of the measures; while for early stages, as it was the case for many countries \cite{Thurner::NetBasedExpl} ($8 \, \textnormal{May} \, 2020$), an exponential growth dominates the spreading of the disease.
By taking care of the modification of the underlying social network structure, in this present thesis, we want to (qualitatively) recover the spreading trends for the states that have or have not applied the NPIs measures. \newline
Specifically, an infection may occur for two reasons:
\begin{enumerate}
    \item interaction between an infected and a susceptible person;
    \item contact is "intense" (e.g. long, close,...) enough to lead to a disease transmission.
\end{enumerate}
So, the rationale behind the social distancing is that it takes to a reduction of both of these factors.
On the other hand, the analytically solvable SIR model assumes that, defined $N, D$ as the total number of individuals in the population and the average number of contacts respectively, there is the same probability that an individual encounters an infected person ("well-mixed population") and all the nodes have the same number of neighbors ($N-1$ or $D$). This approach allow to recover analytical results for an epidemic spreading but it annihilates the underlying network structure. Therefore, as claimed in \cite{VespignaniSatorras2001Epidemic}, there is the need of studying how the network affects the spreading of a disease, but still no focus is put on the spreading below the epidemic threshold \cite{Thurner::NetBasedExpl} as it is the case if nodes were separated through NPIs. 
In this thesis, the main goal is to grasp the relevant features of a complex social network in presence of NPIs and compare them with respect to a "well-mixed" population model.
In detail, by changing the epidemic parameters, we let the SIR Model evolve in different network topologies such as standard models of the graph-theory ("Erdös-Rényi","Scale-Free", "Caveman Model") but also a new involved topology, i.e. a poissonian Small-World Network. As a benchmark of a "well-mixed" population, it has been used an "annealed" mean-field (see ??) which is a natural realization of a mean-field network with a fixed average number of contacts per node.
Furthermore, $R_0 := \beta \cdot D/\mu$ is heuristically defined as the the average number of new infections caused by an individual in a completely susceptible population\footnote{The susceptible individuals are chosen according to "epidemic targets", e.g. only children for the measles} \cite{Kiss::MathOfEpiOnNet}. Moreover, it is well consolidated that for $R_0 > 1$ an epidemic is possible (not guaranteed); while for $R_0 < 1$ the disease will die out. 
Thus, these informations drive naturally to suppose that $R_0$ could also be a signal of the strength of an epidemic outbreak. Instead, this is not the case, since, as reported in the section \autoref{ch:Results}, for equal $R_0$ values there could be different behavior of a disease. Thus, a new parameter has to be introduced that has to:
\begin{itemize}
	\item grasp the different behavior of epidemics;
	\item depend only on the initial condition, i.e. network metrics (e.g. "long-range" parameter $p$) and disease parameters ($\beta \textnormal{ and } \mu$).
\end{itemize}
In particular, $R_0$ should depend on the underlying topology, but it worths anticipating that the considered epidemics model are not designed for this scope: both the homogeneous and the heterogeneous models are averaging the network properties \autoref{ch:sir-models}. 

Finally, as in \cite{Thurner::NetBasedExpl}, a "epidemic" order parameter\footnote{Precisely (see \autoref{sec:OrderParam4LinContagion}), it is the standard deviation of the daily new infected} is computed for each spread to classify a light outbreak, i.e. an outbreak which infect less than the $30\%$ of the entire population, with a stronger one. In this way, a first-order phase transition could be obtained by plotting, for sufficiently high $p$\footnote{$p$ is the long-range rewiring probability.} the order parameter against the average degree of contacts.

On the top of the presentation of the work developed, it seems natural to deepen the area of the Network Science.


\section{Network Science}

Two aspects contribute to the popularity of Network Science: its universality nature of networks and the availability of large datasets\footnote{and the computational power to analyze them} which map huge networks of the furthest kind, e.g. "food-webs" or the Science of Science.
Its roots could be found in the Graph Theory, devoted to the analysis of the networks (or graphs), and Statistical Mechanics, committed to the formalization of the dynamical processes on network, such as self-organization and information theory.
For our purpose, the Graph Theory allows to formalize a "networky" - complex system such as the social network of the individuals over which a disease spreads; while Statistical Mechanics to develop a model focusing on its emergent characteristic.

\subsection{Graph Theory}
\label{sec:GraphTheory}
\begin{figure}[htbp]
	\centering
	\includegraphics[scale = 0.7, trim = {3cm 2cm 4cm 5cm}, clip]{Basic_SN}
	\caption{Contacts interaction Network. A contact is defined as the droplets that reach one individual. Therefore, a symmetric edge could represent a chat among two persons; where directed edges would be a "one-way" interaction such as in a room with pre-existent droplets. The number above each edge are the weights, which could be the minutes spent in the previous quoted room}
	\label{fig:basicSN}
\end{figure}

A graph is a tuple (V,E) where the "V" stands for "vertexes" (1,2,3,4) while "E" for "edges" (the links between the vertexes).
With the same notation, an edge connecting the node\footnote{In the context of a social network, a vertex represents a person/individual} $i$ to $j$ is, often, identified by $(i,j)$.
Formally, the labels of the vertexes are the image of any bijective embedding from the nodes to a chosen set, e.g. integer numbers. The relationship among vertexes are accounted by the edges, which could be unsymmetrical, as for $(1,4)$, and with different strength of the connection, such as $(1,4) \text{ and } (1,3)$ which. If all the edges are symmetric, i.e. if (A,B) implies (B,A), the graph is said undirected, e.g. the friendship network; otherwise it is said directed, e.g. the phone contacts network. 
With the given definitions, it is now clear that roughly everything could be divided in vertexes connected by am arbitrary relation.
The simplest example, is the \textit{simple graph}, which is an unweighted, undirected graph containing no graph loops, i.e. $(i,i)$ edges, or multiple edges such as multiples-$(i,j)$.
\begin{figure}[ht]
	\includegraphics[width = \linewidth]{SimpleGraph_950.png}
	\caption{Edges peculiarities. The labels are $1,2,3,4$ assigned anti-clockwise starting from the bottom}
	\label{fig:simple}
\end{figure}
\newpage

\subsection{Adjacency Matrix}
Typically for real networks, the number of involved nodes is big, e.g. the Google web-graph available at \href{https://snap.stanford.edu/data/#socnets}{Standford University Network Dataset} is of the order of the $800k$ nodes and $5milion$ edges. Thus, despite the clearness of \autoref{fig:basicSN}, the most suitable way of representing a graph is by grouping all its properties in a matrix called the adjacency matrix $A$. 
Starting from the basics, the (symmetric) adjacency matrix for the simple graph above is:
\[
A_{ij} :=
\begin{cases}
1 & \text{if exist an unweighted edge among i and j} \\
0 & \text{otherwise}
\end{cases}
\quad
\Leftrightarrow
\quad
A = 
\begin{bmatrix}
0 & 0 & 1 & 0 \\
0 & 0 & 1 & 1 \\
1 & 1 & 0 & 1 \\
0 & 1 & 1 & 0 \\
\end{bmatrix}
\]

Hereby, $A \in M_4(\mathbb{N})$ holds the nature of a simple graph:
\begin{itemize}[noitemsep]
	\item the rows represent the direct connections between the $i-th$ row and the $j-th$ column; while, the columns the edge $(j,i)$. Thus, there are only 4 nodes;
	\item unweighted: the value of the elements of $A$ is binar;
	\item without loops: the diagonal is $0$;
	\item without multi-edges: the value of the entries of $A$ are strictly $0$ or $1$, account for the presence of, at maximum, one edge.
\end{itemize}

A more involved, adjacency matrix is the one of the rightest graph which present multiples loops\footnote{As far as multi-edges are concerned, the $2$ would be off-diagonal}.
Formally,
\[
A = 
\begin{bmatrix}
0 & 0 & 1 & 0 \\
0 & 0 & 1 & 1 \\
1 & 1 & 0 & 1 \\
0 & 1 & 1 & \textbf{2} \\
\end{bmatrix}
\]

Thus, in \autoref{fig:basicSN} is report a (simple) weighted and directed graph with 4 nodes connected by a "central"\footnote{The "centrality" measure of the node is another important quantity which is context-dependent. In this case, the "centrality" refers to a degree centrality for which the most central node is the one with the highest number of contacts} node $1$.
In particular,
\[
A = 
\begin{bmatrix}
0 & 1 & \textbf{5} & 1 \\
1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\ 
\end{bmatrix}
\]
where it is encoded the direct edges since $A$ is not symmetric.

\subsection{Degree}

\begin{figure}[ht]
    \begin{subfigure}{\textwidth}
        \includegraphics[scale = 0.4]{../images/Networks/DegreeClustCoeffPathLeng}
        \centering
        \caption{Network measures \cite{Olaf:2011_NonRandomBrain}}
        \label{fig:degree_clustcoefficient_pathlength}
    \end{subfigure}
\end{figure}


As quoted before, a (real) network could be very tangled. Thus, the degree distribution, i.e. the normalized number of vertexes with a certain degree, allows to check whether all the nodes have, or not, the same number of contacts even at a plain eye inspection. 

Formally, the degree of a vertex, in a (undirected) graph, is the number of edges connected to it. We denote the degree of node $i$ by $k_i$ and, by mean of $A$, $$k_i := \sum_{i=1}^{N} A_{ij},$$ where $N$ is the number of total nodes.
For directed graphs, as in \autoref{fig:basicSN}, a distinction between the entering and exiting edges to a node has to be defined. In particular, by a generalization of the previous $k_i$, 
\begin{equation}
	k_i^{in} := \sum_{j=1}^N A_{ij}, \quad k_j^{out} := \sum_{i=1}^N A_{ij}.
	\label{eq:kin_kout}	
\end{equation}
where $k_i^{in(out)}$ is the number of in-going (out-going) number of edges. 

\begin{figure}[t]
\begin{subfigure}{.5\linewidth}
	\begin{tikzpicture}
	\begin{axis}[ybar interval, 
		width = \linewidth,
		xtick align=inside,
		ymin = 0,%, ymax=55,ymin=0, minor y tick num = 3]
		ylabel = {Number of Nodes},
		xlabel = {In-degrees},]
	\addplot coordinates { (0, 0) (1, 3) (2, 0) (3, 0) (4, 0) (5, 1) (6, 0) };
	\end{axis}
	\end{tikzpicture}
\end{subfigure}%
\hfill
\begin{subfigure}{.5\linewidth}
	\begin{tikzpicture}
	\begin{axis}[ybar interval, 
		width = \linewidth,
		xtick align=inside,
		ymin = 0,%, ymax=55,ymin=0, minor y tick num = 3]
		ylabel = {Number of Nodes},
		xlabel = {Out-degrees},]
	\addplot coordinates { (0, 0) (1, 1) (2, 0) (3, 0) (4, 0) (5, 0) (6, 0) (7,1) (8,0) };
	\end{axis}
	\end{tikzpicture}
\end{subfigure}
\caption{Degree Distribution of the \autoref{fig:basicSN}}
\label{fig:degreed_basicSN}
\end{figure}

Therefore, by applying the \autoref{eq:kin_kout}\footnote{A practical check is to count the tails for the out-degree and arrows for the inner-degree. Ultimately, each sum has to equate the number of edges (a undirected edge has to be count doubled)} to \autoref{fig:basicSN}, it is possible to recover the degree distribution of the vertexes as in the \autoref{fig:degreed_basicSN}. Thus, a possibile way to build a graph would be to draw the nodes degrees from a fixed distribution and, then, connecting the nodes according to their degree. In fact, this approach is going to be exploited in the further chapters by using a poissonian degree distribution and connecting the vertexes only to nearest neighbors via an ad-hoc algorithm.

\section{Chapters Outline}
In \autoref{ch:network-models}, the models used in this thesis are going to be exposed: starting from a "regular" Watts-Strogatz and Caveman models, passing to the random ones (Erdös-Rényi, Watts-Strogatz, Poissonian-Small-World models); then, ending with the scale-free ones, such as the Barabási-Albert model. 

In \autoref{ch:sir-models}, two kind of SIR models are presented: the mean-field SIR model and the network one, which combine the mean-field SIR with the network structure.

\chapter{The Network Models}
\label{ch:network-models}
As it could be perceived, modeling a complex (social) phenomenon\footnote{The "social" nature has been only evoked for concreteness. Nevertheless, this in-depth presentation of models is interdisciplinary as Network Science itself.}, as the epidemic spreading, in a network has two main tasks. The first is the identification of the nodes, which is a fairly simple task. The second is to pinpoint the emergent correlations in order to connect the vertexes and reproduce the phenomenon. The latter one is the real challenge in network theory.

%Thus, the goal is to artificially reproduce the main topological features of the real networks. 
Many networks are characterized by strong constraints on the node degree which forces a satisfying regularity to be present, e.g. the radial architecture of a spider net or the map of the main streets of Manhattan.
Inspired by this, a first insight, the pathogen is left to act on a regular society, which individual contact are differing at most by one, e.g. "regular" Watts-Strogatz and Caveman model.

Most of the real networks, however, do not show a comforting orderliness. 
%add an image of spider net, Manhattan and Internet Network
Rather, their inner structure seems to be characterized by a certain degree of randomness, e.g. the internet network \cite{barabasi::2016networkbook}. Random network theory embraces this feature by constructing network models that are truly random, e.g. Erdös-Rényi. Meanwhile, they are not able to capture the presence of hubs, alias the "Scale-Free" property. Thus, new models as to be proposed to generate those highly connected degree nodes, such as the Watts-Strogatz and Barabási-Albert models.
A review of these is reported in the following sections; alongside with the newly proposed "Poissonian Small World" network (see \autoref{sec:ATechIntro}, \cite{Thurner::NetBasedExpl}).

\section{Regular Networks}
\label{sec:RLN-Caveman_Description}
\subsection{Lattice Graph}
\begin{figure}[ht]
    \begin{subfigure}{\textwidth}
        \includegraphics[scale = 0.4]{../images/Networks/RegGraphs}
        \centering
        \caption{In (a) - fully-connected graph; In (b) - $D = 4$ regular graph \cite{Zelazo:2011_RSensNet_images}}
        \label{fig:RegGraph}
    \end{subfigure}
\end{figure}

As briefly reported, the "Regular Networks" are characterized by a fixed number of neighbors per node $D = \langle k \rangle$. Thus, they are expected as the constituent graphs for the class of systems which constraints $D$ to a certain value, e.g. solids in the physics of matter.
Ultimately, by trying to study a new phenomenon, it is fundamental to test its behavior on a controlled environment as the one provided by the regular networks.

In \autoref{sec:WS_Model}, there will be presented the "Watts-Strogatz" model which depending by an inner parameter $p$ can interpolate between a regular graph for $p = 0$(\autoref{fig:RegGraph}) and a random network ($p=1$).
So, setting $N$ as the number of nodes, chosen $p=0$ and starting from a fully-connected network ( (a) figure in \autoref{fig:RegGraph}), the average degree $D = N-1$ is halved at each time-step. Thus, to preserve the secondary cases $R_0$, the infectivity of the pathogen $\beta$ is doubled at each time-step.

\subsection{Connected Caveman Graph}
\begin{figure}[ht]
    \begin{subfigure}{\textwidth}
        \includegraphics[trim={2cm 3cm 2cm 4cm}, clip, scale = 0.6]{../images/Networks/CavemanMod_Rew}
        \centering
        \caption{Disconnected and Connected with rewiring Caveman Model \cite{Taube:2005_IndianSoftwIndustry}}
        \label{fig:CavemanMod}
    \end{subfigure}
\end{figure}


By analogy with the NPIs measures which allowed only for the household interactions, it has been considered the "Caveman Model".

The rationale is to consider as if one node, of the previously modeled networks, would be a family composed by a fixed number of people. This idea drives to the "coarse-graining" approach seen in the previous sections.
In detail, the Caveman model enables to fix both the number of "caves"/families and the number of individual for each family. 

In the present thesis, it is going to be developed an intermediate version among the Caveman models shown in \autoref{fig:CavemanMod}. Indeed, connect the nearest caves of the left figure by creating a new edge on randomly choosing nodes within each cave. Then, introduce a "rewiring parameter" to enable "long-range" connections, resulting in "tunneling cuts" among distant nodes as in the right rewired graph. The ending graph is comparable to a Watts-Strogatz model, whose nodes are substituted with an entire cave.

A naive expectation is that the epidemic spread slower and more stochastically, since the nodes are poorly connected with the "outside" rather than in the family.

\section{Random Networks}
Not all the networks are characterized by hubs as WWW or the airports graphs.
In fact, there could be constraints, such as cost-benefit problem, which limit the maximum degree of the bigger node. In those cases, random networks may be the best (thus, not conclusive) fit, e.g. national highways network, power grid of generators and switches, $\cdots$

\subsection{The Erdös-Rényi Model}
%insert a reference image for ER model

\begin{figure}[ht]
    \begin{subfigure}{\textwidth}
        \includegraphics[trim={14cm 0 0 1cm}, clip, scale = 0.6]{../images/Networks/ERmodel}
        \centering
        \caption{Erdös-Rényi Model for $p = 0.2$ \cite{Baronchelli:2017_EpidSpreadCompNets}}
        \label{fig:ERmodel}
    \end{subfigure}
\end{figure}

The Erdös-Rényi (ER - \autoref{fig:ERmodel}) model was the first attempt to formally describe a random network. 
In particular, these two mathematicians were the fathers of the random graph theory, obtained by merging graph theory with probability theory.

The primitive hypothesis, for a random wiring, is that the nodes are linked together with a probability $p$. Formally,

\begin{definition}[\textit{"Random Network"}]
A random network $G(N,p)$ has N nodes where two nodes are connected with probability $p$.
\end{definition} 

Due to its random nature, this model may have different realizations even with $N$, $p$ fixed parameters. Therefore, the number of links and the nodes degree distribution (see \autoref{sec:GraphTheory}) becomes vital informations to be considered.
In particular, a specific realization could exhibit $L$ links with a probability
\begin{equation}
	\label{eq:probLrndNet}
	p_L = p^L \cdot (1-p)^{ \binom{N}{2} - L } \cdot \binom{\binom{N}{2}}{L}.
\end{equation}
The \autoref{eq:probLrndNet} is the product of $3$ terms: the probability to have $L$ edges; the probability not to have the remaining ones knowing that the total possible node pairs are \[ \binom{N}{2} = \frac{N(N-1)}{2}; \] the total combinations in the choice of the $L$ edges, since what counts is only to pick a number $L$ of edges and not which ones.
As expected, $p_L$ could be interpreted as $L$ successful results knowing that an edge is chosen with probability $p$ and the total trials are $\frac{N(N-1)}{2}$ pairs. Thus, a Binomial distribution with the average number of links equal to
\[ \langle L \rangle = \sum_{L = 0}^{\frac{N(N-1)}{2}} p_L L = p\frac{N(N-1)}{2} \label{eq:meanL} \] and, thus, the average per node contacts \[ \langle k\rangle = \frac{2\langle L \rangle}{N} = p(N-1) \label{eq:meank}. \]

The two equations above highlight that $\langle L \rangle$ and $\langle k\rangle$ could be obtained by multiplying $p$ by the total number of available links and neighbors resp.\footnote{This is accordance with the formal definition of the "average"}. Alongside, as the network becomes denser, i.e. increasing $p$, also $\langle L \rangle$ and $\langle k\rangle$ would be enhanced.
Moreover, by multiplying the expression by $q$, it is possible to obtain $\sigma_{\langle x\rangle} = q \, \cdot \langle x\rangle,$ where $x = L \textnormal{ or } k$.

Similarly, the probability of having k number of contacts is 
\begin{equation}
	\label{eq:probkrndNet}
	p_k = p^k \cdot (1-p)^{ \binom{N-1}{2} - k } \cdot \binom{\binom{N-1}{2}}{k}.
\end{equation}
where $N-1$ are the total neighboring nodes differently from $\binom{N}{2}$, which were the total node pairs (cf. \autoref{eq:probLrndNet}).
Another way to recover $\langle k\rangle$ and $\sigma_{k}$ is by means of $p_k$.

Real network are sparse. In fact, by defining the "sparsity" of a network as the ratio $s : = \langle k\rangle/N$, the internet-routers has $s \simeq 3.0 \cdot 10^{-5}$, since $N \simeq 200000 \text{ while } \langle k\rangle \simeq 6$. Many others concrete example support this sparsity nature of real networks \cite{barabasi::2016networkbook}. Therefore, in the $\langle k\rangle / N \ll 1$ limit\footnote{At least for $\langle k\rangle \simeq 50 \text{ and } N \simeq 10^3$}, the \autoref{eq:probkrndNet} becomes the poissonian distribution with average $\langle k\rangle$
\begin{equation}
	\label{eq:sparse_probk}
	p_k = e^{-\langle k\rangle} \frac{\langle k\rangle^k}{k!}.
\end{equation}

Therefore, keeping in mind that the \autoref{eq:sparse_probk} is only valid in the sparse regime,
\begin{itemize}
	\item the explicit expressions of $\langle k\rangle \simeq p\cdot N \textnormal{ and } \sigma_k = \langle k\rangle^{1/2}$ assume a simpler form for a poissonian distribution. Moreover, as the binomial case, $\langle k\rangle \textnormal{ and } \sigma_k$ increases if p increases, since the network is becoming denser and, so, more spread in the node degrees.
	\item it allows the same description for a pletora of networks with the same $\langle k\rangle$, since the network characteristics does not depend on its size $N$.
\end{itemize}

\subsection{The "Small-World" Property}
\label{sec:SWProp}
A captivating argument, network science allows to study is the "Small-World (SW) phenomenon" or "six degrees of separation". The two quoted names comes, respectively, from a theme developed by the Hungarian writer Fridgyes Karinthy in a play called "Chains" ($1978$) and an empirical experiment injected by the psychologist S. Milgram in $1967$. In fact, the most famous version states that a person, e.g. Kevin Bacon, is linked to a random one, e.g. Andrea Scotti, through only few acquaintances: according to \href{https://oracleofbacon.org/movielinks.php}{Kevin Bacon Number}\footnote{https://oracleofbacon.org/movielinks.php} only by $2$ persons in common. So, the distance between two randomly chosen nodes in a graph is small. Random networks account for this phenomenon as we are going to deepen.

On average, the expected number of nodes $N$ at a certain distance $d$ is
\[N(d) \approx 1+ \langle k \rangle + \cdots + \langle k \rangle^d = \frac{\langle k \rangle^{d+1} -1 }{\langle k \rangle - 1},  \] 
where it has been taken advantage of the geometric series, after noting that $``1"$ is the selected nodes, $\langle k \rangle$ are its neighbors, $\langle k \rangle^2 \approx \langle k \rangle (\langle k \rangle - 1)$ the next neighbors and so on.
Thus, by considering that for a real network $\langle k \rangle \ll 1$ and that $N(d_{max}) \sim N$, 
\[
	d_{max} \sim \frac{\ln(N)}{\ln(\langle k \rangle)}
\]
On the other hand, the above equation is more suitable in describing $\langle k \rangle$, since $d_{max}$ is dominated by few distant paths from the core giant component. Hence, 
\begin{equation}
	\langle d \rangle \sim \frac{\ln(N)}{\ln(\langle k \rangle)} 
	\label{eq:SWrandnet}
\end{equation} 
is the formulation of the Karinthy's "Small-World phenomenon" obtained by considering that most of the nodes are contained within $\langle d \rangle$.
Indeed, for a real network, $\ln(N) \ll N$ drives to a "smaller" average path length compared to $N$, which is the typical scaling for a regular lattice\footnote{The common notion of physical distance is based on a regular lattice which do not shows the "Small-World" effect. At first glance, this result could be perceived as an uncomfortable.}
Moreover, the denominator of the \autoref{eq:SWrandnet} implies that the denser it becomes the network, the closer the nodes aggregate.
As a sociological example, the average distance of a random person, in the current century, would be derived by applying the \autoref{eq:SWrandnet} by fixing $N \sim 7\cdot 10^9$ and $\langle k \rangle \sim 10^3$ yielding $\langle k \rangle \sim 3.28$ which is a more reliable results than the "six degrees of separation" \cite{barabasi::2016networkbook}.

\newpage
\subsection{The Clustering Coefficient}
As far as a social network is considered, it seems natural to introduce a measure of community membership (\autoref{fig:degree_clustcoefficient_pathlength}) which capture the possibility to join a local community, e.g. an association or a family. More generally, real networks are characterized by the clustering formation, which "simplify" the network by aggregating the nodes, called clusters, which present common characteristics. Proceeding as the "coarse graining" technique of Statistical Mechanics, it becomes appealing to take care of the emergent properties exhibited by the "coarse-grained" network. In the following section, it is introduced the degree of "clusterization" of a network.

More precisely, the local clustering coefficient $C_i$ measures the chance of having a link among the neighbors of the $i-th$ node. To gain a rough intuition of its purpose, it is worthwhile to consider two opposite limits: 
\begin{itemize}
	\item $C_i = 0$ as there are no links among neighbors of $i$;
	\item $C_i = 1$ as there are fully connected neighbors of $i$.
\end{itemize}

Formally,
\begin{equation}
	C_i := \frac{\textnormal{average links of the i-th neigbors}}{\textnormal{total links among neighbors}} = \frac{\langle L_i \rangle}{\frac{k_i(k_i-1)}{2}} = p = \frac{\langle k \rangle}{N} = \langle C \rangle
	\label{eq:rnd_clustcoefficient}
\end{equation}

where $k_i \textnormal{ and } N$ are the amount of neighboring vertexes of $i$ and the total nodes respectively, \[ \langle L_i \rangle = p \cdot \frac{k_i(k_i-1)}{2} \quad \textnormal{ and } \quad \langle C \rangle := \frac{1}{N} \sum_{i = 1}^{N} C_i. \]

The presence of a link among the two neighbors closes a triangle with the focal node $i$ ( "triadic closure"). Hence, $C_i$ could also be interpreted as the normalized number of triangles centered in the selected node. In addition, $C_i = \langle C \rangle$ is independent by the $i-th$ node degree and, fixing the average degree of a network, it scales with $1/N$: for real networks $p = \langle k \rangle / N$ is small (real networks are sparse) and drives to a smaller $C$ that the one observed. A model that naturally generates "triangle", would be a good candidate to enhance the clustering coefficient even for small $p$.

\subsection{Problems in Random Networks}

By looking at a real social network, e.g. at a cocktail party, there is the co-presence of highly connected nodes, called \textit{"hubs"}, alongside with the less interacting ones. The ER graph does not account for the presence of high-degree nodes\cite{barabasi::2016networkbook}, since they are strongly damped by the  factor term $1/k!$ in \autoref{eq:probkrndNet}. Indeed, real networks are sparse but have also strongly heterogeneous degrees, showing a high variance through node degrees.

Secondly, by looking at the evolution of the contact network, e.g. the one underlying the previous cocktail party, there could be identified 4 topological regimes starting from isolated individuals for $\langle k \rangle = 0$ and ending with a fully connected graph for $\langle k \rangle = N-1$, where $N$ are the total invited persons. The ER model predicts the co-existence of a giant component, containing loops and cycles, and small components, which forms trees, for $1 < \langle k\rangle < \ln(N)$ - find a better description below. However, this is clearly at odds with reality since, as a concrete example, in a random electric power-grid some consumers should not get electric power \cite{barabasi::2016networkbook}. Many network, as the actor network, are in the connected regime; most are in the supercritical one while still connected.
In particular, these regimes scan the formation path of a fully connected network, passing trough a rapid emergence of a giant component.
\newline Schematically,
\begin{itemize}[noitemsep]
	\item Sub-critical Regime for $0 < \langle k \rangle < 1 \quad (p < 1/N)$ 
	\item Critical point for $\langle k \rangle = 1 \quad (p = 1/N)$
	\item Supercritical Regime for $\langle k \rangle > 1 \quad (p > 1/N)$: Single giant component formation
	\item Connected Regime for $\langle k \rangle > \ln(N) \quad (p > \ln(N)/N)$
\end{itemize}
This is in an one-to-one relation with a First-Order-Phase Transition treated in Statistical Mechanics. The formation of an ordered giant component could be compared with ice formation: the passage from disordered water molecules to an ordered structure.

Thirdly, the real world networks exhibits a "Small-World" phenomenon not in agreement with the \autoref{eq:SWrandnet}. Hence, a new "Small-World" measure has to be proposed.

Fourthly, fixing $N$, the theoretical expectation is to grasp the independent nature of $C$ with respect to $k$. Nevertheless, the results is unsatisfactory, since $C$ decreases with the increasing of $k$ for a pletora of the real networks.
%put an image of this
Furthermore, $C/\langle k \rangle (N)$, for high $N$, is higher that its theoretical expectation. Thus, a refined model is needed, to explain the high $C$ even at small $p$ (or high $N$).

\subsection{Watts-Strogatz Model}
\label{sec:WS_Model}
%insert a reference image
\begin{figure}[ht]
    \begin{subfigure}{\textwidth}
        \includegraphics[scale = 0.4]{../images/Networks/WS_Model}
        \centering
        \caption{In the top panel, the changing topology from lattice to random-like network; In the bottom panel, the behavior of average path length and clustering coefficient with $p$ \cite{Olaf:2011_NonRandomBrain}}
        \label{fig:WSmodel}
    \end{subfigure}
\end{figure}

To face the problem of the "high $C$ at low edge probability (for now, redefined as $p_{ER}$)", while preserving the "Small-World" property, the mathematicians Duncan Watts and Steven Strogatz proposed in the $1998$ a hybrid model which interpolated between a regular lattice for $p = 0$ to a random network model for $p = 1$. The new borne model is, thus, called "Watts-Strogatz (WS) model".
Practically, the idea is to start with a grid-like network, where all the nodes have $\langle k \rangle$ neighbors; then, with probability $p$, a link is chosen and one of the endpoints, e.g. the "rightmost", is replaced with a random vertex within the graph. The underlying degree distribution, hence, narrows the nodes degrees since it passes from an one-block histogram centered in $\langle k \rangle$ to a poissonian distribution. Thus, nodes have barely different neighbors degree and no hubs are allowed.
A further constraint is that no multiple edges can connect the same pair of vertexes, i.e. no multi-edges. The presence of multiedges gives a correction of $1/N$ for big $N$, negligible in a real modeling scenario with big $N$.

The rationale of ER model is that by a little decrease of clustering (i.e. destroying triangles) corresponds a significant reduction in the average path length $\delta:= \langle d \rangle$. Other way around, by connecting nodes in a "long-range" fashion, the overall "triadic closures" are still relevant. 

It worths to highlight that the present $p$, called in this context the "rewiring parameter", is the "long-range" rewiring probability, while the ER model $p_{ER}$ is the probability of an edge formation either among neighbors ("short range" connection) or among distant nodes ("long range" connection). Thus, the "distant" connected vertexes are, on average, $p\cdot L$ where $L$ is the total number of links.
In particular, a regular lattice is characterized by a high clustering (neighboring nodes are fully connected) but a high average path length; while a random network, as seen before, exhibits manifestly the "Small-World" property alongside with a low clustering coefficient especially, since triangles are rare for small $p$.
Therefore, the parameter $p$ is introduce to fine tune the co-presence of the two kind of networks. As reported in \cite{Menczer:2020_1stCoursNetSci}, there is a range of rewiring probabilities $p \in (0.01,0.1)$ in which $\langle d \rangle_p \approx \langle d \rangle_{p=0} \textnormal{ and } C_p \approx C_{p=0}$ encoding the fact that the average path length and clustering coefficient of the WS model are, respectively, near the one of a random network and the one of a lattice.

Anyway, this model doesn't explain the presence of the hubs, since neglected by all the degree distributions, and the (inverse) dependence of $C(k)$ on the average network degree.

As a pedagogical spoiler, the (four) previous problems would be faced by modifying the WS model adding the emergent properties of the real networks, such as the "scale-free" property.
Hence, an useful approach would be to start from the degree distribution rather than on imposing a fixed probability of wiring as for ER and WS models.

\section{Configuration Model}
To face the problems previously exposed, a practical starting point could be to reproduce the degree distribution by taking advantage of the configuration model 
\cite{Menczer:2020_1stCoursNetSci}. Indeed, it is possible to built a network from an arbitrary degree sequence which either could be drawn by a distribution or by a real network of interest.
Precisely,
\begin{definition}[\textit{"Degree Sequence"}]
	The degree sequence is a list of $N$ numbers \newline $(k_0, \cdots, k_{N-1})$ where $k_i$ is the degree of node $i$.
\end{definition}
A degree sequence, thus, determines uniquely a degree distribution but the reverse is not true, since for a particular degree distribution there are $N!$ possibilities of labeling the nodes: $(1,2,3)$ has the same degree distribution of $(3,2,1)$.
Additionally, a method to wire the nodes has to be chosen. Especially, the configuration model wires the nodes randomly; thus, building a random network. For these reasons, the generated graph is also called "random network with a pre-defined degree sequence". 

As done before, a practical way of constructing a network clarifies many ideas.
As a first step, consider to have a set of nodes with an assigned degree sequence. Pictorially, this could be depicted as a node with a number of dangling stubs as its degree. Thus, the number of isolated stubs have to be twice the sum of all the degrees. 
The network is, further, composed by the following iterative steps:
\begin{itemize}
	\item A pair of stubs is selected at random;
	\item An edge is formed, by attaching together the selected stubs. 
\end{itemize}
This tying technique is replicated until all the stubs are saturated and it is random by construct. Therefore, this method generates random networks with a fixed degree distribution.

Since the number of stubs equals the node degree, each node ends up having the desired degree. Thus, from the selected distribution, multiple (random) networks could be created; even, showing some peculiar properties, such as loops or multi-edges. Therefore, further constraints has to be imposed to obtain a specific topology, such as a connection only to "short-range" nodes. 

The network is characterized by mainly two properties:
\begin{itemize}
	\item the probability that a fixed node $i$ may connect to another one $j$ is
		\begin{equation}
			p_{ij} = \frac{k_ik_j}{2L-1}.
		\end{equation}
		In fact, there are $k_i$ stubs (or rays) trying to match the $k_j$ ending of node $j$; while $2L -1$ are the overall rays available for the chosen one which produces the $"-1"$;
	\item the number of self-loops and multi-links decreases as $N$ increases, since it is less probable to connect with a specific node as $p \sim 1/N$. Typically it is not
	needed to exclude them \cite{Newman:2010_Net:AnIntro}. More importantly, arbitrarily rejecting self-loops or multi-links would result in a modification of the starting degree distribution, yielding the analytical calculations difficult. 
\end{itemize}

The different realizations of the same degree distribution have a statistically objective. Indeed, let's suppose a certain feature, such as the presence of hubs, is independent on the other network characteristics rather than the degree distribution. By exploiting an ensemble of graphs, it would be possible to obtain an average and a standard deviation of that feature. Thus, by comparison with the real value, evaluate the unique dependence on the degree distribution.

Another method to generate a network from a fixed degree distribution is the "hidden parameter ($\eta$) method" which, in addition, do not form multi-edges and loops \cite{barabasi::2016networkbook} and allows to tune the average degree $\langle k \rangle$ modifying $\eta$.
On the other hand, to recover a randomized version from the degree sequence, such as of a fixed real network, the "degree-preserving randomization" is the best help the theory provides. In this way, by just swapping two endpoint nodes at a time, it is possible to build an networks ensemble with the same degree distribution. So, check whether a quantity depends only on the degree distribution itself or has a more involved structure.
In the following sections, only the Configuration Model is going to be used.

\clearpage
\subsection{Poissonian Small-World Network}
\label{sec:PSW_network}
Social network are highly non trivial structure, by including a multilevel organization; weak ties between communities; and temporal aspects that suggest a degree of fluidity with stable social cores \cite{Thurner::NetBasedExpl}.
As seen in the \autoref{sec:ATechIntro}, however, the lockdown measures prevents the formation of highly connected nodes (hubs), expected in an unconstrained (social) scenario. Thus, a possible approach would be to draw a degree sequence from a poissonian distribution, regardless on the ratio $\langle k \rangle / N$ (differently from \autoref{sec:WS_Model}); and build a graph by connecting the neighboring nodes.
This involved structure, called "Poissonian Small-World (PSW) Network" (\cite{Thurner::NetBasedExpl}), enables to capture the heterogeneity in the number of contacts (node degree); the SW phenomenon and the clusterization of families alongside with their overlap. Moreover, the WS parameter $p$ represents the fraction of individuals that are connected at "long-range" through leisure activities ($p = 0$ these are prohibited).

In order to built a PSW network, the starting point is by using the configuration model with a degree sequence drawn from a poissonian distribution. To simulate a closed population, the nodes are assumed to be put on a circle: the last and the first of nodes of the degree sequence are assumed as neighbors. Hence, both the "SW" property (see "The emergence of the Small-World property" \autopageref{sec:SWProp}) and the heterogeneity on the node degrees could be embodied at the same time. 

With low values of the poissonian parameter $\mu$ is possible to take into consideration the lock-down scenario where the average per node degree of contacts is the household size $D \sim 2.5$ \cite{Thurner::NetBasedExpl}. The configuration model generates a random wiring of the links, producing "long-range" connections which spoil the locality of the community structures (families). Therefore, by an ad-hoc algorithm, the stubs are forced to attach to the nearest nodes in the circle according to their degree. In practice, the wiring algorithm iterates over few steps\footnote{Recall that the degree sequence is already fixed and drawn by a poissonian distribution}:
\begin{itemize}
	\item choose the lowest degree $k_l$ node that has not already been saturated;
	\item link an even number of times $\lfloor k_l /2 \rfloor$ on the clockwise and anti-clockwise nodes, where $\lfloor * \rfloor$ is the floor function of $*$;
	\item if the degree of the selected node is odd, wire one last time ($k_l - \lfloor k_l /2 \rfloor \, = k_l \bmod(2) =  1$), e.g. anti-clockwise;
	\item delete, among the available nodes, both the chosen node and the target nodes which have saturated their degree.
\end{itemize}

More practically, fixed $[(1, 0), (6, 1), (0, 2), (2, 2), (4, 2), (7, 2), (8, 2), (9, 3), (5, 3), (3, 4)]$, i.e. a degree sequence composed by (node,degree) tuples. 
The edges $(i,j)$ are created by connecting $i \textnormal{ to }j$ and diminishing the degree of $j$ at each step:
\begin{itemize}
	\item node $1$ is left alone as $deg(1)=0$;
	\item $(6,7)$;
	\item $(0,2)\textnormal{ and }(0,9)$
	\item $(2,0) \text{ (already present) and } (2,3)$;
	\item $(4,3)\textnormal{ and }(4,5)$;
	\item $(7,6)\textnormal{ (already present) and }(7,8)$; 
	\item $(8,7) \textnormal{ (already present) and } (8,9)$;
	\item $(9,3), (9,8) \textnormal{ (already present) and } (9,5)$ - "tunneling" nodes ;
	\item $(5,4), (5,9) \textnormal{ (already present) and } (5,3)$;
	\item node $3$ is left with $deg(3) = 1$, but this issue reduces with sparsity $\langle k \rangle \ll 1$.
\end{itemize}

Hence, the algorithm preserves the poissonian degree distribution, while enhancing the local clustering of the nodes. However, it is possible to have "long-range" connections ($(9,5)$), since the nearest available vertexes could be already saturated; therefore, resulting in a distant linking. This "tunneling" nodes are neither "super-spreaders" (or hubs of the infection), as they are forbidden by the poissonian degree distribution; nor "super-infectors", which would be characterized by an higher infectivity rate $\beta$.

\newpage
\section{Scale-Free Networks}
\subsection{Scale-Free Property}

\begin{figure}[h]
    \begin{subfigure}{\textwidth}
        \includegraphics[scale = 0.5]{../images/Networks/Scale-free-network-and-power-law-distribution-A-B-The-US-highway-system-has-a.png}
        \centering
        \caption{On the left: the comparison of a random network(top panel) and a scale-free one(bottom panel; On the right: their degree distributions on log-log scale) \cite{barabasi::2016networkbook}}
        \label{fig:PLDsVSEBDs}
    \end{subfigure}
\end{figure}

\begin{definition}[Scale-Free Network \cite{Barabasi:1999_ScalRndNet}]
	A scale-free network is a network whose degree distribution follows a power law.
\end{definition}

By plotting the degree distribution of the real networks, due to "hubs", a better approximation is obtained via a power-law distributions (PLDs) of the kind\footnote{Usually, reported in the log-log scale linear version: $\ln(p_k) \sim -\gamma \ln(k)$.} $p_k \sim k^{-\gamma}$  where $\gamma$ is called the "degree exponent". 

{\large \textbf{How a scale does (not) emerge}} \\
The PLDs are long-tailed functions; hence, describing a phenomenon that has not a reference scale.
For example, the heights distribution of a sample population displays a peak at the average height, which is the reference scale for that population. Instead, PLDs do not exhibit a peak but allows also for outliers, e.g. 100ft tall individuals.
More precisely, the variance, for a poissonian distribution, is $\sigma_k = \langle k \rangle ^ {1/2}$, limited as the network grows. So, the $\langle k \rangle$ scale is trustworthy. 

At the same time, for a PLDs the variance diverges since \cite{barabasi::2016networkbook}	
\begin{equation}
	\lim_{N \to \infty} \sigma_k^2 \approx k_{max}^{2-\gamma+1} = \infty.
	\label{eq:kn_SFnets}
\end{equation}
since real networks have typically $\gamma \in (2,3)$ (WWW has $\gamma =  2.1$ \cite{barabasi::2016networkbook}) and $k_{max} \sim N$. 

So, a reference scale for PLDs is not naturally present as, extending the poissonian result, for the "Exponential Bounded Distributions" (EBDs) \cite{barabasi::2016networkbook}, i.e. when there is a exponential or faster decay for high $k$, such as binomial/poissonian/Gaussian distributions.

{\large \textbf{The Degree(s) of Hubs}} \\
In the following, it is developed a quantitative comparison between the maximum degree allowed by EBDs and by PLDs to gain a numerical perception of their difference.
In detail, $k_{max}$ (or the "natural cutoff") is expected to be, roughly speaking, the size of the bigger hub; while $k_{min}$ approximates the smallest degree.
So, for an exponential distribution \cite{barabasi::2016networkbook}, 
\begin{equation}
	k_{max} = k_{min} + \frac{\ln(N)}{\lambda}.
	\label{eq:Expkmax_up}	
\end{equation}
where $\lambda = 1$ for convenience. The same $\ln(N)$ dependence would be recovered of any other EBD.

On the other hand, applying the same rationale to a scale-free network, 
\begin{equation}
	k_{max} = k_{min}\,N^{\frac{1}{\gamma-1}}.
	\label{eq:SFkmax_up}
\end{equation}

The results are somehow expected, since for:
\begin{itemize}
	\item EBDs: $k_{max} \sim k_{min}$ since $\ln(N)$ strongly reduces contribution with $N$;
	\item PLs: there could be order of magnitude of difference between $k_{max}$ and $k_{min}$.
\end{itemize} 
Indeed, as a simple example, WWW forms a graph of on $N \sim 10^5$ documents with $\langle k \rangle \simeq 4.6$ and $\gamma = 2.1$ \cite{barabasi::2016networkbook}. 
Knowing that $k_{min} = 1$ for EBDs, $k_{max} \sim 14$; while $k_{max} = 95.000$. 
This reinforces the insight that big hubs are naturally arising by increasing $N$ only for PLDs.
\label{sec:SFProperties_up}

{\large \textbf{Ultra Small-World}} \\
Hubs represent a one-hop bridge for many nodes, by centralizing the otherwise-distant nodes. Indeed, the average distance $\langle d \rangle \sim \ln(\ln(N))$ for $\gamma \in (2,3)$, formally describing the so called "ultra small-world phenomenon". An heuristic example is the airport lines, which diminish the \(\langle d \rangle\) among (far) cities with respect the national highways.

Surrounding the "Ultra Small-World", there are $3$ other regimes
\cite{Cohen:2003_SFUSW}:
\begin{enumerate}
	\item \underline{\textit{Anomalous Regime ($\gamma = 2$)}} producing a "wheel rim configuration", 
	a central hub with spoke nodes. In this regime, $\langle d \rangle = constant$;
	\item \underline{\textit{Ultra-Small World Regime ($2 < \gamma < 3$)}}: $\langle d \rangle \sim \ln(\ln(N))$ gives rise to the "ultra-small world phenomenon" as a slower growth in $\langle d \rangle$ than random network;
	\item \underline{\textit{Critical Point ($\gamma = 3$)}} for which $\langle d \rangle \sim \frac{\ln(N)}{\ln(\ln(N))}$, marking the passage between the small-world and the ultra-small world;
	\item \underline{\textit{Small World ($\gamma > 3$)}} random networks are recovered, resulting in $\langle d \rangle \sim \ln(N)$.
\end{enumerate}
Thus, \(\langle d \rangle\) is changing with $\gamma$, as the smaller it is, the shorter would be the average distance; but also with $N$, as it is the "ab ovo" hypothesis for having big hubs.

{\large \textbf{Growth and Preferential Attachment}} 

Even with broad different origins, most real networks display two important features without which it is impossible for hubs to emerge \cite{barabasi::2016networkbook}: growth and preferential attachment (see \autoref{App:RecPLD},\autoref{App:OriginOfPA}). 

In particular, growth is an active process driven by the sequential increasing of the size of the network $N$ as per the addition of new nodes. Rather, a random network assume $N$ to be fixed. \newline
Furthermore, most real networks new nodes prefer to link to the more connected ones, a process called "preferential attachment". On the other hand, random networks wire randomly with other nodes.

Ça va sans dire \label{cit:A.Marzo}, the derided model, developed in \autoref{sec:BA_model}, has to be dynamical and encode a "preferential" probability which changes every time ("event time") there is a modification of the topology. 

{\large \textbf{Final Remarks on Scale-Free}} \\
It worths to recall that, in general, the hosted phenomenon (-a) is complex and affects the graph topology and, in turn, the degree distribution. Rebus sic stantibus \label{cit:D.Massa}, it is sufficient to establish a fair starting point, or EBDs or PLDs class, over which developing gradually ad-hoc features, rather then the meticulous fitting.

As a final remark, many interesting features of scale-free networks, from their robustness to anomalous spreading phenomena, are linked to the $\gamma \in (2,3)$; thus, due to the small average distance which strengthen the considered phenomenon.
For a more detailed dissertation, see \autoref{sec:SFD_details}.

\newpage
\subsection{Barabási-Albert Model}
\begin{figure}[ht]
	\includegraphics[scale = 0.4]{../images/Networks/Barabasi_Albert_Model}
	\centering
	\caption{Formation of a scale-free network \cite{Barabasi:2009_SF_DecadeBeyond}}
	\label{fig:LCD_growth}
\end{figure}

\label{sec:BA_model}
To take advantage of the coexistence of both growth and preferential attachment in the real networks, it has been proposed a minimal model, the Barabási-Albert model \cite{barabasi::2016networkbook}:
\begin{itemize}
	\item start with $m_0$ nodes randomly connected and a degree at least equal to $1$;
	\item \textbf{Growth:} at each event time add a new node with $m(\leq  m_0)$ stubs which are linked to different nodes;
	\item \textbf{Preferential Attachment:} the probability to wire with a node $i$ depends on the degree centrality, i.e. importance based on the degree:
	\begin{equation}
		\Pi(k_i) = \frac{k_i}{\sum_{j = 1}^N k_j}
	\end{equation}
\end{itemize}

So, preferential attachment has a stochastic nature not a deterministic one. Nevertheless, it increases the appeal of the (already) high-degree nodes, alias \label{cit:S.Sagone} "rich-gets-richer" phenomenon. As a result, while most of the nodes end up having few links, a bunch of nodes become highly connected; thus, performing the role of big hubs. In turn, this translates into a degree distribution which does not decay exponentially, i.e. a power law with $\gamma = 3$ \cite{barabasi::2016networkbook}. 

Empirical studies on networks, such as actor and scientific collaboration networks, have found that the exponent at the numeratore of $\Pi(k_i) \sim k_i^\alpha$ could differ from $1$.
Indeed, there are $3$ paradigmatic kinds of Preferential Attachments (PAs) by varying the exponent $\alpha$: sublinear PAs ($0<\alpha<1$), (linear\footnote{Commonly, the here-called "linear" PAs is just called "preferential attachment" without further specification.}) PAs ($\alpha = 1$), superlinear PAs ($\alpha>1$). Briefly, the sublinear PAs drives to an "stretched exponential distribution" (see \autoref{sec:SFProperties}); the linear to the standard scale-free network; while in the superlinear almost all nodes connects to few super-hubs (winner-takes-all phenomenon).
Hence, only for $\alpha = 1$, a power-law distribution could be recovered, recalling that knowing the high-degree behavior is just a starting point to model the underlying phenomenon. 
In addition, after $t$ event times, the network has gained $t$ new nodes for a total of $N = m_0 + t$ nodes and $m_0 + mt$ links.

\newpage
\subsection{Linearized Chord Diagram}
Having in mind the general purpose and results, a more precise description on the building process would clarify two point left open, i.e. 
\begin{itemize}
	\item the arrangement of the initial $m_0$ nodes;
	\item whether the next $m$ links were added simultaneously or one at the time. This could drive to the formation of multi-links, i.e. conflicting edges, due to assumed independent nature of the new stubs. 
\end{itemize}

These problem are solved with the idea of sequentially adding nodes to the graph(s) created at a previous times.

Fixing $m=1$ for simplicity, start with the empty graph $G_{m}^t = G_1^0$.
At each event time, choose an initial vertex $v_i$ of \\ $G_1^{(t-1)}$ and connect it to the new vertex $v_t$ with probability $p$ such that
\begin{equation}
	p =
	\begin{cases}
		\frac{k_i}{2t-1} & if \quad i \in [1,t-1] \\
		\frac{1}{2t-1} & if \quad i = t
	\end{cases}
	.
\end{equation}

Rephrasing it, connect $v_t$ with $v_i$ with a probability of $\frac{k_i}{2t-1}$ if the nodes are different; otherwise, $\frac{1}{2t-1}$. This method does not forbid the formation of multi-edges and loops, but constraints their number to be negligible as the network grows.
The present technique referred to $m=1$ for an immediate perception. Whether $m > 1$ stubs are involved, add them one at the time, enabling their contribution to the hubs degree.

\section{Annealed Mean-Field Network}
\label{sec:Annealed_MF_Network}
As a benchmark of the network results, it has been introduced the "annealed-mean-field" network (AMFN) to properly simulate a homogeneous mixing with a fixed degree per node $D$. %For now, it is enough to know that the SIR model allows the susceptible nodes to be infected at a rate $\beta$ and the infected to recover at a rate $\mu$ (see \autoref{ch:sir-models}).
The definition of a AMFN will be given through the practical implementation of a SIR spreading over it, i.e. using a bottom-up approach. Briefly, the SIR model divides the individuals in 3 compartments and accounts for the (direct) infection scheme: $S - susceptible \stackrel{  \beta }{\implies} I-infected \stackrel{  \mu }{\implies}  R-recovered$, where $ \beta \textnormal{ and} \mu$ are the transmissibility and recovery rate.

Indeed, the mean-field SIR model is obtain, at every time-step $t$, with the following scheme: 
\begin{itemize}
	\item select an individual $I_1$ over the pool of infected;
	\item chose $\langle k \rangle$ neighbors at random among all the nodes\footnote{A fair spreading should consider as available contacts all the nodes, i.e. the susceptible but also the already infected or recovered ones. Indeed, all nodes are possible count; while only the susceptible ones could be infected.}, e.g. $I_2$,$I_3$;
	\item try to infect them with probability $\beta$ and to recover with probability $\mu$.
\end{itemize}
With this procedure, whenever the target infected node, e.g. $I_2$, is chosen to pass the pathogen at a future time $t'$, it is highly improbable to re-select its past infector $I_1$, since the probability goes as $D/N$, which typically is much less than one. 
\\Moreover, the number of neighbors is fixed to $\langle k \rangle$, giving to this model the "annealed" characteristic name.

As promised, a AMFN is defined as a (directed) random network whose $D$ links are randomly chosen by each node where the agent lays down. With this technique, the contacts are well mixed, emulating a "mean-field" approximation.

The spreading on network resembles the mean-field model, alias AMFN in our case, whenever either $R_0$ or the long-range connections $p \cdot N$ are high enough to allow the disease, reaching the epidemic state rapidly. Indeed, the comparison between mean-field and network spreading regards the fraction of total infected nodes and not how the nodes are infected: for the mean-field case the neighbors are chosen randomly along the circle; while they are fixed for the chosen network case. For this reason, there could an accordance in the measures considered.



\chapter{The Epidemiological Models}
\label{ch:sir-models}	
The most studied dynamical process on networks is the spreading of an agent. Rather than its common interpretation as a pathogen, this agent could also be considered as an information ("knowledge spreading"), an ask for collaboration, a mobile/computer virus, a business practice, a rumor or a meme, etc. These phenomena acts on different types of networks; are characterized by peculiar time-scales and follow various microscopic-transmission mechanisms. Despite all of these differences, they obey to a common pattern conveniently captured by analyzing their spreading on a network: the quoted graphs (cf. \autoref{ch:network-models}) provides different insights on how links could be created and preserved. Thus, although it has been starting from the biological sector \cite{VespignaniSatorras2001Epidemic}, it is used also in many other human-related fields, as the social sciences and digital security.

This thesis is devoted to the infective epidemiology: a paradigmatic model (SIR model) is let spread on different topologies simulating the COVID-19. In particular, infectious diseases are also called contagious, due to their transmission by a contacts with a secretion of an ill person, e.g. droplets. They account for $43\%$ of the global burden of diseases alongside with other noninfectious diseases (for example obesity, gambling or smoking) which are equally affected by the graph of contacts \cite{barabasi::2016networkbook}.

The epidemiological frameworks, in general, find their roots in two fundamental hypotheses:
\begin{itemize}
	\item Compartmentalization: Each individuals is classified according to a state, or compartment, allowed by the considered model. In particular, the simplest compartments are the following:
	\begin{itemize}
		\item Susceptible (S): vulnerable people which have not been in contact with the disease;
		\item Infectious (I): individuals, which are able the spread the pathogen to their neighbors;
		\item Recovered (R): infected individuals which has are removed from the I-state. No difference among deaths or recovered has been settled\footnote{In reality, this is not the case since death individuals are no more present in the society and leaves space for new contacts; while immunize nodes shields the more inner nodes, by arranging in a sun-like configuration (a susceptible core and spoken "recovered" nodes)}. Thus, many texts use the fair word "Removed" as a definition of the "R".
	\end{itemize}
	
	Driven by some transition probabilities, individuals are able to move among compartments: $S \rightarrow I$ is a random event regulated by the transmission rate ($\beta$); while $I \rightarrow R$ by the recovery rate ($\mu$). So, as for COVID-19, in the early stages a large fraction of the population was susceptible, e.g. $99\%$, called "naive population". Then, the pathogen drives some nodes into the infectious state; while, at the end, to the "Recovered" compartment.

	For convenience, it is also possible to introduce many other compartments to allow for additional states, such as the "asymptomatic" which are able to spread the disease without exhibit symptoms, the first signal of the infection.
	The epidemiological frameworks are generally called as their constitutive compartments characterizing the model, e.g. "SIR" stands for the node path among the susceptible-infectious-recovered states.
	The division in compartments has to be considered at a macroscopic scale, since the aim is to improve the prediction of the number of infected; while not caring about the microscopic infection within the host body; all these details, indeed, are encoded in $\beta \text{ and } \mu$;
	\item Mixing Level: People live and mixes but has a fixed number of total individuals $N$, i.e. no death or birth are taken into account (see \cite{Newman:2010_Net:AnIntro}). Hence, two possible hypothesis on how the individuals, restricted to their compartments, could get exposed to the disease are the:
	\begin{itemize}
		\item \textit{homogeneous mixing}: this hypothesis assumes that each individual has the same chance of interact with another one, e.g. an infectious. For this reason, it is also called the "fully-mixed", "mass-action" or "mean-field" approximation. Indeed, the specific degree of a single node is not taken into account; rather, the number of infectious-contacts per node is fixed. In turns, this could be seen as a "field" of mean infectious spikes which attack the susceptible.
		Only the "global" scale is present;
		\item \textit{heterogeneous (degree-based) mixing}: this approach hypothesizes that the individuals are grouped by their degree of contacts $k$ and considered "statistically" equivalent within that class: this is a finer mean-field approximation and could be improved by considering an "individual-based" approximation. 
	\end{itemize}
\end{itemize}
It seems pretty natural that the "fully-mixed" hypothesis is false at some extent.
In particular, it would be possible to observe that for small "epidemic strength" ($R_0$) the network structure deeply plays a role (see \autoref{ch:Results}), since individuals may transmit a pathogen only to (local) neighbors. Nevertheless, if a pathogen is "strong" enough the total number of infected are comparable to the one of a "mean-field" approach.

The model which may be composed with the quoted compartments are the $SI$, the $SIS$ and the $SIR$ (the case of study). In detail, the $SI$ model accounts for the fact that it would be impossible to recover from the disease, e.g. herpes; the $SIS$ model for diseases where it would be not possible to waning immunity, thus, returning susceptible, e.g. the HPV, generic flu; the $SIR$ where the last state is the gained immunity, e.g. the most strain of seasonal influenza or chickenpox.

Before gaining the details on the two mixing hypothesis, it is worth defining the variables involved in the two approach.
In particular, by denoting the total number of individuals $N$, $s(t) := S(t)/N$ is the number of susceptible (healthy) normalized with $N$; $i(t):=I(t)/N$ is the fraction of already infectious persons; while $r(t) := R(t)/N$ the recovered one. In this way, the "closed population" constraint translates into $s(t)+i(t)+r(t)=1$ at any time $t$. 
Thus, at the initial time ($t = 0$), a seed of infected has to be injected to start the infection, e.g. $I(0) := 10$. So, $s(0) = 1 - i(0)$ since the recovered are $r(0) = 0$.
Furthermore, let assume that the likelihood of transmitting the disease to a susceptible per unit time is $\beta$, i.e. the "transmission rate"; while the "recovery rate" is $\mu$. Hence, defining the \textit{basic reproduction number}  $R_0 := \beta \langle k \rangle / \mu$, this quantity embodies the number of secondary infected in a naive population.

The final goal would be to find the evolving number of new daily infected at each time (a slight modification of $I(t)$). Moreover, since the disease-spreading is a stochastic process, it would be the average of the various scenarios--in our case 50.

\newpage
\section{Homogeneous Mixing}

\begin{figure}[ht]
	\includegraphics[scale = 0.38]{Ch_TheEpidModels_Methodology/D4_ws_graph_sir.png}
	\caption{Mean-Field SIR using a Complete Graph with $N = 10^{3}, \langle k \rangle = 999, \beta = 0.001,  \mu = 0.2$}
	\label{fig:MF_SIR}
\end{figure}

Within this class of hypotheses, people mingle and meet completely at random: an infected person can be in contact with {\large $\frac{\langle k \rangle \, S(t)}{N}$} susceptible individuals. Taking into account that at a time $t$ there are $I(t)$ infected which spread the disease at a rate $\beta$\footnote{In many text books, e.g. \cite{Newman:2010_Net:AnIntro}, the $\beta \langle k \rangle$ total rate considered in the following is just $\beta$.} and recovers at a rate $\mu$,
the SIR equations are the following \cite{Newman:2010_Net:AnIntro}

\begin{equation}
	\begin{cases}
		\frac{ds}{dt} = -\beta \langle k \rangle s i \\ \\ 
		\frac{di}{dt} = \beta \langle k \rangle s i - \mu i = \beta \langle k \rangle \, (1-i-r) \, i -\mu i = \mu( R_0 s - 1) i  \\ \\
		\frac{dr}{dt} = +\mu i
	\end{cases}
	\label{eq:SIR_MF}
\end{equation} 
where it has been exploited the fact that the population is closed, i.e. no death/births are considered. This translates into $s+i+r=1 \quad  \forall t$ or even $ ds/dt + di/dt + dr/dt = 0 \quad \forall t$. In addition, the right most equality shows how the definition of $R_0$ could be recovered directly from the equations. Finally, note the removal of the time dependence since $s(t), i(t), r(t)$ is understood.

To solve \autoref{eq:SIR_MF}, by integrating "division" of the third equation and the first equation
\begin{equation}
	s(r) = s_0 e^{-\beta \langle k \rangle r / \mu}\
\end{equation}
where it has been required that $s(t=0)=s_0$ arbitrarily. 
Then, substituting $s(r)$ into the second equation \cite{Newman:2010_Net:AnIntro}, 
\begin{equation}
	\frac{dr}{dt} = \mu \, (1-r-s_0e^{-\beta \langle k \rangle  r/\mu}).
	\label{eq:r_SIR_MF}
\end{equation}

Now, solving this equation for $r$ gives, in turn, the possibility to find $s$ and, thus, $i$.
In principle,
\[
	t = \frac{1}{\mu} \int_0^r \frac{du}{1-u-s_0e^{-\beta \langle k \rangle  u/\mu}}.	
\]
The initial conditions can be chosen arbitrarily but the most suited is to fix a small number $c << N$ of infected individuals such that $s_0=1-c/N\approx1$, $i_0:=i(0)=c/N\approx0$ and $r_0\approx0$.

Unfortunately, this equations have no closed form, but special insights cloud be reached by looking at the plot of the numerical solutions of \autoref{eq:SIR_MF}.


The case where $R_0 < 1$ represents the scenario when the pathogen will die out fast from the population, i.e. a limited spreading occurs. 
\\Indeed, at early times $t\approx 0 \implies i(0)\sim 0, r(0)\sim 0$. 
\\So, \cite{barabasi::2016networkbook} \[i(t\approx 0) \sim i_0 \, e^{(\mu(R_0 -1)t)}.\]
On the other hand, the SIR features are more involved if $R_0 > 1$:
\begin{enumerate}
	\item $s(t)$ is monotonically decreasing; while $r(t)$ is always increasing; 
	\item $i(t)$ increases depending on the availability of susceptible: at early times the increasing is exponential; then, deflect and decreases on the way to the end of the epidemic, i.e. $i(t_{max})=0$;
	\item $s(t_{max}) > 0$ since when $i \to 0$ there are no infected individuals left to infect the remaining susceptible. Formally, $r(\infty):=\lim_{t \to \infty} r(t)$ is the total size of the outbreak, i.e. the total number of individuals who ever catch the disease. 
	
	Setting $dr/dt \stackrel{!}{=}0$, $s_0\simeq1$ ("naive population") and using \autoref{eq:r_SIR_MF}, the solution of 
	\begin{equation}
		r = 1 - e^{-\beta \langle k \rangle  r /\mu}
		\label{eq:r_SIR_MF_GiantComp}
	\end{equation}
	is precisely $r(\infty)$. Therefore, whenever $r(\infty)\neq1$ is a sign that "herd-immunity" is reached, alongside with $1-r(\infty)$ susceptible left;
	\item the characteristic time \[ \tau  = \frac{1}{\mu(R_0-1)}\] is the time needed to reach about the $36\%$ of the population and it is the inverse of "disease velocity", which further depends both on the possibility to infect and the average nearest neighbors. With this new measure of epidemics, it would be possible to recover the previous result observing that there is a spread iff $\tau > 0$, i.e. $R_0 > 1$.
\end{enumerate}

Interestingly, it would be possible to recognize $S \leftrightarrow r$ and $D:= \langle k \rangle \leftrightarrow \beta \langle k \rangle/\mu$ (\autoref{eq:r_SIR_MF_GiantComp}) where the "left" quantities are describing the percolation phenomenon and the "right" ones the SIR spreading describes. In this way, the equation \autoref{eq:r_SIR_MF_GiantComp} regulates the size $S$ of the giant component of a Poisson random graph. In particular, the SIR infection of neighboring nodes could be seen as a bond percolation process of an agent, e.g. water, which propagates according to the same probability distribution \cite{Newman:2010_Net:AnIntro}, \cite{barabasi::2016networkbook}. 
This analogy allows to double the interpretations of the results of \autoref{eq:r_SIR_MF_GiantComp}.
Indeed, in percolation theory, the giant component size $S$ depends on the $D$ average degree of links per node when $N \to \infty$ . 
\\So, the final outbreak size $r(\infty)$ depends on $\beta \langle k \rangle / \mu$ and the \textit{basic reproduction number} is, conveniently, defined as $R_0 := \beta \langle k \rangle / \mu$. 
\\Moreover, $r(\infty)$ goes continuously to $I(0)/N \sim 0$ as $1^{+} \leftarrow R_0$; while it is constantly $I(0)/N$ for $R_0 \to 1^{-}$. Analogously of the appearance of the giant component, the point where (first-order) phase transition occurs, i.e. $R_{0c} = 1$, is called epidemic transition. Thus, $R_{0c}$ separates the "epidemic" ($R_0 > R_{0c}$) from the "non-epidemic" ($R_0 < R_{0c}$) regime.
The $R_0$ dependence of $r(\infty)$ has to be somehow expected since if an infection has less transmissibility than recovery ($R_0 <1$), the pathogen would die out fast. On the way to $R_{0c}$, an increasing of the $\beta / \mu$ ratio would describe a less strong epidemic, which consecutively provide a minor $r(\infty)$.

To be precise, $R_0$ is the number of second infections in a naive population, i.e. at early times $t \approx 0$: $\beta/\mu = \beta \tau$ is the probability to infect over a time-interval of $\tau$. Thus, $R_0$ is an estimate of the slope of the curve describing the infection at the start of the epidemic. Furthermore, it is a rough marker of an exponential growth\footnote{For SI model, $\lim_{\mu \to 0} R(t) = \infty$ since every infected could arbitrarily infect all the other nodes. In a real scenario, $R_0 \, (\leq N')$ is finite, as the size $N'$ of a population is finite.}. (for $R_0 > 1$) or vanishing of the pathogen (for $R_0 < 1$) since $I(t) \approx R_0^{t/d}$. On the other hand, $R_0 =1$ is not indicative of whether this slope is going to increase until $r=1$ (pandemic scenario), decreases fast guiding into $r(\infty)<<1$, or both resulting in $r(\infty)<1$. Indeed, for a more precise signal on the spreading behavior depending only by the initial conditions, higher-order features of the underlying network should be taken into account. A new reproduction number is going to be proposed in the next section.

\newpage
\section{Heterogeneous Mixing}
\label{sec:degree-basedMF}
\begin{figure}[ht]
	\includegraphics[scale = 0.38]{Ch_TheEpidModels_Methodology/D4_ws_graph_sir.png}
	\caption{Degree-Based SIR using Erdös-Rényi Graph with $N = 10^{3}, p = 0.5, \langle k \rangle = 500, \beta = 0.001,  \mu = 0.2$}
	\label{fig:DB_SIR}
\end{figure}
As quoted before, within the well-mixed population it has been considered that every node could infect, at any time, a fixed amount of susceptible neighbors $\langle k \rangle$. This is not the case, e.g., for a pletora of real networks (Barabási-Albert or Linearized Chord Diagram models) as they are "complex"/"scale-free" in the sense that, for $\gamma \leq 3, \, \langle k^2 \rangle$ typically diverges. Thus, fixing the number of contacts to $\langle k \rangle$, it drives to underestimate the spreading, which could have worser effect than overestimating it. 

An improvement would be to consider that if a pathogen spreads on a network, individuals with more links are more likely to be infected. Therefore, the nodes are further classified according to their degree and assumed statistically equivalent within their class. This method may, thus, called degree-based mean-field approach. Indeed, there would be $3 \cdot k_{max}$ classes, i.e. $3$ compartments for each degree, but there are no individual differences among nodes with the same degree $k$. 

The total fraction of infected $i(t) = \sum_{k=0}^{k_{max}} p_k i_k(t)$. Alongside with the other decomposition of susceptible and recovered, substituting them in the \autoref{eq:SIR_MF} it would be possible to recover for each $k = 0,\cdots,k_{max}$
\begin{equation}
	\begin{cases}
		\frac{ds_k}{dt} = -\beta  k  s_k \Theta_k \\ \\ 
		\frac{di_k}{dt} = \beta  k  s_k \Theta_k - \mu i_k = \beta  k  \, (1-i_k-r_k) \, \Theta_k -\mu i_k \label{eqs:SIR_degree-based} \\ \\
		\frac{dr_k}{dt} = +\mu i_k
	\end{cases}	
\end{equation}
where $\Theta_k(t) = i_0 \frac{\langle k \rangle - 1}{\langle k \rangle} e^{t/\tau}$ is the density function, representing the fraction of infected nodes surrounding a susceptible individual with degree $k$. It would be recovered, and commented, shortly but note that, a priori, it can depend both on $k$ and $t$.

The above equations depend, by construction, on $k$, the $\langle k \rangle$ dependence is no more present; while network structure is encoded into the $\Theta_k(t)$ factor, which in the homogeneous case was $i$.
Before tackling the problem of solving them, it is necessary to explicitly recover the $\Theta_k(t)$ factor.

\subsection*{The Density Function}
The density function, as reported before, provides the fraction of infected nodes neighboring a susceptible degree-k node. In fact, as we are going to prove, this estimation account for all the infectors connected with an arbitrary number of hops, since it has no upper-limit. 

Formally, it has been assumed that the network lacks the degree correlation: nodes are connected at random, thus an hub could equally connect with another hub or a lower-degree node; so, called, a "neutral network". Hence, the probability of a vertex to point to a degree $k'$ node is the excess degree
\begin{equation}
	q_{k'} = \frac{k' p_{k'}}{\langle k \rangle}
	\label{eq:q_excess_degree}
	.
\end{equation}
Indeed, the probability of choosing a degree-$k'$ node is $p_{k'}$ but, since it has been $k'$ possibilities with one stub to connect with it, this has been enhanced by a factor of $k'$. As $q_{k'}$ should be a probability, the denominator comes to match the $\sum_k q_k \stackrel{!}{=} 1$ constraint.
The \autoref{eq:q_excess_degree} is local, random and linear in the "bias term" $k$. So, as reported in the \autoref{sec:linear_pa_link_selection}, it is the starting point to build the (linear) preferential attachment. Moreover, \autoref{eq:q_excess_degree} grows with $k'$ or $p_{k'}$, i.e. the "presence" of $k'$ degree nodes.

For a disease to spread, it is required at least one of the spoke of the infected node is connected to the one target node: only $k'-1$ links would be available.
Thus,
\begin{equation}
	\Theta_k(t) = \frac{\sum_{k'} (k'-1)p_{k'}i_{k'}(t)}{\langle k \rangle} = \Theta(t)
	\label{eq:Theta_generic}
\end{equation}
where the rightmost equation explicitly display the independence on $k$.

In order to capture its time-dependence, it would be take advantage of the specific model of spreading, e.g. SIR, following these steps: differentiate the \autoref{eq:Theta_generic}, insert \autoref{eqs:SIR_degree-based} and consider $1-i_k-r_k \approx 1$, which it is the case at early times. In this way, it could be obtained that \cite{barabasi::2016networkbook}

\begin{equation}
	\begin{cases}
		\frac{d\Theta}{dt} = \left( \beta\frac{\langle k^2 \rangle - \langle k \rangle}{\langle k \rangle} - \mu \right) \Theta \\ \\
		\Theta(0) \stackrel{!}{=} i_0 \frac{\langle k \rangle -1}{\langle k \rangle } \qquad \text{I.C.}
	\end{cases}
\end{equation}
which solving by $\Theta$ gives 
\begin{equation}
	\Theta(t) = C e^{t/\tau}
	\label{eq:Theta_SIR_network}
\end{equation} 
where
\begin{equation}
	C := i_0\frac{\langle k \rangle -1}{\langle k \rangle}  \qquad \text{and} \qquad
	\frac{1}{\tau} := \beta \left(\frac{\langle k^2 \rangle}{\langle k \rangle}-1\right)
	-\mu.
	\label{eq:tau_SIR_networks}
\end{equation}

\begin{figure}[t]
	\includegraphics[scale = 0.38]{../images/Ch_TheEpidModels_Methodology/ER_Epid_Threshold.jpg}
	\centering
	\caption{Epidemic Threshold in a ER Graph with $N = 10^{4}, \beta \in [0,0.4], \langle k \rangle \sim 10, \mu = 0.8$. Note that the departure from zero happens near 0.1, due to finite size effects. Thus, $R_0 < 0.1 \cdot 10 = 1$ drives to a fast-decay of the infection}
	\label{fig:ER_Epidem_Thr}
\end{figure}

A global outbreak is possible if, and only if, $\tau > 0$, which implies (cf. \autoref{eq:Theta_SIR_network}), that the number of infected nodes grows exponentially with time. This allows to recover the condition for a global outbreak
\begin{equation}
	\lambda > 
	\lambda_c:=\left[ \frac{\langle k^2 \rangle}{\langle k \rangle} - 1\right]^{-1}
	\Rightarrow
	R_0 > R_{c-net}:=\frac{ \langle k \rangle^2 }{\langle k^2 \rangle-\langle k \rangle} \geq 0
	\label{eq:lambdac_Rc_netSIR}
\end{equation}
where it is specified the interdependence of the degree-heterogeneity ($\langle k^2 \rangle$), $\mu$ and $\langle k \rangle$ for a decay in the presence of the pathogen. For these reasons, $\lambda$ is called the (biological) "spreading rate", since include all the biological factor of the disease, and $\lambda_c$ the "epidemic threshold". Indeed, the number of the contagions is not linear with the enhancement of the spreading rate; rather, it is highly enhanced by passing $\lambda_c$. 

By specializing \autoref{eq:tau_SIR_networks}, it is possible to recover that, in the case of random networks, the final size of an outbreak displays a first-order-phase transition; while for scale-free network $\lambda_c$ vanishes meaning that every epidemics could spread, regardless its biological weakness.
\\In particular, for a \underline{\textit{poissonian network}} $\langle k^2 \rangle = \langle k \rangle (\langle k \rangle + 1 )$\footnote{As a rule of thumb, for a Poisson-distributed random variable $k$ holds $\sigma_k^2 = \langle k^2 \rangle - \langle k \rangle^2 = <k>$. So, $\langle k^2 \rangle = \langle k \rangle^2 + \langle k \rangle$}, 
resulting in the "epidemic threshold of a random network" \cite{barabasi::2016networkbook}
\[ \lambda_c = 
	\frac{1}{\langle k \rangle} \Rightarrow R_{c-net} = 1.
\]
Being $\langle k \rangle$ always finite, and the reference scale for a random network, $\lambda_c$ play the role of a true threshold, which separates the epidemic regime to the non-endemic regime for as recovered in the previous section.
\\For a \underline{\textit{scale-free network}}, $\langle k^2 \rangle$ diverges for large networks ($N\to \infty$), since $\gamma < 3$ and the probability of having big hubs does not vanish. In this limit, $\lambda_c, \tau \to 0$ showing, respectevely, the fact that even low-transmissibility disease are successfully spreading in the population and are instantaneous.
An heuristic explaination of this is that, as for the "network-distance" contraction, hubs are super-infectors of every disease. Thus, even if a pathogen has a weak load of infectiveness, when a hub is infected, it could be able to pass it to a larger number of nodes, enabling the pathogen to persist within the population. In the present context, hubs are responsible of a "time-length" contraction.
\\The exposed examples are at the extrema of the possible scenarios. For most networks in the middle, e.g. the ones described in \autoref{ch:network-models}, the enanchement of $\langle k^2 \rangle > \langle k \rangle (\langle k \rangle +1)$ implies a reduction both of the epidemic threshold and the characteristic time of the disease. Thus, in contrast with the "mean-field" predictions, it is possible to refine the calculations for a specific network.

To sum up, for $\lambda>\lambda_c$ the total epidemic outbreak size $i_{\infty} \neq0$; while for $\lambda < \lambda_c$, $i_{\infty} = 0$, since the pathogen is not strong enough to reach an endemic state.
Moreover, $\lambda_c = 0$ for a "typical"($\gamma<3$) large scale-free network; while $\lambda_c \neq0$ for the random networks.

\subsection*{About the SIR equations}

No analytic solution exists for the equations regulating the SIR epidemic phenomenon. Furthermore, even the early times analysis exhibits some technical difficulties which demands for the numerical simulations of the ODEs of \autoref{eqs:SIR_degree-based}.

\subsection*{Early Times}
At early times ($t\approx 0 \implies r_k, s_k\approx0$), according to \autoref{eqs:SIR_degree-based} and \autoref{eq:Theta_SIR_network}, the fraction of infected is \vspace{3mm}
\begin{equation}
	\begin{cases}
		\frac{di_k}{dt} = - \mu i_k + i_0 \beta k \frac{\langle k \rangle -1}{\langle k \rangle } e^{t/\tau} \\ \\
		i(0) \stackrel{!}{=} i_0 \qquad \text{I.C.}
	\end{cases}
\end{equation}
whose, first equation, could be straightforwardly rewritten as
\begin{equation}
	\frac{d i_k}{dt} + \mu i_k - i_0 B e^{t/\tau} = 0
\end{equation}
where $i_0 B(k) := i_0 \beta k \frac{\langle k \rangle -1}{\langle k \rangle }$.

Hence, the solution of the last equation assumes the form
\begin{equation}
	i_k(t)=i_0 \left[C_1(k)e^{t/\tau} + C_2(k) e^{-\mu t}\right]
\end{equation}
where it has been defined as \vspace{3mm}
\begin{equation*}
	\begin{cases}
		C_1(k):= \frac{B(k) \tau}{1+ \mu \tau} = \lambda \cdot \left[k\left(\frac{\langle k \rangle -1}{\langle k \rangle }\right)\right]
		\cdot \frac{1}{\tau} \\ \\
		%\frac{\beta}{\mu+\tau^{-1}} \left(\frac{\langle k \rangle -1}{\langle k \rangle }\right) k \\ \\
		C_2(k):= 1 - C_1(k)
	\end{cases}
\end{equation*}
where there have been isolated, in $C_1(k)$,  the contribution from the pathogen ($\lambda:=\frac{\beta}{\mu}$), the network and the interaction among the two ($\tau^{-1}$). In all the previous equations, only the dependence was made explicit, since normally the epidemics parameter are fixed, while the degrees $k$ vary.

In this way, the initial exponential-phase has two contribution: the first, which grows in time, due to an interplay among the disease and the underlying network; the second is diminishing the number of infected according to the recovery of infected individuals.

For \textit{real networks}, the "degree exponent" $\gamma<3$ allowing, in the large networks limit, for $k$ and $\tau^{-1}$ to diverge. This, in turns, gives birth to a peculiar limit of $C_1(k)$: \vspace{3mm}
\begin{equation}
	C_1(k)\propto \tau k \propto \frac{1}{\int k^2 p_k dk}k  \to 0 \cdot \infty.
\end{equation}
where the last proportionality displays that $k$ diverge alongside with $\langle k^2 \rangle$. More specific insights could be reached only by considering the underlying probability degree distribution $p_k$.

On the other hand, for \textit{limited $\langle k \rangle$ networks} which find a good approximation in the random limit, i.e. when the individual contacts are limited by a cost-benefit constraint (e.g. power-grids), $\langle k^2 \rangle $ is limited and, therefore,
\begin{equation}
	i_k(t) \approx i(0)  + \left(C - C_2(\langle k \rangle )\mu \right) t
\end{equation}
where $C:= \frac{\beta (\langle k \rangle -1)}{\tau}$ since $\langle k \rangle \approx k$ and $e^{at}\approx1+at$ as either $t\approx0$ or $a \approx 0$. \vspace{3mm} The dependence on $k$ is, thus, lost: at the beginning of an epidemic all the nodes are statistically equivalent.

Ultimately, the degree-block approximation is itself a "mean-field" framework, since all the nodes, endowed with the same degree, are equivalent. However, this approach, although simplifying the calculations, it is not completely correct. Indeed, for the SIS model (with reinfection), the formal mathematical calculations drive to a vanishing epidemic threshold even for $\gamma>3$ \cite{barabasi::2016networkbook}.

\section{Order Parameter}
\label{sec:OrderParam4LinContagion}
In order to analyze the linear behavior of $\pi(t)$, it is worth defining \textit{critical basic reproduction number} the  $R_{0c} := \beta D_c / \mu \stackrel{!}{=} R_{c-net}$ and in the case where the network is not so heterogeneous $ R_{c-net} \sim 1$, where $1$ is for a poissonian distribution. This is, indeed, the case for the reported networks (\autoref{ch:Results}) yielding a critical degree $D_c = \frac{\mu}{\beta}$. 

A rough estimate of $C(t) \propto R^{t/d}$. So, as the epidemic is fixed, the exponential growth is recovered for $D > D_c$; while $D < D_c$ displays a drop out of the pathogen. For $D \sim D_c$ \cite{Thurner::NetBasedExpl}, the number of infected will behave critically, i.e. $\pi(t) \propto t^{\alpha}$, where $\alpha \in (1,2)$ for a simply connected (no multiedges or loops) short-range network embedded in dimension $2$. More in detail, $\alpha \sim 2$ for a network resembling a regular lattice in $2$ dimensions, since $C(t) \propto t^2$ is proportional to the covered area of the infection front; while $\alpha \sim 1$ for a tree or a collection of 1-dimensional chains, as the front spreads linearly as a flame along a fuse, i.e. $C(t) \propto t$. In between these regimes, the specific $\alpha$ is influenced and shaped by non-pharmaceutical interventions (NPIs).

For a sub-exponential growth, it has to be expected that the network features, e.g. $\langle k^2 \rangle$, play a significant role in this context. In particular, for a fixed pathogen with $\lambda = \beta/ \mu$, $R_0 \propto D$ and it would be natural to search for a critical average degree $D_c$ (\cite{Thurner::NetBasedExpl}), when it is informative (e.g. not in scale-free networks), such that, fixing $p, \beta, \mu$, 
\begin{itemize}
	\item for $D>D_c$, the situation resemble a mean-field with $\pi(t) = R(t+d)$, where the shift in time is due to the fact that the average time of one infected to recover is $d$ days. In addition, the number of infected assumes a typical bump on the immediate days after the starting, which is an evident mark of an exponential growth both in $C(t) \text{ and } \pi(t)$.
	\item for $D<D_c$, the sub-exponential behavior is recovered with the consequence that the final outbreak size is lower than the mean-field one.
\end{itemize}
A proper way to characterize this transition is per the definition of the order parameter \cite{Thurner::NetBasedExpl} as the standard deviation (SD) of the new daily cases, after excluding all the days without new cases.
\\Formally,
\begin{equation}
	O := SD(C(t)) = 
	\begin{cases}
		0 \qquad \text{if $C(t) = constant$}\\\\
		\neq 0 \qquad \text{otherwise} 
	\end{cases}
\end{equation}
where $SD$ is the "standard deviation".
\\As anticipated, a linear growth is possible only if the new daily cases are constants, since $\pi(t) \sim t^{\alpha}$. For the "S-shaped" curve, resembling a logistic curve, a bump in the daily infected occurs producing an enhancement of the $SD$. Hence, a SD deviating from zero signals the presence of a nonlinear increase of the cumulative positive cases, $\pi(t)$ \cite{Thurner::NetBasedExpl}. Finally, $D_c$ depends on the set of parameters $p, \beta, \mu$ \cite{Thurner::NetBasedExpl} and, eventually $N$.

Also $R_0$ should be modified as it represents only the initial strength of the pathogen without considering the underlying topology, e.g. the "long-range" contacts $pN$. In other words, equal $R_0$s could drive to completely different scenarios (see the next chapter).

To sum up, it is going to be analyzed the graphic of the epidemiological quantities, such as $\pi(t)$ and $C(t)$, simulated via an heterogeneous SIR model on networks (Regular graph, Watts-Strogatz model, Poissonian SW Network, Caveman model and a Barabási-Albert Model) and benchmarked with the homologous $\pi(t)$ and $C(t)$ on a AMFN. In these plots, there are also reported the standard $R_0 := \beta D / \mu$ alongside with the $R_0(\delta) := R_0 /\delta$ where $\delta:=\langle d \rangle $ is the average path length.

Aside of this, it is reported the dependence of $SD(C(t))$ on $D$, in order to characterize the (quasi-linear) behavior of $\pi(t)$ with an $SD(C(t)) \sim 0$; while the exponential curves with a standard deviation $SD$ sensibly different from $0$.

\subsection*{Analytical critical degree}
In Statistical Mechanics, the dimensionality of a system is crucial to legitimate the mean-field approximation: the Ising Model is well modeled by a mean-field in a dimension $d \geq 4$ as it captures its phase transition.

In the following, $D_c$ is derived for an ER model and a ring-shaped poissonian small-world network (see \autoref{sec:PSW_network}) with $D$ average degree, $p$ rewiring probability, $D(1-p)$ short-range per node links, e.g. family, and $Dp$ long-range connections, e.g. leisure activities. So, the probability that an infector will infect a susceptible person in the next d days is $\beta_d:= 1-(1-\beta)^d \sim \beta d = \beta/\mu$, i.e. the complementary probability of not having a "risky" interaction for $d$ days.

Taking care of the dimensional effects, it is worthwhile to define the curvature of the front as $ \mathcal{K}:= \pm 1/\mathcal{R}_{front}$ where $\mathcal{R}_{front}$ is the radius of the fitting circle of the front: $\mathcal{K}>0$ for concave case; while $\mathcal{K}<0$ if the front surrounds an island of susceptible. For the poissonian small-world network, $\mathcal{K} = 0$, since the front is flat.    

Moreover, $R_0$ is exact only for a naive population, i.e. at early times. In fact, if a node is later infected, it would find at least one neighbor infected, alias its infector. So, taking care of the epidemic spreading, $R := (D-n)\lambda$, where $n$ is the average number of not-susceptible neighbors which, further, implies that $\nu := 1 - n/D$ is the fraction of susceptible. This latter quantity has to be interpreted as an general estimate of how infectious is a disease, i.e. for low infectivity, $\nu\sim1$ which implies $D \lesssim D_c$.
To be precise, for the "fuse-mode" $\nu$ represents only the fraction of susceptible in the neighborhood of the infection front; while for the ER model, $n$ embodies the entire population.
\\Whenever, $n$ is known, 
\begin{equation}
	D_c = n + \lambda^{-1}.
	\label{eq:D_c_analytical_discussion}
\end{equation}   

Moving forward with the calculations, the network models have to be exploited. In particular, it is going to be discussed the ER model (see \autoref{ch:network-models}) and the tree- or chain-like "fuse model" \cite{Thurner::NetBasedExpl}. For low infection level, i.e. $\beta<<1$ or $\nu\sim1$, the claim is that $n\sim 1$ for a random graph, while $n \sim (1-\nu)D$ with $\nu\sim 1/2$ for a "fuse-model": $\nu\sim 1/2$ since the disease spreads on half of the tree, living the remaining nodes as susceptible. 

\subsection*{Erdös-Rényi Graph}
For a Erdös-Rényi random graph with "rewiring probability" $p$, the average degree of contacts is $D = Np$. So, the number of un-infectable nodes from an infected is $n \sim 1 + (D-1)(1-\nu)$, as $1$ is the past infector of the fixed node summed with the remaining neighbors which are infected or recovered \cite{Thurner::Appendix_NetBasedExpl}. 
\\Inserting the obtained $n$ in \autoref{eq:D_c_analytical_discussion}, 
\begin{equation}
	D_c^{ER} = 1 + \frac{ \lambda^{-1} }{\nu}.
	\label{eq:Dc_ERenyi}
\end{equation}
which grasps the fact that for low infection levels $n\sim1$. 
In addition, $D_c^{ER}$ increases as $\nu$ drops: it has been needed more contacts to spread a disease if there are less susceptible available. However, for low infection or at the starting of the epidemics $\nu \sim 1$ and the is fixed by the epidemics parameters.
%\\Other way around, for a fixed $D$ since $\nu \leq 1$,
%\begin{equation}
%	\nu_c = \text{min}\left(1, \frac{1}{(D-1)rd}\right)
%	\label{eq:nu_c_ER_model}
%\end{equation} 
%which diminish as $D$ increases: if, on average, $\nu' < \nu$ it is likely to have a diffuse pathogen since $D_c' >  D_c$. 
%Moreover, $\nu_c = 1$ provides that a pathogen dies out whenever, at the beginning of the spreading, the average contacts $D < 1+1/rd$.

\subsection*{The fuse model}
\label{sec:D_c_fuse_model}
\begin{figure}[t]
	%scale = 0.27, trim = {10cm 0 1cm 0}
	\includegraphics[trim = {2cm 0 1cm 0}, width=.8\textwidth]{../images/Results/PSW/OPSW/OrdParam/p0.3/NNO_Conf_Model_addE_True_ordp_p0.3_beta0.015_mu0.07.png}
	\centering
	\caption{SD-Phase Transition of a Poissonian-SW Network with $N = 10^{3}, \beta = 0.1, D_c \sim 4.8, \mu = 0.25$. As before, the $D_c$ of the figure differs from the analytical estimation for finite size effects. The "fuse model" estimate could drive to a higher estimation of $ D_c$ due to its assumptions, e.g. \autoref{fig:Ordp_BA_COVID}.}
	\label{fig:SD_Threshold_Fuse_Model}
\end{figure}

In this section, it is going to be obtained, for a one-dimensional regular grid with average degree $D$, the critical degree \cite{Thurner::NetBasedExpl} 
\begin{equation}
	D_c = 1 + \frac{2 \lambda^{-1}}{1+p}.
	\label{eq:Dc_PSW_network}
\end{equation}
The same result may be used for the poissonian small-world network.

The first step is to introduce an invariant probability function $\rho(x,t) = f(x-vt)$ that grasps the dynamics of the infection front: $\rho(x,t)$ is the probability a node in the $x$ location is infected at time $t$.
In particular, $x$ is the network distance, as the number of in-between nodes from the initial infected node; $t$ is the time passed in days after disease started infecting and $v$ is the traveling velocity of the spreading along the $x$ axis.
A pedagogical representation of $\rho$ could be find in \cite{Thurner::Appendix_NetBasedExpl}: roughly speaking, it has the shape of a $\arctan(x-vt)$ where, on the $x$-axis, there is the 1d-chain on  nodes; while, on the ordinates, it has been translated to fit the $y \in [0,1]$ interval\footnote{The "past" nodes have higher $\rho$ while the future nodes have diminishing probability of being infected.}.   Furthermore, it has been centered to an arbitrary node ($q$), which has $\rho(q) = 1/2$.

To estimate $v \text{ and } \rho(x,t)$, it is taken advantage on $\rho(x,t+1)$ assuming that $v$ is such that all the "non-susceptible nodes" are indeed infected, not recovered. So, the probability of being infected at a following time $t+1$ is
\begin{equation}
	\rho(x, t+1) = \rho(x,t)+(1-\nu)2D \beta \langle \rho \rangle(x,t) (1-\rho(x,t))
	\label{eq:fuse_model_rho(x,t+1)}
\end{equation}  
where $\langle \rho \rangle(x,t)$ is the (local) average probability of being infected at $(x,t)$ over the adjacent neighbors.
In other words, the \autoref{eq:fuse_model_rho(x,t+1)} considers the probability of already being infect; alongside with 2 terms:
\begin{itemize}
	\item $1-\nu$ and $1-\rho$ which are the fraction of infected nodes and the probability of being susceptible;
	\item $\delta:= 2(1-\nu)D \beta$ is the total probability rate of having (at least) a contagion over the $2(1-\nu)D$ infected neighbors.
\end{itemize}
For a one-dimensional front, its radius proceed at a $x=vt$ pace and, defining $\tau$ as the next infection time, the a ratio of the non-susceptible/susceptible nodes may be approximated as 
\begin{equation*}
	r:= \frac{1-\nu}{\nu} = \frac{t+\tau}{t} = 1+\frac{\tau}{t} = 1+v \tau \mathcal{k}.
\end{equation*}
Moreover, defining $\tau$ as the time one single node is infected $v \cdot \tau = 1$ implying that 
\[ R = \frac{1}{2+\mathcal{K}} = \frac{1}{2} \]
since for a flat "fuse model" $\mathcal{K} = 0$. 
The naive expectation is recovered as $\nu \sim 1/2$. 

Inserting the \autoref{eq:fuse_model_rho(x,t+1)} on the invariance condition 
\begin{equation}
	\frac{\partial \rho}{\partial t} = - \frac{\partial x}{\partial t} \frac{\partial \rho}{\partial x} = -v \frac{\partial \rho}{\partial x},
\end{equation}
with the further approximation that $\langle \rho \rangle(x,t) \sim \rho(x,t)$,
\begin{equation}
	\rho(x,t) = \frac{1}{1+e^{\delta(\tau x - t)}}
\end{equation}
with $\delta:= 2(1-\nu)D \beta$. 
For this solution, $\rho(vt,t) = \rho(0,0) = 1/2$ marks the infection front \cite{Thurner::Appendix_NetBasedExpl}. 

The average probability of the nodes on the left of the front being infected is
\begin{equation}
	\hat{\rho} \sim \frac{1}{\nu D} \int_{-\nu D}^{0} dx \rho(x,0) = 1+ \frac{1}{\delta'} \log( \frac{1+e^{-\delta'}}{2} )
	\sim \frac{1}{2} + \frac{1}{8} \delta' + O(\tau^3) 
	\label{eq:rho_hat_fuse_model}
\end{equation}
where it has been exploited that $\delta':=\tau \delta \nu D$ and the last expansion as $\tau<<1$. Thus, $\hat{\rho}$ could also interpreted as the probability of the past infected nodes.

Moreover, $n$ could be obtained considering the situation where a $\nu$ nodes are susceptible but the "past" nodes are not and viceversa: $\nu \hat{\rho} \text{ or } (1-\nu)(1- \hat{\rho})$. 
\\Formally,
\begin{equation}
	n \sim 1+ (D-1)(\nu\hat{\rho}+(1-\nu)(1-\hat{\rho}))
	\sim 1+ (D-1)(\frac{1}{2} - (1-2\nu) \frac{1}{8} \delta')
\end{equation}
which for a flat infection, where $\nu\sim 1/2$, reduces to
\begin{equation}
	n = 1 + \frac{1}{2}(D-1).
\end{equation} 

Ultimately, introducing the $pD$ long-range ("lr") neighbors; while keeping the infection low, $n_{lr} \sim 1$.
\\Since $n$ is a local estimate, removing the $n_{lr}$ contribution,
\begin{equation}
	n = 1 + \frac{1-p}{2}(D-1)
\end{equation}   
and, therefore,
\begin{equation}
	D_c = 1+ \frac{2 \lambda^{-1}}{1+p}.
	\label{eq:final_D_c_fuse_network}
\end{equation}

This estimate express the critical degree, i.e. the average number of contact under which the mean-field approximation fails, as a function of $p,\beta,\mu = 1/d$. As a second order approximation in $\tau$ as been exploited in \autoref{eq:rho_hat_fuse_model}, overestimates the number of infected which, in turn, drives to an overestimation of $D_c$. Other sources of divergence from the simulated critical degree may be the finite size effects or deviation from $\mathcal{k} = 0$.

\chapter{Methodology}
\label{ch:Methodology}
As reported in \autoref{sec:ATechIntro}, the goal of this thesis is to study the "(quasi-)linear regime"\footnote{The "(pseudo)-linear" is recovered for an epidemic where $C(t) = \textnormal{ constant}$ for a reasonable amount of time, e.g. 20 time-steps. In turns, this implies a linear growth of the number of total cases.} of $\pi(t)$ as a result of modeling the interactions among individuals with a network. Indeed, as NPIs measures prune the social network to avoid the presence of highly connected nodes (super-spreaders or hubs), the "well-mixed assumption" drives to diverging results from the real ones. Therefore, the network models exposed in \autoref{ch:network-models} try to capture the essence of the lockdown by considering the basic principles of the social interactions. This, in turn, drives to a sub-exponential growth of $\pi(t)$ even at early times, which is in direct contrast with the mean-field compartmental models (cf. \autoref{ch:sir-models}) . To appreciate this difference, the evolution on a network and on a "Annealed Mean-Field Network" (see \autoref{sec:Annealed_MF_Network}) have been plotted in the same figure (cf. \autoref{ch:network-models}).

\textbf{Infection Scheme}

As in \cite{Thurner::NetBasedExpl}, initialize the $N = 1000$ total individuals as susceptible and randomly chose $i(0) = 10$ vertexes as the starting infectors. Moreover, select an "epidemic", i.e. fix $\beta, \mu$, alongside with the average number of contacts $\langle k \rangle =:D$ and $p$\footnote{The fraction of long-range contacts} in the underlying network.

The infection scheme, equal for both the mean-field (AMFN) and the network models, is the following.
At every time-step $t$, find all the infected nodes and infect the susceptible neighbors with probability $\beta$ (transmissibility or microscopic spreading rate). After the "infectious" phase, at the same time-step $t$, simulate their recovery with probability $\mu = 1/d$ (recovery rate), where $d$ are the average days an individual stays infected.

\noindent\fbox{%
    \parbox{0.97\textwidth}{%
	\textit{To implement the stochasticity nature of an epidemic, generate a random number between $q \in [0,1]$ and infect the chose susceptible neighbors as $q\leq \beta$. Similarly, for the recovery transition with $\mu$.}
    }%
}

For every $t$, collect the "new daily cases" $C(t)$\footnote{$C(t) = N(i(t) - i(t-1) + r(t) - r(t-1))$: for this two states spreading \newline $(I(t-1),S(t-1)) \to (R(t),I(t))$, $C(t) = 1$ as $i(t)-i(t-1)=0$ but $r(t)-r(t-1) = 1/N$.} alongside with their (normalized\footnote{It has been chosen not to normalize the "new daily cases" since are $O(1)$ while $N\sim O(3)$}) "cumulative sum" $\pi(t):=\sum_{\bar{t}\leq t} C(t)/N$, as shown in the panels in \autoref{sec:ATechIntro} reporting the real collected data. Proceed iterating the infection chain, until the dynamic comes to a halt, i.e. $i(\infty)=0$, while $s(\infty)\neq0, r(\infty) \neq0$.

After having collected $C(t) \textnormal{ and } \pi(t)$ also for the "Annealed Mean-Field Network (AMFN)" (see \autoref{sec:Annealed_MF_Network}), it is produced a plot with the joined $C(t) \textnormal{ and } \pi(t)$ from a specific network, e.g. Watts-Strogatz, and the AMFN. Moreover, there would be a series of plots for each model, since there are some free parameters to be fixed: $ D = \langle k \rangle, p, \beta,\mu$. Thanks to this method, there could be gained more insights on the departure of the network spread from the mean-field SIR.

Ultimately, the recovered percentage are high or negligible? Even if the $ 100 \%$ of the population would be infected it would still be really small, by a direct comparison with the COVID-19 growth factor: \autoref{fig:USA-AUT-ITAtotalcasesOWID} suggests that $ max/min |_{USA} \sim 10^{5}$ implies that for 10 infected nodes the maximum is expected to be at $ max \sim 10^{6} = 10^{8} \%$. 

{\textbf{Relevant Quantities on Adjacency Matrix Plots}  }
%\label{sec:RegLat_p0}
\begin{figure}[h]
	\centering
	\includegraphics[width = .5\linewidth]{Results/WS_Pruned/AdjMat/WS_Pruned_AdjMat_1000_500.0_0.0.png}
	\caption{Lattice Graph panel for $D = 500$. On the upper left, there is the graphical realization of the network, characterized by a fan-shaped structure of connections for each node. On the upper right, the adjacency matrix; while, on the bottom, the probability distribution.
	$N_{3-out} \textnormal{ and } SW_{C}$ are, respectively the number of nodes whose degree is $k>3D$ and the coefficient of small-worldliness $SW_{C} := \delta / \ln(N)$ where $C$ stands for "connected" when the network is connected. Instead, the size of the largest component is going to be returned.} 
	%\label{fig:RegLatAdjMatrix}
\end{figure}

\textbf{Relevant Quantities on SIR Plots.}\\
The SIR plots, e.g. \autoref{fig:sir_CM_D5_ORL1}, display from left to right: $R_0 \textnormal{ and } R_{c-net}$; the average degree $D$ with at the subscript $N$; the rewiring probability $p$ and the epidemic parameter $\beta \textnormal{,} \mu$. The errors are reported in the rounded brackets.
On the other hand, in the legend there are present: $P(t) \textnormal{ or }\pi(t)$ of the mean-field and the network evolutions; $ C(t)$ of both the AMFN and the peculiar model; the estimated maximum ($\bigstar$) both of the mean-field and the network epidemics; the percentage of the starting infected nodes. 
Furthermore, the difference $ \Delta R_0 := R_0 - R_{c-Net}$; the $ \langle k^2 \rangle$; the average path length $ \delta_C$; the division by the average path length of the previous quantities and the Order Parameter $ SD(C(t))$.

Formally, the estimation of the $\bigstar$ comes from \autoref{eqs:SIR_degree-based}, which yields $R_0 s(t) \stackrel{!}{=} 1$: by interpolating with a straight line the two points obtained by the simulations, yields to $(t_{critical}, p_{critical})$. From that time on, the number of cases are going to decrease since $R_0 \cdot s(t) < R_{c-net}$, as the susceptible are no more feeding the pathogen which is going to die out. In particular, \autoref{eqs:SIR_degree-based} is valid at every time only for the AMFN case. Thus, the orange start is always right; while the green one approximates the maximum of the $ i(t)$ only if the pathogen is strong or the network resembles a AMFN.

In addition, the "network effectiveness" $\frac{1}{\delta}$ capture the fact that as long-range nodes increase the epidemic is favored, since there are allowed the "jumps" into a new pool of susceptible.
So, $\Delta R_0(\delta)$ is proposed as a better estimation of the severity of an epidemic.

\textbf{Non trivial dependence of $ t_{max}$ and $ \sigma(t_{max})$.} \\
Surely, $\beta \textnormal{ and } \mu$ are well classified as increasing and reducing the number of infected. Rather, $D \textnormal{ and } p$ could act differently with respect the underlying topology. Moreover, severe epidemics could reach herd immunity with different growth rate. So, there is no clear dependence on the role of $D \textnormal{ and } p$ parameters. Indeed, in \autoref{fig:sir_O-PSW_COVID}, $t_{max}$ diminishes in the first increase of $D$ in the last figure, is increased.

The second one, namely the variance $\sigma(t_{max})$, embodies the "degree of stochasticity" of the different trajectories displayed in pale green (network-based) and orange (mean-field): it is reported after the $\pm$ sign in the legend. Specifically, it diminishes with the increasing of the epidemic strength, as the pathogen forces the number of total cases to be roughly the same. Its intrinsic dependence on the parameters $D, p, \beta, \mu$ is highly non trivial, but it could be seen it increasing whenever there is room, allowing different behavior: for roughly $\textnormal{total number of cases} \sim 0.5$ a plethora of behaviors are allowed.

\textbf{Order Parameter at work} \\
The theoretically estimated $D_c$ are 3, one for the three model applied: homogeneous mean-field approximation, ER-model, fuse-model (cf. \autoref{eq:D_c_analytical_discussion}).
As a pedagogical remark, note that $D_{c-homog model} < D_{c-ER model} < D_{c-fuse model}$, since, as the above plots show, it easier to exponentially grow in a random network than following local paths (fuse model). In addition, the fuse model, in many cases, overestimate the $D_c$, since the theoretical approximation overestimates the number of infected. Other sources of divergence from the simulated critical degree may be the finite size effects or deviation from the null curvature, formally, $\mathcal{k} = 0$ (see \autoref{sec:D_c_fuse_model}). Thus, instead of a exact prediction the estimated $ D_c$ allow to put an upper and a lower bound on the $D_c$ of the studied networks. Indeed, the ER case provides a good comparison with an homogeneous approximation, as for networks with $ p \neq 0$ ; while the "fuse model" is on the other extrema where the nodes overlaps only locally, i.e. the regular lattice case reported in \autoref{sec:RegLat_p0}.

Ultimately, the difference with \autoref{fig:ER_Epidem_Thr} is that, even if similar in shape, $D$ is driving the evolution of the $SD(C(t))$ instead $ \beta$ driving $ i(t_max)$. A possible improvement should consider to plot $ i(t_{max})$ as driven by $ D$.

\textbf{Parameters Discussion} \\
According to \cite{Liu::2021_Review_SContactPattern} (2021), the most relevant studies \footnote{These works are extracted from PubMed, Medline, Embase and Google Scholar} on social contact patterns report $2-5$ average contacts per day during the lockdown, i.e. nearly the household size. This estimate is, roughly, $5$ times less than the pre-COVID-19 ones of $7-29$ average contacts per day. On the other hand, Mossong et Al. (\cite{Mossong:2008_preCOVID-europe_SCP}) (2008), based on $8$ European states, reported a pre-COVID-19 average of $13$ contacts per day, while Leung et Al. \cite{Leung:2017_HKSocialCP} (2017) find an average per day contacts of $8$ individuals for the habitants of Hong Kong. Moreover, the quoted surveys have taken into consideration individuals of different ages, with the further possibility of determine how society could be clustered by age, e.g. elderly are likely in contact with children.
The present thesis does not aim at guessing a balanced average of contacts per day but, anyway, its purpose is to be grounded on real estimates. Thus, the main plots are going to be focused on the $D < 30$ regime, especially on $ D = 6,11,14,24$. Little room is going to be left for $D > 30$ as it can be the case for leisure activities, e.g. exhibitions or discotheques. Since COVID-19 spreads over face-to-face interactions, the latter case is crucial for the re-opening of the crowded activities.

The epidemic parameters are chosen as $ \beta = 0.015$ (\cite{Thurner::NetBasedExpl}) and $ d = 14$ (\cite{LaurerSA:2020_IncPeriodCOVID-19}). Then, in order to capture different scenarios, $ \beta = 0.1$ or $ d = 4$ while preserving the other parameters, e.g. $ D,\cdots$ . Nevertheless, transmissibility $\beta$ and recovery rate $\mu = d^{-1}$ depends on many factor and especially on the studied population. Therefore, to have a broad inspection of the capabilities of a network topology it is worth considering different combinations of $\beta \textnormal{ and }\mu$. 

\chapter{Results}
\label{ch:Results}
In this chapter, after a preliminary analysis on the nature of $R_0$, it is going to be discussed the simulated epidemics on networks. Specifically, the latter are obtained by means of the Degree Based Mean Field (DBMF) SIR model diffusing over the frameworks of chapter \autoref{ch:network-models}. The discussion provides some insights on which topology is better suited in constraining a pathogen among a Regular Graph, Overlapping-PSW, Sparse-PSW, connected Caveman model and a Scale-Free network. In particular, settling as a benchmark the COVID-19 ($\beta = 0.015 \textnormal{ and } d = 14$), there are further analyzed the cases where $\beta = 0.1$ or $ d = 4$ while the other parameters are left untouched. %Thus, discovering that the "Caveman Model" is better suited to face the problem of the epidemic burden.

The networks, in the following, are all composed by $N = 1000$ nodes and $10$ starting infected individuals. The simulations are performed over the Google Colab CPUs of \cite{GoogleColab}.

\section{Lattice Graph}
\begin{figure}[H]
	\begin{subfigure}[t]{0.48\linewidth}
		\centering
		\includegraphics[width = 0.9\linewidth]{Results/WS_Pruned/AdjMat/WS_Pruned_AdjMat_1000_500.0_0.0.png}
		\caption{Lattice Graph panel for $D = 500$} 
		\label{fig:net_RegLat_D500}
	\end{subfigure} 
	\hfill
	\begin{subfigure}[t]{0.48\linewidth}
		\centering
		\includegraphics[width = 0.9\linewidth]{Results/WS_Pruned/AdjMat/WS_Pruned_AdjMat_1000_500.0_0.3.png}
		\caption{Regular graph with $D = 500$ with $p = 0.3$.
		The shadowed center of the graphical representation (upper left) signals the presence of shortcuts connecting distant nodes.}
		\label{fig:net_RegLat_D500_p0.3}
	\end{subfigure}
	\caption{Regular Lattice Network for $ D = 500$ }
	\label{fig:net_RegLat}
\end{figure}
The goal of this section is to test whether $R_0$ is a suitable quantity to estimate the "strength" of an epidemic among different network models. In fact, despite its media power, $R_0$ represents only the number of secondary cases in the early time-steps: the overall severity of a epidemic is driven by a more involved definition of $R_0$. From this, the proposal of a new quantity $\Delta R_0(\delta):= \frac{R_0 - R_{c-Net}}{\delta}$ which accounts for the network "effectiveness" $1/\delta$. 

The work-flow is the following. Starting from a fully connected regular graph, i.e. $D = 999, p=0$, $R_0$ remains constant by dividing $D$ and doubling $\beta$ while keeping fixed $ \mu$. In formulas, $R_0 = const = (D/q) \cdot (\beta q) / \mu$ where $q$ goes from 1 to the maximum power of $2$ s.t. $D/q > 1$.
Specifically, to generate a fully-connected graph it has been used a "Watts-Strogatz" algorithm with $p = 0$, so called "Regular WS". Since this algorithm works only with even $D$, all the uneven $D$ where, arbitrarily, rounded to the nearest minor even integer obtaining that  $D \in [999,500,124,62,30,14,6,2]$. Furthermore, there are also developed the epidemics on a Watts-Strogatz graph with short-cut probability $p = 0.3$ (called "Long-Range WS") in \autoref{sec:RegLat_p0.3}. The network panel in \autoref{fig:net_RegLat} show a WS network with $D = 500$ for both $ p=0 \textnormal{ and } p=0.3$.\\

This network assumes a fan-shaped structure of contacts, more open for higher $D$s (see \autoref{fig:net_RegLat}), similarly to \autoref{fig:net_O-PSW}. Thus, the number of new daily cases is limited (blue curve) and, so, it is the number of total cases (green one).\\
Finally, since $\beta \textnormal{ and } D$ are arranged in a peculiar way, it is not reported the behavior of the $SD(C(t))$ Order Parameter. 

\subsection*{Regular Lattice graph}
\label{sec:RegLat_p0}
\begin{figure}[h]
	\includegraphics[width = \linewidth]{Results/WS_Pruned/p0.0/WS_Pruned_SIR_R0_3_N1000_D2.0_p0.0_beta0.105_d14.0.png} %../images/
	\centering
	\caption{Network SIR model benchmarked with the AMFN for $R_0 > R_{c-Net}$. A Caveman model with $ On-Ring-Links = 1$ \autoref{fig:sir_CM_D5_ORL1} exhibits an outbreak size of $4.3\%$ for a clique of $ D = 5$. Arranging the nodes in families, reduces the diffusion.}
	\label{fig:sir_RegLat_D2_p0}
\end{figure}
In this subsection, the SIR curves for a local ( $ p = 0$ ) lattice graph is reported while compared with a Long-Range WS (see \autoref{sec:RegLat_p0.3}).

As a starting point, the \autoref{fig:sir_RegLat_D2_p0} shows that for $R_0 > R_{c-net}$ the epidemic remains limited to a much smaller fraction of the population ($4.3 \%$ of the total population) than the AMFN case. Indeed, the epidemic is just spreading on a 1D lattice, e.g. a fuse, with $ D = 2$ (only nearest neighbors are connected) meanwhile the mean-field behaves randomly on the fuse. Thus, the stochasticity of an epidemic of a AMFN is slightly larger than the network one, since $ D$ is still low. Moreover, the increasing value of the azure dots, representing the value of $ i(t)$, implies that the epidemic has started but the fuse model is strongly containing the pathogen. Then, by fixing $R_0 \sim 2.94$, the evolution of the  curves exhibit a growth in the total cases as in \autoref{fig:sir_RegLat_D61430}.
Moreover, \autoref{eq:lambdac_Rc_netSIR} implies that $R_{c-net} = (1-D^{-1})^{-1}$ decreases when $D$ increases. As expected, the critical basic reproduction number is more likely to be overtaken when a regular lattice has more neighbors.

Strengthening the relationship among nodes, namely increasing $ D$, some peculiar behaviors arise.
Starting from the lowest $D = 6$, \autoref{fig:sir_RegLat_D6} captures the (pseudo-)linearity since the early stages of the contagion.
The final outbreak is $ 3$ times higher than the $ D=2$ cases and so are the fluctuation over the mean. The explanation is that the individuals are spreading the disease more effectively but there is also more room for casual trajectories far from the mean. Note that $D = 6$, is slightly above the upper bound of $2-5$ lockdown contacts per person \cite{Liu::2021_Review_SContactPattern}. 
So, even in a lockdown restriction, this network diffuses the agent to $ 13\%$ of the total population. 

Is this percentage high or negligible? This fraction is really small compared to COVID-19 growth factor: \autoref{fig:USA-AUT-ITAtotalcasesOWID} suggests that $ max/min|_{USA} \sim 10^{5}$ implies that for 10 infected nodes the maximum is expected to be at $ max \sim 10^{6} = 10^{8} \%$.

By continuing enhancing $ D$, the network becomes more tangled. Thus, the epidemic diffuses more rapidly showing that the $ R_0$ is informative only to early times. Rather, $\Delta R_0(\delta)$ seems a more subtle quantity to account for the epidemic severity, since it augments as $D$ increases.
Moreover, in \autoref{fig:sir_RegLat_D30}, it is possible to recover that when the pathogen is favored by the network structure, also the estimated maximum of $ i(t)$  starts to resemble the mean-field one.

\clearpage
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.9\linewidth}
        \centering
        \includegraphics[width=0.97\linewidth]{Results/WS_Pruned/p0.0/WS_Pruned_SIR_R0_3_N1000_D6.0_p0.0_beta0.035_d14.0.png} 
        \caption{Regular Lattice with $D = 6 \textnormal{ and } \beta = 0.036$. The number of cases are increasing from \autoref{fig:sir_RegLat_D2_p0}} 
		\label{fig:sir_RegLat_D6}
    \end{subfigure}
	\vfill
    \begin{subfigure}[t]{0.9\linewidth}
        \centering
        \includegraphics[width=.97\linewidth]{Results/WS_Pruned/p0.0/WS_Pruned_SIR_R0_3_N1000_D14.0_p0.0_beta0.015_d14.0.png} 
        \caption{Regular Lattice with $D = 14$. Linearity is lost.} 
		\label{fig:sir_RegLat_D14}
    \end{subfigure}
    \vfill
    \begin{subfigure}[t]{0.9\linewidth}
        \centering
        \includegraphics[width=.97\linewidth]{Results/WS_Pruned/p0.0/WS_Pruned_SIR_R0_3_N1000_D30.0_p0.0_beta0.007_d14.0.png} 
        \caption{Regular Lattice with $D = 500$. Alongside with the loose of (pseudo-)linearity also there a lot of room for fluctuations. Nevertheless, the latter are similar to the $ D = 14$ spreading, since the upper bound of $ N$ total individuals limit the plethora of possibilities. Every lime-green trajectory could be thought as a realization of the COVID-19 in different parts of the world, as if evolving with these parameters.} 
		\label{fig:sir_RegLat_D30}
    \end{subfigure}
    \caption{Some regular lattice spreading curves, which exhibit a strong dependence on $D$. As $D$ increases, the classical $R_0$ remains constant, while its (proposed) rescaled version ($R_0(\delta):=R_0 / \delta$) varies.}
	\label{fig:sir_RegLat_D61430}
\end{figure}

\clearpage
\subsection*{Long-Range Watts-Strogatz model}
\label{sec:RegLat_p0.3}
As the probability of connecting to distant nodes is switched on, i.e. $p \neq 0$, the "Long-Range" WS model starts to resemble a mean-field network (see \autoref{fig:net_RegLat_D500_p0.3}). This implies an increasing number of total cases: $\pi(t \to \infty) \sim 80\%$ against the $ \sim 12\%$ of the regular case. This is the result, this work was looking for from the \autoref{sec:ATechIntro}: the network really play a role in constraining the disease, yielding a linear growth of the total infected even for big $R_0$ ($R_0 = 2.94$) and a $ D=6$ if not "open" as a AMFN. Note that $ D = 6$s could be thought as the number of contacts in a light lockdown \cite{Liu::2021_Review_SContactPattern}.

The take on message is that social distancing is effective, which could naturally lead to some improbable behavior, such as linearity.

\begin{figure}[p]
    \centering
	\begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=.97\textwidth]{Results/WS_Pruned/p0.3/WS_Pruned_SIR_R0_3_N1000_D2.0_p0.3_beta0.105_d14.0.png} 
        \caption{Remarkably linear regime over barely the entire evolution. As the network accounts for small $ D$ and low rewiring $ p$, the epidemic is flatten similarly to the $ p=0$ case: \autoref{fig:sir_RegLat_D2_p0}} 
		\label{fig:sir_RegLat_D2_p0.3}
    \end{subfigure}
	\vfill
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=.97\textwidth]{Results/WS_Pruned/p0.3/WS_Pruned_SIR_R0_3_N1000_D6.0_p0.3_beta0.035_d14.0.png} 
        \caption{Remarkably, in this regime, the (quasi-)linearity is lost and the epidemic growth is recovered. By contrast to the $ D=2$ case, here the difference with respect to \autoref{fig:sir_RegLat_D6} is relevant.} 
		\label{fig:sir_RegLat_D6_p0.3}
    \end{subfigure}
	\vfill
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=.97\textwidth]{Results/WS_Pruned/p0.3/WS_Pruned_SIR_R0_3_N1000_D14.0_p0.3_beta0.015_d14.0.png} 
        \caption{Watts-Strogatz model for $D = 14 \textnormal{ and } p = 0.3$} 
		\label{fig:RegLat_D14_p0.3}
    \end{subfigure}
    \caption{Watts-Strogatz model spreading curves for $D = 6$ and $D = 14$ and $p = 0.3$ .}
	\label{fig:sir_RegLat_D2614_p0.3}
\end{figure}

\clearpage
\section{Poissonian Small-World Network}
\begin{figure}[ht]
    \begin{subfigure}{.45\linewidth}
        \includegraphics[width = \linewidth]{Results/PSW/AdjMat/NNO_Conf_Model_addE_True_AdjMat_1000_14.0_0.0.png}
        \centering
        \caption{Overlapping PSW degree distribution. }
        \label{fig:net_O-PSW}
    \end{subfigure}
	\hfill
	\begin{subfigure}{.45\linewidth}
        \includegraphics[width = \linewidth]{Results/PSW/AdjMat/NN_Conf_Model_AdjMat_1000_13.0_0.0.png}
        \centering
        \caption{Sparse PSW degree distribution}
        \label{fig:net_S-PSW}
    \end{subfigure}
	\caption{Two kind of PSW network for a "Overlapping" and "Sparse" scheme. The case of $p = 0.3$ is not show for conciseness, but could be straightforwardly obtained by the core $p = 0.0$ case.}
\end{figure}
The NPIs measures constraint the individuals to lower their number of contacts, i.e. $2-5$ contacts \cite{Liu::2021_Review_SContactPattern}, in order to avoid the hubs formation, whose role is to produce a vanishing epidemic threshold \autoref{ch:sir-models}.
Therefore, to capture the absence of hubs while retaining the heterogeneity on the number of contacts, a poissonian SW network is introduced. In addition, the nodes are properly rewired only to the nearest nodes, in order to mimic a lockdown scenario \cite{Thurner::Appendix_NetBasedExpl}. Nevertheless, this arises the choice (\autoref{fig:net_O-PSW}, \autoref{fig:net_S-PSW}) of whether to preserve locality, obtaining a narrower distribution $p_k$, or to fix the poissonian distribution, introducing some long-range connections. Based on the kind of connections, the former would be addressed as the "overlapping-PSW" (O-PSW); while the latter the "sparse-PSW" (S-PSW). To gain a perceiving understanding of this, assume the nodes are arranged on a circle and wire them anti-clock wise from the bottom. If "locality" as to be preserved (\autoref{fig:net_O-PSW}), the rewiring scheme allows that a "left-most" node could wire to a node on its right, whose degree has been already saturated. Therefore, the poissonian distribution is lost. This kind of $ p_k$ resembles the WS one for $ p_k \neq 0$ (\autoref{fig:net_RegLat_D500_p0.3}). The difference is that $ p \neq 0$ drives to a network with long-range interactions; while here only neighborhood is reached.\\
On the other hand, by aiming at preserving $p_k$  (\autoref{fig:net_S-PSW}), the wiring procedure allows to rewire at the "next" available node, which could be also distant from the source neighborhood. Since in \cite{Thurner::Appendix_NetBasedExpl}, there are reported some intriguing behaviors for a PSW but not which of the two kinds, O-PSW and S-PSW are built.

In order to simulate the COVID-19 pandemic, $\beta = 0.015 \textnormal{ and } d = 14$ are chosen from \cite{Thurner::NetBasedExpl} and \cite{LaurerSA:2020_IncPeriodCOVID-19}: $\beta \textnormal{ and } d:= \frac{1}{\mu} \textnormal{ with } \mu \sim 0.07$ are the transmissibility rate and recovery days, i.e. the incubation period. Moreover, a direct comparison with $d = 4$ (fixed $\beta = 0.015$, , see 
\autoref{fig:sir_O-PSW_D24_d4}) alongside with $ \beta = 0.1$ (fixed $ d = 14$, see \autoref{fig:sir_O-PSW_D5_d14_b0.1} )  cases is treated. The exact number of $4$ days as a new recovery time is arbitrary but comparable with the recovery time of a generic flu. Hopefully, this could be the case after a strong vaccination campaign. It is worth recalling that the SIR model does not distinguish among asymptomatic or symptomatic. Therefore, $d = 4$ would characterize both an asymptomatic and an hospitalized infected.

\subsection*{Overlapping PSW}
The goal of this section is to report the typical behaviors of a SIR model on a "O-PSW" network for both $p = 0.0 \textnormal{ and } p=0.3$. The focus would be narrowed to the (pseudo-)linear evolution of the number of total cases for $D < D_c$. For brevity, the former would be called the "regular" OPSW (R-OPSW) while the latter "long range" OPSW (LR-OPSW).\\
Ultimately, there would report also the $SD(C(t))$ order parameter, which is a proper way to highlight the separation among a sub-exponential regime and an exponential growth of $ \pi(t)$.

\subsubsection*{SIR on Regular O-PSW}
The triptych in \autoref{fig:sir_O-PSW_COVID} displays the appearance of the exponential growth of the total number of cases $\pi_{max}:= \pi( t_{max}) \to \infty$ as the average number of contacts $D$ increases. Specifically, $ t_{max}$ is defined as the last time reported for a network evolution of the disease. Moreover, for small $D$, there is not an exponential growth, even at early times where there is a big pool of susceptible. 

As in \autoref{sec:RegLat_p0}, by varying the $D$ parameter, fixing $\lambda:=\frac{\beta}{ \mu}$ , it could be noted that, the "severity" of a disease is better described by the difference $\Delta R_0 (\delta) := \frac{R_0 - R_{c-Net}}{\delta}$ rather than $R_0 $ itself. As a direct comparison, compare the epidemic severity among the \autoref{fig:sir_O-PSW_D5_p0} and \autoref{fig:sir_O-PSW_D14_p0}.\\
In \autoref{fig:sir_O-PSW_D5_d14_b0.1}, there is underlined how increasing the transmissibility also the $ \pi_{max}$ rises.\\
In \autoref{fig:sir_O-PSW_D24_d4}, it is displayed a pathogen faced with medical interventions such that $\mu$ is lower than the COVID-19 one. Even with $D$ enhanced to barely $23$, the epidemic final size is less with respect to \autoref{fig:sir_O-PSW_D11_p0}, due to the smallness of $ d = 4$. The choice of $D \sim 24$ is due to the smallness of $\lambda \sim 0.06$: the epidemic threshold is reached for $D = 16$ and for $D < 24$ the curves are more stochastic. The comparison with \autoref{fig:sir_O-PSW_D5_p0} shows how a new pathogen could behave on the same network.
\begin{figure}[p]
	\centering
    \begin{subfigure}[t]{\linewidth}
        \includegraphics[width = .90\linewidth]{Results/PSW/OPSW/p0.0/NNO_Conf_Model_addE_True_SIR_R0_1_N1000_D5.0_p0.0_beta0.015_mu0.07.png}
		\centering
        \caption{SIR spreading for $D = 5$ with COVID-19 parameters. Note that $R_0 < R_{c-Net}$ showing that the pathogen is naturally extinguishing.}
        \label{fig:sir_O-PSW_D5_p0}
    \end{subfigure}
	\vfill
	\begin{subfigure}[b]{\linewidth}
		\centering
        \includegraphics[width = .90\linewidth]{Results/PSW/OPSW/p0.0/NNO_Conf_Model_addE_True_SIR_R0_2_N1000_D11.0_p0.0_beta0.015_mu0.07.png}
        \caption{SIR spreading for $D = 11$}
        \label{fig:sir_O-PSW_D11_p0}
    \end{subfigure}
	\vfill
	\begin{subfigure}[b]{\linewidth}
		\centering
        \includegraphics[width = .90\linewidth]{Results/PSW/OPSW/p0.0/NNO_Conf_Model_addE_True_SIR_R0_3_N1000_D14.0_p0.0_beta0.015_mu0.07.png}
        \caption{SIR spreading for $D = 14$}
        \label{fig:sir_O-PSW_D14_p0}
    \end{subfigure}
	\caption{SIR spreading with COVID-19 parameters. This is the standard behavior for the epidemic curves as $D$ increases. Therefore, for brevity, it is going to be shown the last (quasi-)linear behavior but not the first exponential one, as done in the second and third figures.
	For $D > 14$ the epidemic curves on network would approximate the mean-field ones, reaching the entire population at the end of the diffusion.}
	\label{fig:sir_O-PSW_COVID}
\end{figure}

\begin{figure}[p]
	\centering
	\begin{subfigure}{\linewidth}
		\centering
		\includegraphics[width = .90\linewidth]{Results/PSW/OPSW/p0.0/Pec_Beh/NNO_Conf_Model_addE_True_SIR_R0_7_N1000_D5.0_p0.0_beta0.1_mu0.07.png}
		\caption{A pathogen with more transmissibility than COVID-19 but same recovery rate.}
		\label{fig:sir_O-PSW_D5_d14_b0.1}
	\end{subfigure}
	\vfill
	\begin{subfigure}[t]{\linewidth}
		\centering
		\includegraphics[width = .90\linewidth]{Results/PSW/OPSW/p0.0/Pec_Beh/NNO_Conf_Model_addE_True_SIR_R0_1_N1000_D24.0_p0.0_beta0.015_mu0.25_SEASINFLUENZA.png}
		\caption{A pathogen faced with medical interventions such that $\mu$ is lower than the COVID-19 one even if the transmissibility has become $ 7$ times higher.}
		\label{fig:sir_O-PSW_D23_d4_p0.3}
	\end{subfigure}
	\vfill
	\begin{subfigure}{\linewidth}
		\centering
		\includegraphics[width = .90\linewidth]{Results/PSW/OPSW/p0.3/Pec_Beh/NN_Conf_Model_SIR_R0_1_N1000_D24.0_p0.3_beta0.015_d4.0.png}
		\caption{The sub-exponential growth for a $D$ slightly above $D_c$ of \autoref{fig:Ordp_OPSW_highmu_p0.3} and $p = 0.3.$ It has been reported here for a direct comparison with the $p = 0$ case. The straightforward comparison with the \autoref{fig:sir_O-PSW_D11_p0.3} support the efficiency of the PIs, as vaccines. Indeed, the present gaining factor is around $2$ while allowing for long-lasting infected ($d = 14$), it is near $3$.}
		\label{fig:sir_O-PSW_D24_d4}
	\end{subfigure}
	\caption{Spread for different epidemic parameters}
	\label{fig:OPSW_COVID_p0.3}
\end{figure}

\clearpage
\subsubsection*{SIR on Long Range O-PSW}
The diffusion of the pathogen for $p = 0.3$ shares the overall shape with the $p = 0$ case, but with an enhancement due to the possibility to jump into a new pool of susceptible. In this way, an infected node may have more susceptible around and could spread easily. Moreover, by fixing the epidemic parameters, the shape of the total number of cases for $D_{p=0} \cdot p$ are roughly the same of $D_{p=0.3}$ when $\pi(t_{max}) \sim 1/2$, since the trajectories have room to develop their peculiar behavior. Other way around, at low $D$ where the outbreak final size is barely the same (see \autoref{fig:sir_O-PSW_D5_p0}, 
\autoref{fig:sir_O-PSW_D5_p0.3}), as stochasticity could drive many trajectories to die out fast. Also for high $D$, it could be observed a similar collapse of $\pi(t_{max})$ for $p = 0.0 / 0.3$  due to the fact the population is fixed to $N$ individuals. For a complete perception of these facts, compare the panel \autoref{fig:OPSW_COVID_p0.3} with \autoref{fig:sir_O-PSW_COVID}.\\
In \autoref{fig:sir_O-PSW_D23_d4_p0.3}, there could be seen how a pathogen is suppressed by the PIs measure ( $ d = 4$ ) even if $ p=0.3$ and $ D = 24$. In particular, the enhancement is small with respect to the other escalations reported.

In addition, when fixing $D, \beta, \mu$, the standard deviation $\sigma(t_{max})$ of the number of total cases for the network is higher for $p = 0.3$ since the pathogen have more possibilities to diffuse.


\begin{figure}[p]
	\centering
	\begin{subfigure}[t]{\linewidth}
		\centering
		\includegraphics[width = .90\linewidth]{Results/PSW/OPSW/p0.3/NNO_Conf_Model_addE_True_SIR_R0_1_N1000_D5.0_p0.3_beta0.015_mu0.07.png}
		\caption{Diffusion in a network with $p = 0.3$ fraction of long-range nodes. It does not differ as \autoref{fig:sir_O-PSW_D5_p0} since $D_{p=0} \cdot (1+p) \sim 6.5$. Note that $R_0 < R_{c-Net}$ showing that the pathogen is naturally extinguishing.}
		\label{fig:sir_O-PSW_D5_p0.3}
	\end{subfigure}
	\begin{subfigure}[t]{\linewidth}
		\centering
		\includegraphics[width = .90\linewidth]{Results/PSW/OPSW/p0.3/NNO_Conf_Model_addE_True_SIR_R0_2_N1000_D11.0_p0.3_beta0.015_mu0.07.png}
		\caption{$\pi(t_{max})$ is similar to the one for $D_{p=0} \cdot p \sim 14$ \autoref{fig:sir_O-PSW_D14_p0}. Note how, if limited at the first hundred days, the epidemic produce a total number of infected increasing, roughly, linearly in time. This is a common feature for $D < D_c$.}
		\label{fig:sir_O-PSW_D11_p0.3}
	\end{subfigure}
	\begin{subfigure}[t]{\linewidth}
		\centering
		\includegraphics[width = .90\linewidth]{Results/PSW/OPSW/p0.3/Pec_Beh/NNO_Conf_Model_addE_True_SIR_R0_7_N1000_D5.0_p0.3_beta0.1_mu0.07.png}
		\caption{SIR diffusion for $ D = 5$ but $\beta$ risen. Note, how $\pi(t_{max})$ reach $\sim 2.3$ times the total infected of the $D_{p=0}$ case.}
		\label{fig:sir_O-PSW_D5_p0.3_b0.1}
	\end{subfigure}
	\caption{Overlapping PSW spread for COVID-19 parameters with $p = 0.3$ }
\end{figure}

\clearpage
\subsubsection*{Order Parameters}
\begin{figure}[t]
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth, keepaspectratio]{Results/PSW/OPSW/OrdParam/p0.0/NNO_Conf_Model_addE_True_ordp_p0.0_beta0.015_mu0.07.png} 
		\caption{OP with fixed COVID-19 parameters, $D_c \sim 14$ as shown in \autoref{fig:sir_O-PSW_COVID}.}
		\label{fig:Ordp_OPSW_COVID-19}
	\end{subfigure}
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth, keepaspectratio]{Results/PSW/OPSW/OrdParam/p0.0/NNO_Conf_Model_addE_True_ordp_p0.0_beta0.015_mu0.25.png}
		\caption{OP with higher $D_c$ since recovery rate is enhanced. $D_c \sim 20$.}
		\label{fig:Ordp_OPSW_highmu_COVID19}
	\end{subfigure}
	\caption{OP for Regular OPSW with different recovery rates at $p = 0$ }
\end{figure}

\begin{figure}[t]
	\begin{subfigure}{0.48\linewidth}
		\includegraphics[width=\linewidth, keepaspectratio]{Results/PSW/OPSW/OrdParam/p0.3/NNO_Conf_Model_addE_True_ordp_p0.3_beta0.015_mu0.07.png}
		\caption{Order Parameter dependence on $D$ with fixed COVID-19 parameters}
		\label{fig:Ordp_OPSW_COVID-19_p0.3}
	\end{subfigure}
	\begin{subfigure}{0.48\linewidth}
		\includegraphics[width=\linewidth, keepaspectratio]{Results/PSW/OPSW/OrdParam/p0.3/NNO_Conf_Model_addE_True_ordp_p0.3_beta0.015_mu0.25.png}
		\caption{Higher $D_c$ since recovery rate is higher than the transmission rate}
		\label{fig:Ordp_OPSW_highmu_p0.3}
	\end{subfigure}
	\caption{OP for Long Range OPSW of different pathogens and recovery rates.}
\end{figure}

In this subsection, there are going to be displayed the behavior of the order parameters (OP) developed in \autoref{sec:OrderParam4LinContagion}, namely the standard deviation of the total number of cases $SD(C(t))$ . More specifically, the evolution of $SD(C(t))$, driven by $D$, marks that there is a critical degree of contact $D_c$ above which the constant rate of daily new infected is lost and, thus, an exponential growth appears. The theoretically estimated $D_c$ are 3, one for the three model applied: homogeneous mean-field approximation, ER-model, fuse-model (cf. \autoref{eq:D_c_analytical_discussion}).
As a pedagogical remark, note that $D_{c-homog model} < D_{c-ER model} < D_{c-fuse model}$, since, as the above plots show, it easier to exponentially grow in a random network than following local paths (fuse model). In addition, the fuse model, in many cases, overestimate the $D_c$, since the theoretical approximation overestimates the number of infected. Other sources of divergence from the simulated critical degree may be the finite size effects or deviation from the null curvature, formally, $\mathcal{k} = 0$ (see \autoref{sec:D_c_fuse_model}). Nevertheless, a good estimation of $ D_c$ could be obtained in $ p \neq 0$ case: the model is fairly approximated by the mean-field theory.
Finally, a similarity could be understood since these graphs resemble the \autoref{fig:ER_Epidem_Thr} where the number of total cases is exhibiting a phase transition as $\beta$ grows. Here, the dependence is changed, as $D$ is driving the evolution of the $SD(C(t))$.

\textbf{Regular O-PSW Order Parameter}

In the following panel, it is reported how the $D_c$ behaves by changing the epidemic parameters. The first figure exhibits the COVID-19 transition from linear to exponential growth; while the latter embodies a different agent more infective but also less severe, e.g. a diffuse light flu. As $ \mu$ is strong enough to recover one individual in $ 4$  time-steps, e.g. reachable per a strong vaccination campaign, $D_c$ grows as a signal that exponential growth is more difficult. Indeed, the contagious individuals have to surely infect some neighbors in $4$ time-steps, unless they are going to be neutralized by gaining immunity.  

\textbf{Long Range O-PSW Order Parameter}
The order parameters displays the fact that, switching on the long-range interactions, the epidemic acquires enough strength to ease the exponential growth. Thus, $D_c$ is low and still depends from the epidemic parameter, in similar manner of before. Nevertheless, here even if $\beta$ is changed the transmissibility of the pathogen is not strong enough to prevent a fast recovery of the individuals. Thus, taking to a small number of daily cases, which is reflected by a small order parameter for many $D$.
The enhancement of the recovery rate could be thought as an improvement for the pharmaceutical interventions to support an ill person. Since $ \mu = 1$, the recovery acts in $1$ day, which could be the case for a different pathogen than COVID-19 or, hopefully, reached by a strong vaccination campaign. It is worth recalling that the SIR model does not distinguish among asymptomatic or symptomatic. Therefore, $mu = 1$ is the same for a person who is contagious but without symptoms or with an hospitalized one. This fact support the idea of recovering from a pathogen in $1 day$, e.g. a temporary cold. 

\newpage
\subsection*{Sparse PSW}
This section aims at describing the diffusion of a pathogen, described by a SIR model, on a "S-PSW" network for both $p = 0.0 \textnormal{ and } p=0.3$. Inheriting the previous notation, the former would be called the "regular" S-PSW (R-SPSW) while the latter "long range" S-PSW (LR-SPSW).
This type of network reinvigorates the agent with respect to the O-PSW, as the underlying topology of S-PSW accounts for long-rage edges even at $p=0.0$  (cf. \autoref{fig:net_S-PSW}). As before, there is going to be reported the behavior of the order parameter at the end of the subsection.

\subsubsection*{SIR on Regular S-PSW}
As quoted before, the panel in the next page shows how the pathogen diffuse more easily than in \autoref{fig:sir_O-PSW_COVID}. The collected epidemic curves are the ones from $D = 5$ to $D = 14$ (\autoref{fig:sir_SPSW_COVID}), since at lower $D$, it is understood that the stochasticity is going to play a relevant role in blocking the diffusion; while on higher $D$ the exponential growth is recovered.

As a first remarkable result, the $ D = 11$ drives to $ \pi_{max}$ which is higher of a $ 1.5$ factor with respect to the one in \autoref{fig:sir_O-PSW_D11_p0}. Thus, the extension of the linearity depends on the arrangement of the susceptible around the infect nodes. Indeed, the evolution, in a OPSW, bends nearly at the same time but with less infected, since the "local" interactions does not sustain the linear growth. However, all the curves with a $D \lesssim D_c$ develop a pseudo linearity in the first part of the evolution (\autoref{fig:sir_O-PSW_D11_p0.3}). Then, they bend reaching the herd immunity.

\begin{figure}[H]
	\centering
	\begin{subfigure}{0.8\linewidth}
		\includegraphics[width = \linewidth]{Results/PSW/SPSW/p0.0/Pec_Beh/NN_Conf_Model_SIR_R0_1_N1000_D23.0_p0.0_beta0.015_d4.0.png}
		\caption{(pseudo-)Linear growth of the $\pi(t)$ for $d = 4$, e.g. after a COVID-19 vaccination campaign.}
		\label{fig:sir_SPSW_D23_beta0.015_d4}
	\end{subfigure}
	\par\medskip
	\begin{subfigure}{0.8\linewidth}
		\includegraphics[width = \linewidth]{Results/PSW/SPSW/p0.0/Pec_Beh/NN_Conf_Model_SIR_R0_6_N1000_D5.0_p0.0_beta0.1_d14.0.png}
		\caption{Evolution of a disease with same recovery rate of \autoref{fig:sir_S-PSW_D5_p0.0} but an augmented $\beta$. The network provides less "stability" in containing the epidemic with respect to the O-PSW (see \autoref{fig:sir_O-PSW_D5_d14_b0.1})}
		\label{fig:SPSW_b0.015_d14}
	\end{subfigure}
\end{figure}

\begin{figure}[p]
	\centering
	\begin{subfigure}{\linewidth}
		\centering
		\includegraphics[width = 0.9\linewidth]{Results/PSW/SPSW/p0.0/NN_Conf_Model_SIR_R0_1_N1000_D5.0_p0.0_beta0.015_d14.0.png}
		\caption{SIR spreading for $D = 5$ and COVID-19 parameters as in \autoref{fig:sir_O-PSW_D5_p0}. As in that case, $R_0 < R_{c-Net}$ provides a disease, which naturally dies out in time.}
		\label{fig:sir_S-PSW_D5_p0.0}
	\end{subfigure}
	\vfill	
	\begin{subfigure}{\linewidth}
		\centering
		\includegraphics[width = 0.9\linewidth]{Results/PSW/SPSW/p0.0/NN_Conf_Model_SIR_R0_2_N1000_D8.0_p0.0_beta0.015_d14.0.png}
		\caption{The evolution for a larger number of contact $D \sim 8$. The linearity is preserved longer with respect to the $p = 0.3$ case.}
		\label{fig:sir_S-PSW_D8_p0.0}
	\end{subfigure}
	\vfill
	\begin{subfigure}{\linewidth}
		\centering
		\includegraphics[width = 0.9\linewidth]{Results/PSW/SPSW/p0.0/NN_Conf_Model_SIR_R0_2_N1000_D10.0_p0.0_beta0.015_d14.0.png}
		\caption{The evolution for a larger number of contact $D = 10$. This is slightly above the threshold, implying the exponential outbreak at early times. As compared to the homologous in \autoref{fig:sir_O-PSW_D11_p0} there is a sensible enhancement (nearly of a $1.7$ factor of the total cases), due to the different network topology.}
		\label{fig:sir_S-PSW_D10_p0.0}
	\end{subfigure}
	\caption{Sparse PSW spread for COVID-19 parameters with $p = 0.0$ }
	\label{fig:sir_SPSW_COVID}
\end{figure}

\newpage
\subsubsection*{SIR evolution on Long Range S-PSW}
For brevity, it is shown only a SIR evolution (\autoref{fig:sir_SPSW_p0.3_D7}) where the linearity is supported by the "long-range" interactions as in the cases seen before. All the "linear" curves share a number of total cases approximatively from $10\% - 20\%$. Moreover, these two figures are especially selected in order to display the relationship among the epidemic parameters with the network ones: allowing for a faster recovery, the average number of contact has to be enhanced to obtain a strong epidemic.

\begin{figure}[h]
		\includegraphics[width = \linewidth]{Results/PSW/SPSW/p0.3/NN_Conf_Model_SIR_R0_1_N1000_D7.0_p0.3_beta0.015_d14.0.png}
	\caption{(Quasi-)Linearity for the last of simulated epidemics. An evolution for $D > D_c \sim 7$ would exhibit a exponential growth as seen in the transition \autoref{fig:sir_O-PSW_COVID}.}
	\label{fig:sir_SPSW_p0.3_D7}
\end{figure}

\begin{figure}[h]
	\includegraphics[width = \linewidth]{Results/PSW/SPSW/p0.3/NN_Conf_Model_SIR_R0_1_N1000_D24.0_p0.3_beta0.015_d4.0.png}
\caption{(Quasi-)Linearity for the same transmissibility of COVID-19 but less days of recovery. Increasing to $D > D_c \sim 24$, the curve would exhibit an exponential growth of the number of total cases.}
\label{fig:sir_SPSW_p0.3_D24}
\end{figure}


\newpage
\subsubsection*{Order Parameters}
In this subsection, it is going to be shown the evolution of $SD(C(t))$ driven by $D$, alongside with $D_{c-homog \, model} < D_{c-ER \, model} < D_{c-fuse \, model}$. The precise OP quantity could be find among \textit{"Other Measures"}: a panel inside every SIR spreading figure. Moreover, the critical degree $D_c$ is difficult to estimate at a plain eye inspection, since the transition among the linear-to-exponential is not so sharp. On the other hand, by looking at the SIR simulations, $D_c$ could be defined as the point, which $SD(C(t))$ grows with a different slope for.
For example, in \autoref{fig:Ordp_SPSW_COVID}, it could be seen that the departure from a pseudo-linearity is expected at $D \sim 10$.

\textbf{Regular and Long-Range S-PSW}

\autoref{fig:Ordp_SPSW_p0.3} displays two evolution of the OP similar to the $p = 0.0$ case.
Switching on the long-range interactions, the epidemics acquire enough strength to ease the exponential growth. Nevertheless, $p = 0.3$ produce a light modification in the number of daily cases which, in turn, produce a similar behavior of the OP. Thus, it has been used a cross comparison with previously reported SIR evolutions, \autoref{fig:Ordp_SPSW_COVID} and \autoref{fig:Ordp_SPSW_COVID_p0.3}, for a better estimation of in the different $D_c$ circumstances.
\begin{figure}[t]
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{Results/PSW/SPSW/OrdParam/p0.0/NN_Conf_Model_ordp_p0.0_beta0.015_d14.0.png} 
		\caption{OP with fixed COVID-19 parameters. \newline $D_c \sim 8-10$ as in \autoref{fig:sir_O-PSW_COVID}}
		\label{fig:Ordp_SPSW_COVID-19_d14}
	\end{subfigure}
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{Results/PSW/SPSW/OrdParam/p0.0/NN_Conf_Model_ordp_p0.0_beta0.015_d4.0.png}
		\caption{OP exhibiting a higher $D_c \sim 23$ since $\mu$ has been enhanced (cf \autoref{fig:sir_SPSW_D23_beta0.015_d4})}
		\label{fig:Ordp_SPSW_d1.0_COVID19}
	\end{subfigure}
	\caption{Regular OP at different epidemic rates $\mu = 1/d$}
	\label{fig:Ordp_SPSW_COVID}
\end{figure}

\begin{figure}[t]
	\begin{subfigure}{0.48\linewidth}
		\includegraphics[width=\linewidth, keepaspectratio]{Results/PSW/OPSW/OrdParam/p0.3/NNO_Conf_Model_addE_True_ordp_p0.3_beta0.015_mu0.07.png}
		\caption{OP dependence on $D$ with fixed COVID-19 parameters. $D_c \sim 7$ \cite{Thurner::NetBasedExpl} (cf. \autoref{fig:sir_SPSW_p0.3_D7})}
		\label{fig:Ordp_SPSW_p0.3}
	\end{subfigure}
	\begin{subfigure}{0.48\linewidth}
		\includegraphics[width=\linewidth, keepaspectratio]{Results/PSW/OPSW/OrdParam/p0.3/NNO_Conf_Model_addE_True_ordp_p0.3_beta0.015_mu0.25.png}
		\caption{$D_c \sim 23$ as shown for \autoref{fig:sir_SPSW_p0.3_D24}}
		\label{fig:Ordp_SPSW_d4_p0.3}
	\end{subfigure}
	\caption{Long Range OP for different recovery rates.}
	\label{fig:Ordp_SPSW_COVID_p0.3}
\end{figure}

\clearpage
\section{Caveman Model}
\begin{figure}[t]
	\centering
	\includegraphics[width = 0.8\linewidth]{Results/Caveman_Model/AdjMat/Caveman_Model_AdjMat_999_3.2_0.3.png}
	\caption{Adjacency Matrix of a Caveman Model with $1$ link on the ring of communities and probability $p = 0.3$ of long-range wiring. Therefore, \autoref{eq:CavemanMod_D} yields the reported $D$}
	\label{fig:CM_AdjMat_p0.3}
\end{figure}
The most sensible lock-down measure is the household restriction. By limiting the individuals to have contact only within families (or small communities), the pathogen is enclosed into small pools of susceptible and hopefully suppressed, e.g. \autoref{fig:sir_CM_D5_ORL1}. As seen in the previous cases, gaining time is a fruitful occasion to develop vaccines or proper NPIs. 

In order to realize this kind of ordering of nodes, there is a particular model, called the "Caveman Model" which is a regular graph for which nodes are organized in communities/families, the so called caves (see \autoref{sec:RLN-Caveman_Description}). By contrast to the segregation of separate caves, it has been used a "connected" caveman model, since every community is linked to the nearest neighboring one with $ 1 \textnormal{ or } 3$ "on-ring links". To account for this, in the plots there is a quantity called "OnRing - InRing". By introducing this "local" interactions, the underlying assumption is that a family shares with two others only one "risk contacts", e.g. going to the supermarket. This could be really the case whether families are distant or only a small fraction of the entire population is traced by a voluntary usage of a mobile application such as Immuni.

Moreover, it has been introduced the wiring probability $p$ to the distant nodes, to capture a "reopening" of the leisure activities as expected for a light lockdown. All the edges are added to the previously existing connections not to destroy the family structures. Therefore, the topology allows for a higher $D$, which could be estimated by the following formula: 
\begin{equation}
	D \sim N_{cl}-1+2 \cdot p+\frac{2\cdot L_{nx}}{N}
	\label{eq:CavemanMod_D}
\end{equation} 
where $N_{cl}, L_{nx}$ are, respectively, the number of nodes composing a community and the number of links, which the next community is reached with. $L_{nx}$ is so defined to settle, during the implementation, only half of the links to be used to link to a forward clique. As a brief outline, it is going to be treated first the COVID-19 cases ($\beta = 0.015 \textnormal{ and } d = 14$), then, changed one among the new values $\beta = 0.1, \mu = 4^{-1}$ by fixing the old value the other one. More specifically, changing $\beta$ it could be addressed the problem on how the epidemic curves depend on the pathogen itself or by the network topology; while updating $ \mu$ is in the direction of support the human-recovery with PIs measure, such as a vaccine. In this way, it is possible to develop a cross comparison on the epidemic evolutions; while trying to validate the perception that this topology would slow down the pathogen more than the other models.


\clearpage
\subsection*{SIR with Local Interactions}
In this part, there are reported the typical behavior of a pathogen spreading on a Caveman Model with $1$ local link and $p = 0.0$ probability of wiring to distant cliques. In fact, the simulations are conducted for both $1 \textnormal{ and 3}$ on-ring-links, but it has been reported only the $OnRing = 1$ case since $OnRing = 3$ have the same behavior with reaching a higher fraction of population. As seen previously, by fixing the epidemic while increasing $D$, the epidemic grows faster.
\begin{figure}[H]
	%here comes 2 \vbox badness 10000
	\centering
	\begin{subfigure}{0.8\linewidth}
		\includegraphics[width = \linewidth]{Results/Caveman_Model/p0.0/Caveman_Model_SIR_R0_1_N1000_D4.0_p0.0_beta0.015_d14.0.png}
		\caption{A pathogen spreading on a Caveman Model with $D \sim 4.4$, as yielded by the \autoref{eq:CavemanMod_D} for a clique with $5$ individuals and $OnRing-link = 1$. The $D = 2$ ($ N = 3$ household size for European country) evolution has the same shape, while reaching $1.5 \%$ of the population. A SIR evolution with nodes all on a circle is \autoref{fig:sir_RegLat_D2_p0} and reaches a more significant fraction of the population.}
		\label{fig:sir_CM_D5_ORL1}
	\end{subfigure}
	\begin{subfigure}{0.8\linewidth}
		\includegraphics[width = \linewidth]{Results/Caveman_Model/p0.0/Caveman_Model_SIR_R0_2_N990_D10.0_p0.0_beta0.015_d14.0.png}
		\caption{A pathogen spreading on a Caveman Model with $D = 11$}
		\label{fig:sir_CM_D10_ORL1}
	\end{subfigure}
	\begin{subfigure}{0.8\linewidth}
		\includegraphics[width = \linewidth]{Results/Caveman_Model/p0.0/Caveman_Model_SIR_R0_3_N994_D13.0_p0.0_beta0.015_d14.0.png}
		\caption{A pathogen spreading on a Caveman model with $D = 14$}
		\label{fig:sir_CM_D13_ORL1}
	\end{subfigure}
	\caption{SIR evolution on a Caveman Model with "OnRing" = 1}
	\label{fig:sir_CM_COVID}
\end{figure}

\clearpage
\subsection*{Exploring SIR parameter on Caveman Model}
This part is devoted to some peculiar behavior on the Caveman model obtained by a modification of the parameters $OnRing, p, \beta, \mu$. In particular, in \autoref{fig:sir_CM_D5_ORL1}, $\beta = 0.1$ but the other parameters are drawn from \autoref{fig:sir_CM_D5_ORL1}. An example could be a new variant immune to a vaccine developed for the older one. Nevertheless, since the network allows for a localized spread, the overall number of total cases stays small even if the pathogen has $7$ times the transmissibility of the initial agent.

The second figure \autoref{fig:sir_CM_D13_ORL1} has a $d = 4$ while other parameter are in accordance with the \autoref{fig:sir_CM_D23_d4}. This is a paradigmatic situation where a PIs are at work and the mean $ d = 4$ accounts for the fact that both vaccinated and not-vaccinated individuals are present inside a population. Therefore, it is reported a decreasing in the overall infected nodes. 

\begin{figure}[H]
	\centering
	\begin{subfigure}{0.93\linewidth}
		\includegraphics[width = \linewidth]{Results/Caveman_Model/p0.0/Pec_Beh/Caveman_Model_SIR_R0_6_N1000_D4.0_p0.0_beta0.1_d14.0.png}
		\caption{A pathogen stronger than COVID-19 keeping fixed the time to recover, e.g. a new variant immune to a vaccine for a different type of COVID-19.}
		\label{fig:sir_CM_D4_OR1_d14_b0.1}
	\end{subfigure}
	\par\bigskip
	\centering
	\begin{subfigure}{0.93\linewidth}
		\includegraphics[width = \linewidth]{Results/Caveman_Model/p0.0/Pec_Beh/Caveman_Model_SIR_R0_1_N984_D23.0_p0.0_beta0.015_d4.0.png}
		\caption{Fixed the transmissibility coefficient to COVID-19, while decreasing the recovery to $d = 4$, the pathogen spreads less than in \autoref{fig:sir_O-PSW_D24_d4}.}
		\label{fig:sir_CM_D23_d4}
	\end{subfigure}
\end{figure}

\clearpage
\newgeometry{left = 4cm, bottom = 1cm, right = 2cm, top=2cm}
\subsection*{Long Range Caveman model}
Here, there would be analyzed the epidemic curves of a Caveman with both local ("On Ring Links") and long-range interactions by switching on the (long-range) wiring probability $p = 0.3$. Thus, the network has families arranged on a circle, but also distant links due, for example, to work or leisure activities.
Clearly the curves would be enhanced, but at which extend strongly depend on the network. Thus, for a correct interpretation of the curves, it would be done a direct comparison with other topologies for $p = 0.3$. In particular, the tryptic below validate the insight that this model constraint more the epidemic that others, since O-PSW displays $\pi(t)|_{p = 0.3} / \pi(t)|_{p=0}\sim 3$ (cf. \autoref{fig:sir_O-PSW_COVID}).
\begin{figure}[H]
	\centering
	\begin{subfigure}{0.9\linewidth}
		\includegraphics[width = \linewidth]{Results/Caveman_Model/p0.3/Caveman_Model_SIR_R0_1_N1000_D5.0_p0.3_beta0.015_d14.0.png}
		\caption{A pathogen spreading in a Caveman Model with $D \sim 4.4+ 2\cdot p = 5$, according to \autoref{eq:CavemanMod_D}}
		\label{fig:sir_CM_D4_OR1_p0.3}
	\end{subfigure}
	\par\smallskip
	\begin{subfigure}{0.9\linewidth}
		\includegraphics[width = \linewidth]{Results/Caveman_Model/p0.3/Caveman_Model_SIR_R0_2_N990_D11.0_p0.3_beta0.015_d14.0.png}
		\caption{A pathogen spreading in a Caveman Model with $D = 11, p = 0.3$. The rising of $\pi(t)$ is comparable with the one of \autoref{fig:sir_O-PSW_D11_p0.3} but the overall value is nearly $3$ times the present one. This supports the initial idea of constraining more the epidemic.}
		\label{fig:sir_CM_D11_p0.3}
	\end{subfigure}
	\par\smallskip
	\begin{subfigure}{0.9\linewidth}
		\includegraphics[width = \linewidth]{Results/Caveman_Model/p0.3/Caveman_Model_SIR_R0_3_N994_D14.0_p0.3_beta0.015_d14.0.png}
		\caption{A pathogen spreading in a Caveman model with $D = 14, p = 0.3$. The discussion is the same of $D = 11$ one.}
		\label{fig:sir_CM_D13_p0.3}
	\end{subfigure}
	\caption{SIR model with COVID-19 parameters evolution for different average number contact per individual.}
	\label{fig:sir_CM_COVID_p0.3}
\end{figure}

\clearpage
\restoregeometry
\subsection*{Exploring SIR parameters}
On the way to understand more deeply how network interacts with the pathogen, it is worth addressing the questions on how a different epidemic could express and at which extent the recovering of the nodes are relevant on the same underlying model. The former would be analyzed by changing only $\beta = 0.1$; while the latter by fixing $d = 4$. Every network has its peculiar enhancement and reduction for $\beta = 0.1 \textnormal{ and } \mu = 0.25$ which is open to comparisons.
\begin{figure}[H]
	\centering
	\begin{subfigure}{0.9\linewidth}
		\includegraphics[width = \linewidth]{Results/Caveman_Model/p0.3/Pec_Beh/Caveman_Model_SIR_R0_7_N1000_D5.0_p0.3_beta0.1_d14.0.png}
		\caption{A pathogen more contagious than COVID-19 but with same time of recovering, e.g. a new variant immune to a vaccine for a different type of COVID-19. Weighted against \autoref{fig:sir_O-PSW_D5_d14_b0.1}, the suppression is not so high since the pathogen is really strong. This is the effect of a strong epidemic agent, which could be nearly approximated by a "well-mixed" model: there are $p \cdot N \sim 600$ distant edges.}
		\label{fig:sir_CM_D4_p0.3_OR1_b0.1}
	\end{subfigure}
	\par\bigskip
	\centering
	\begin{subfigure}{0.9\linewidth}
		\includegraphics[width = \linewidth]{Results/Caveman_Model/p0.3/Pec_Beh/Caveman_Model_SIR_R0_1_N984_D24.0_p0.3_beta0.015_d4.0.png}
		\caption{Fixed the transmissibility coefficient to COVID-19, while decreasing the recovery to $d = 4$, the pathogen is less diffuse than in \autoref{fig:sir_O-PSW_D23_d4_p0.3}.}
		\label{fig:sir_CM_D23_d4_p0.3}
	\end{subfigure}
\end{figure}


\clearpage
\section{Barabási-Albert Model}
\begin{figure}[t]
	\centering
	\includegraphics[width = 0.5\linewidth]{Results/BA_Model/AdjMat/BA_Model_AdjMat_1000_95.0_0.0_50_50.png}
	\caption{Barabási-Albert Model which generates the hubs according to the preferential attachment and growth mechanisms (see \autoref{sec:BA_model} and \autoref{App:OriginOfPA}). The white square on the top left represents the star graph of $D/2$ nodes: the starting point to build a BA model.}
	\label{fig:BA_model_Network}
\end{figure}

As seen in \autoref{sec:BA_model}, this network topology allows to fit a variety of real networks, since it accounts for the presence of the hubs. As for the description of the "lockdown" restriction, this is not going to be the case, but it is worthwhile to dedicate a section to itself, in order to capture the novelties that hubs could provide with respect a less extended degree distribution such as a binomial one.
Ultimately, this kind of network is going not to be suited for suppressing a pathogen, since it enhances its spreading power. Rather, it would better used for other kind of spreading, e.g. the diffusion of knowledge.

\subsubsection*{SIR evolution}
The panel \autoref{fig:sir_BA_COVID} displays different epidemic curves depending on the average number of contacts $D$. By contrast to the previous evolutions, for $D = 6$ the scale-free topology allows to increase the pathogen power over the mean-field one, as shown in \autoref{fig:sir_BA_D6} and described in \autoref{ch:sir-models}.
After that burst, starting from $D = 10$ on, the "well-mixed" model retakes the lead in spreading the disease. 

An extended linearity, such as frequent as in the other topologies (\autoref{fig:sir_O-PSW_D24_d4}). However, it could be the case when the pathogen is constrained either by a small amount of neighbors (\autoref{fig:sir_BA_D2}) or by a high recovery rate \autoref{fig:sir_BA_d1D47}. More in depth, that figure shows a $D \sim 22 \textnormal{ and } d = 4$, which represents a huge number of average contacts and the smallest period of incubation. Therefore, the scale-free networks could enhance the pathogen strength, as the hubs are the high-ways for diffusion. On the other hand, it is possible to reduce the pathogen settling a vaccination campaign targeting the hubs (doctors, nurses,$\cdots$) or, further, closing the infrastructural hubs, such as airports or cruises.

\clearpage
\begin{figure}[H]
	\begin{subfigure}{\linewidth}
		\includegraphics[width = \linewidth]{Results/BA_Model/p0.0/BA_Model_SIR_R0_0_N1000_D2.0_p0.0_beta0.015_d14.0.png}
		\caption{A pathogen spreading in a BA model with $D = 2$}
		\label{fig:sir_BA_D2}
	\end{subfigure}
	\vspace{.5cm}
	\begin{subfigure}{\linewidth}
		\includegraphics[width = \linewidth]{Results/BA_Model/p0.0/BA_Model_SIR_R0_1_N1000_D6.0_p0.0_beta0.015_d14.0.png}
		\caption{A pathogen spreading in a BA model with $D = 6$. The final outbreak size is far higher that the previous models (see \autoref{fig:sir_O-PSW_D5_p0})}
		\label{fig:sir_BA_D6}
	\end{subfigure}
	\vspace{.5cm}
	\begin{subfigure}{\linewidth}
		\includegraphics[width = \linewidth]{Results/BA_Model/p0.0/BA_Model_SIR_R0_2_N1000_D10.0_p0.0_beta0.015_d14.0.png}
		\caption{A pathogen spreading in a BA model with $D = 10$}
		\label{fig:sir_BA_D10}
	\end{subfigure}
	\caption{SIR evolution on a Barabási-Albert Model}
	\label{fig:sir_BA_COVID}
\end{figure}

\clearpage
\begin{figure}[H]
	\begin{subfigure}{\linewidth}
		\centering
		\includegraphics[scale = 0.15]{Results/BA_Model/p0.0/Pec_Beh/BA_Model_SIR_R0_8_N1000_D6.0_p0.0_beta0.1_d14.0.png}
		\caption{Diffusion on a BA model with $\beta = 0.1$. The straightforward comparison with the \autoref{fig:sir_SPSW_D23_beta0.015_d4}, highlights a pathogen with a remarkable power that with the same underlying parameter it has spread on the entire population with respect to COVID-19. Moreover, the fluctuations of the trajectories over the mean are negligible, yielding a scenario where it is highly improbable not to get the disease.}
		\label{fig:sir_BA_D6_b0.1}
	\end{subfigure}
	\par\bigskip
	\begin{subfigure}{\linewidth}
		\centering
		\includegraphics[scale = 0.15]{Results/BA_Model/p0.0/Pec_Beh/BA_Model_SIR_R0_1_N1000_D22.0_p0.0_beta0.015_d4.0.png}
		\caption{Despite the fast healing of the infected individuals, the network enables a diffusion on a larger fraction of population than "free" previous spreading. Only a vaccination campaign addressed to the hubs could produce a relevant drop in the number of cases.}
		\label{fig:sir_BA_d1D47}
	\end{subfigure}
\end{figure}

\clearpage
\subsubsection*{Order Parameters}
As the network enhances the possibility of a pathogen to diffuse, for the COVID-19 parameters a clear $D_c$ does not exists; meanwhile, taking advantage of the (possible) Pharmaceutical Interventions (PIs), the $d = 4$ case is characterized by a sharper transition among the linear and exponential growth. Only the "mean-field" approximation could capture the behavior of the $ D_c$ since hubs are enhancing the network strength of the pathogen over the AMFN. The "fuse model" clearly deviates from the $ D_c$ estimate. 
\begin{figure}[t]
	\begin{subfigure}[t]{0.48\linewidth}
		\centering
		\includegraphics[width = \linewidth, height = 5cm]{Results/BA_Model/OrdParam/BA_Model_ordp_p0.0_beta0.015_d14.0.png}
		\caption{Linear growth in the $SD(C(t))$ for COVID-19 parameters with no critical degree.}
		\label{fig:Ordp_BA_d14}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\linewidth}
		\centering
		\includegraphics[width = \linewidth, height = 5cm]{Results/BA_Model/OrdParam/BA_Model_ordp_p0.0_beta0.015_d4.0.png}
		\caption{OP for a fast recovery rate $ d = 4$.}
		\label{fig:Ordp_BA_COVID_d1}
	\end{subfigure}
	\caption{Order Parameter for Barabási-Albert Model}
	\label{fig:Ordp_BA_COVID}
\end{figure}


\chapter{Conclusions}


\appendix
\chapter{Appendix - Network Models}

\section{Scale-Free Distributions Details}
\label{sec:SFD_details}
In the following part, the discrete approach, assuming $k = 0,1,2\cdots$, is going to be developed; while only the continuum result would be reported. In fact, the discrete approach provides the ready-to-use formulas handle by a calculator which, by construction, performs discrete operations. On the other hand, the continuum approach is useful for analytically support the calculations and could be obtained similarly to the discrete case.
Formally, by exploiting the constraint on probabilities $p_k = C\cdot k^{-\gamma}$ \[\sum_{k=1}^{\infty} p_k = 1,\] it is possible to recover the closed form \cite{barabasi::2016networkbook}
\begin{equation}
	p_k = \frac{1-p_0}{\zeta(\gamma)}k^{-\gamma}.
	\label{eq:p_scalefree}
\end{equation}
where $p_0 = p_{k=0}$ has been separated, since the power-law diverges for $k=0$.

On the another hand, the continuum formalism drives to \[p(k) = (\gamma-1)k_{min}^{\gamma-1}k^{-\gamma},\] since the the smallest value for the power law to hold is not $1$, as in the discrete approach, but the minimum degree for a node $k_{min}$. 

Formally, $k_{min}$ is the degree for which
\begin{equation}
	\int_0^{k_{min}-1} p(k) dk \stackrel{!}{=} 1/N \qquad \textnormal{or} \qquad P(k_{min}-1) \stackrel{!}{=} 1/N.
	\label{eq:kminrel}
\end{equation}

In the next part, the allowed maximum degree $k_{max}$ would be obtained both for an exponential bounded and a power-law distributions. Through this quantitative insight, it would be shown the dependence of $k_{max}$ on $N$, i.e. the size of the network. 

In particular, the exponential distribution for which \(\int_{k_{min}}^{\infty} p(k) dk = 1\) holds, is \[ p_k = \lambda e^{\lambda k_{min}} e^{-\lambda k} \] where, using \autoref{eq:kminrel}, $k_{min} = 1$.
Furthermore, $k_{max}$ (or the "natural cutoff") is defined to be such that the at most one node $i$ could have a degree $k_i$ larger than $k_{max}$. \newline In other words, the cumulative distribution for degrees greater than $k_{max}$ is $1/N$; and, hence, $k_{max}$ is expected to be the size of the bigger hub.

In formulas, 
\begin{equation}
	\int_{k_{max}}^{\infty} p(k) dk = 1/N \implies k_{max} = k_{min} + \frac{\ln(N)}{\lambda}.
	\label{eq:Expkmax}	
\end{equation}
So, $k_{max}$ would not differ much with respect to $k_{min}$ since $\ln(N)$ strongly reduces contribution due to the size of the graph $N$.
For a Poisson distribution, more effort is needed but the same interpretation holds.

On the other hand, applying the same rationale to a scale-free network, 
\begin{equation}
	k_{max} = k_{min}\,N^{\frac{1}{\gamma-1}}.
	\label{eq:SFkmax}
\end{equation}
In this case, there could be order of magnitude of difference between $k_{max}$ and $k_{min}$.
Indeed, as a simple example, WWW forms a graph of on $N \sim 10^5$ documents with $\langle k \rangle \simeq 4.6$ and $\gamma = 2.1$ \cite{barabasi::2016networkbook}. 
By assuming $\lambda = 1 \textnormal{ and recovering that }, k_{min} = 1$ using \autoref{eq:kminrel}, $k_{max} \sim 14$; while $k_{max} = 95.000$ for a scale-free network with $\gamma = 2.1$, e.g. WWW network. This short results reinforces the quoted insight that for the class of "exponentially bounded distributions" the hubs are strongly forbidden since the nodes have comparably the same degree. Instead, the highly connected nodes are naturally arising by increasing $N$ with an underlying power-law distribution.
\label{sec:SFProperties}

Not all the network are scale-free since the hubs are present only if nodes have unlimited degree capacities.
In fact, there could be constraints, such as cost-benefit problem, which limit $k_{max}$ of the bigger node. In this overview, random network may be the best fit, e.g. national highways network, power grid of generators and switches, $\cdots$

With all the previous effort a more quantitative approach to the explanation on the "scale-free" etymology could be ultimate.
More precisely, defining the statistics momentums as
\begin{equation}
	\langle k^n \rangle := \int_{k_{min}}^{k_{max}} k^n\, p(k) dk
\end{equation}
it is possibile to recover the the mean and the variance for $n = 1,2$.
Thus, for a poissonian distribution, the nodes degree $k$ belong to $\langle k \rangle \pm  \langle k \rangle ^ {1/2}$ interval, which drives to the desired $\langle k \rangle$ scale. At the same time, for a power-law distribution holds	
\begin{equation}
	\langle k^n \rangle \sim \frac{k_{max}^{n-\gamma+1}-k_{min}^{n-\gamma+1}}{n-\gamma+1}
	.
\end{equation}
Therefore, knowing that real networks have typically $\gamma \in (2,3)$, e.g. WWW has $\gamma =  2.1$ \cite{barabasi::2016networkbook}
\begin{equation}
	k \in \langle k \rangle \pm \lim_{N \to \infty} \langle k^2 \rangle = \infty.
\end{equation}
where the hidden limit of \( k_{max} \to \infty \) is understood, since $k_{max}$ increases with the system size. 
So, no scale naturally arises.

The result rapidly quoted before is surrounded by three other regimes 
\cite{Cohen:2003_SFUSW}:

\begin{equation}
	\langle d \rangle = 
	\begin{cases}
		constant & \gamma = 2 \\ \\
		\ln(\ln(N)) & \gamma \in (2,3) \\ \\
		\frac{\ln(N)}{\ln(\ln(N))} & \gamma = 3 \\ \\
		\ln(N) & \gamma > 3
	\end{cases}
	\label{eq:USWdistance}
\end{equation}
\newline
\textbf{Anomalous Regime ($\gamma = 2$)}: Due to $k_{max} \sim N$, the network is forced into a "wheel rim configuration", a central hub with spoke nodes. In this regime, the average path length is independent on the size of the graph.
\newline
\textbf{Ultra-Small World Regime ($2 < \gamma < 3$)}: By predicting a $\ln(\ln(N))$ trend of the average distance, \autoref{eq:USWdistance} guides to the concept of "ultra-small world phenomenon" as a slower growth than random network. Indeed, recalling the worldwide social network, \(\ln\ln(N) \sim 3 \text{ while } \ln(N) \sim 22\) more than few times higher than the expected "six degrees of separation".
\newline
\textbf{Critical Point ($\gamma = 3$)}: Looking back on the \autoref{eq:kn_SFnets}, the variance ($n=2$) is finite, marking the passage between the small-world ($\sim \ln(N)$) and the ultra-small world ($\sim \ln(\ln(N))$) classes of graphs.
\newline
\textbf{Small World ($\gamma > 3$)}:
In this regime, the finiteness of the variance is understood, due to the light aggregation that the high-degree nodes could perform in these kind of networks.

Moreover, recalling the root of "ultra-small world" \autoref{eq:SFkmax}, for $N \lesssim 10^2$ the path length distributions overlap in all the reported regimes and, so, converges to a Poisson distribution ($\gamma > 3)$ whose hubs are small to produce the "ultra-small world" phenomenon. In fact, to validate the power-law nature of a distribution, the nodes degree should be separated by 2-3 orders of magnitude. Hence, reverting the \autoref{eq:SFkmax},
\[N = \left(\frac{k_{max}}{k_{min}} \right)^\frac{1}{\gamma -1},\]
which lucidly reproduce $N \sim 10^2$ for $\gamma = 2.1$ (WWW network).

Thus, \(\langle d \rangle\) is changing with $\gamma$, as the smaller it is, the shorter would be the average distance; but also with $N$, as it is the "ab ovo" hypothesis for having big hubs.

A sociological fact has to be remarked when trying to map the "ultra-small" world over social interactions. Indeed, an online experiment which aimed to replicate the "six degrees" concept find that individuals involved in complete chains, reaching their target, were less likely to send a message to a hub than individuals involved in incomplete chains \cite{Dodds:2003_GSonlineNet_6deg}. The reason may be self-imposed, since hubs are perceived as being busy, so contacted only in real need and avoid to concretely complete an "online experiment" challenge. Thus, network is not everything since actual success depends sensitively on individual incentives.


\newpage
\section{Recovering Power-Law Distribution}
\label{App:RecPLD}
Due to a new $m-$stubs node, the degree of an existing node $i$ changes at a rate
\begin{equation}
	\frac{dk_i}{dt} = m \Pi(k_i) = m \frac{k_i}{\sum_j k_j}
	\label{eq:BA_dk_i/dt}
	.
\end{equation}
Hence, for $t >> 1$, 
\begin{equation}
	k_i(t) = m \left(\frac{t}{t_i}\right)^\beta
	\label{eq:BA_modki(t)}
\end{equation}
where $\beta$, alias "dynamical exponent", has the value $\beta = 1/2$ \cite{barabasi::2016networkbook} and $t_i$ is the entering time for the node $i$.

The remarks, coming from the \autoref{eq:BA_modki(t)}, are multiple:
\begin{itemize}
	\item all the nodes follow the same power law, since $\beta$ is independent on the node label $i$. Furthermore, it is independent on $m$ and $m_0$ constructing parameters;
	\item the inverse dependence of $k_i \sim 1/t_i$ is a clear signal of the fact that old nodes are supposed to be larger, also called "first-mover advantage" in business;
	\item both $\beta = 1/2 < 1$ (sublinear growth) and the rate $dk_i/dt \sim 1/\sqrt{tt_i}$ underly a competing scenarios: the existing nodes compete for the $m-$new stubs with a growing bunch of nodes, resulting in a sublinear growth, which drives to a decreasing rate of acquaintances. 
\end{itemize}

To validate the power-law nature of the probability of having a certain node degree, by exploiting \autoref{eq:BA_modki(t)} it is possible note that the nodes $i$ with a degree higher than $k$ are such that \[t_i < t \left(\frac{m}{k} \right) ^{1/\beta}. \] Thus, for $t >> m_0$, the (cumulative) probability of having a degree less than $k$ is \[P(k) = 1 - \left(\frac{m}{k} \right)^{1/\beta}, \] from which the probability of having a degree $k$ could be recovered, by differentiating $P(k)$ and substituting $\beta = 1/2$, $p_k = 2m^2k^{-3}$ \cite{barabasi::2016networkbook}. As desired, the distribution exhibits a long-tail nature with $\gamma = 1/\beta +1 = 3$. Alongside with the BA model simulations, the dependence between $\gamma \textnormal{ and } \beta$ demonstrate the deep relationship among the graph topology and the temporal degree evolution, respectively contained in $\gamma$ and $\beta$ parameters.

Another feature present in \autoref{eq:BA_modki(t)} is that the degree distribution is independent on $t \textnormal{ and } N$ or a "stationary distribution". This result is the one expected, since the model aims at describing the real networks in general; thus, of rather different age and size.

\newpage
\section{The Origins of Preferential Attachment}
\label{App:OriginOfPA}
Since preferential attachment plays a relevant role in guiding the dynamics of a real network, the goal is to recover the same $\Pi(k)$ as the one assumed in the Barabási-Albert model.
It worths to introduce two classes of microscopic processes that generate it naturally \cite{barabasi::2016networkbook}:
\textit{"local (or random) mechanisms"} representing the interplay between random events and some structural properties of the network; and
\textit{"global (or optimized) mechanisms"} which take advantages of a cost-benefit analysis on the whole graph. 
In the following, it would be reviewed the "Link Selection" and "Copying" Models which belongs to the random class; and, finally, the "Optimization" Model of the second one.

\subsection{Local Mechanisms}
\subsubsection{Link Selection}
\label{sec:linear_pa_link_selection}
Assuming growth is at work, i.e. at each event time a new node is added, select randomly an edge and connect the new node to a vertex belonging to that edge.
Thus, the probability $q_k$ that the selected node, already present in the graph, has degree $k$ is 
\begin{equation}
	q_k = \frac{1}{\langle k \rangle} \cdot k p_k
\end{equation}
where the first factor exploiting the normalization constraint $\sum_k q_k = 1$.
Hence, it has the hoped form, being linear in the number of stubs ($k$) and in the frequency of the degree-k nodes ($p_k$). This displays an increasing of the chance of wiring to a degree $k$ node, by enhancing either the number of degrees or the presence of degree-k nodes themselves.

\subsubsection{Copying Model}
The idea is compute the probability of obtaining a connection of a degree-k node by mimicking the phenomenon of authors of a new webpage which copy links from existing webpages on the related topic \cite{Kleinberg:1999_WebAsAGraph}.

By introducing a new node $n$ in the network, as before, the difference stands in the link selection.
More in detail, selecting an existing node $u$ regardless of its degree, e.g. picking an arbitrary web document which topic is related to $n$, there are two possible way of connections:
\begin{itemize}
	\item Random (direct) connection: with a probability $p$, link $n$ to $u$;
	\item Copying (undirect) connection: with likelihood $1-p$, wire $n$ to an neighboring node $v$ of $u$. Reprising the web example, the new webpage $n$, copying the link of $u$, connects with its target $v$.
\end{itemize}

As a constraint to recover a dependence on $k$ of $\Pi(k)$, fix that $v$ should have degree $k$, by considering $p_v = k/2L$.
Hence, 
\begin{equation}
	\Pi(k) = \frac{p}{N} + \frac{k}{2L}(1-p)
	\label{eq:Pik_recovering}
\end{equation}
which is linear in k, yielding the desired preferential attachment.
The power of this approach is its "empirical" root which could be detected from citation networks to the social network of acquaintances.

\subsection{Global Mechanisms}
\subsection{Optimization Mechanism}
The principle of minimization of cost-benefits analysis in an economic market
\cite{Shively:2012_OverviewOfB-CAnalysis} can lead to preferential attachment whether a proper cost function is considered.

For simplicity assume that the vertexes are laying in an unit square, e.g. routers in a square continent. As before, at each event time a new router $i$ is added, while an edge with an existing node $j$ is created according to the cost function \cite{barabasi::2016networkbook}
\[
	C_i = min_{j\in G(V,E)} \left(\delta d_{ij}+h_j\right).
\]
where $d_{ij}$ is the euclidean distance between the vertex $i$ and $j$; while $h_j$ is the number of hops from $j$ to a pre-defined "center" which embodies the best network performance, e.g. net distribution hub. More clearly, $d_{ij}$ and $h_j$ represent the physical distance and "network-based" distance. Their summation describes the cost to be minimized for real results.

Depending on $\delta \textnormal{ and } N$, there are $3$ network topologies:
\begin{itemize}
	\item \textbf{Star Network $(\delta < (1/2)^{1/2})$}: only $\delta d_{ij}$ drives the growth, thus, the new nodes connects with the central one forming a star;
	\item \textbf{Random Network $(\delta \geq N^{1/2})$}: $h_j$ is the minimum quantity to be minimized. So, the new nodes connect to the nearest node;
	\item \textbf{Scale-free Network $(4 \leq \delta \leq N^{1/2})$}: in this intermediate regime scale-free topologies may be recovered as a result of optimization and randomness. In particular, optimization creates a basin of attraction with size $s \sim k_j$ and centered in $j$, within all the appearing nodes $i$ wire with $j$. On the other hand, Randomness is the chance of choosing a specific basin.
\end{itemize}

Thus, linear preferential attachment can come from both rational choice and random actions. Yet, most complex systems are driven by processes that have a bit of both luck or reason.

\section{Error Propagation and recovery time table}
There are reported the formulas for the estimation of the errors of all the relevant quantity. They are reported inside the plot under the rounded brackets "($\cdots$)".
Error propagation for $R_0$:
\begin{equation}
	\sigma_{R_0} = \sqrt{\beta / \mu} \cdot \sigma_D
\end{equation} 
Error propagation for $R_{c-net}$:
\begin{equation}
	\begin{aligned}
		\sigma_{R_{c-net}}^{2} &= \widehat{
			\left[\frac{2 D \langle k^2 \rangle - D^{2}}{\left(\langle k^2 \rangle - D \right)^{2}}\right]^{2} }
			\cdot \sigma_D^{2} + 
			\widehat{
			\left[ \frac{D^{2}}{\left(\langle k^2 \rangle - D\right)^{2}}\right]^{2} } 
			\cdot \sigma_{\langle k^2 \rangle}^{2} \\ \\
			&= \widehat{\left[\frac{R_{c-net}}{D}\right]^{4}} \cdot 
			\left[
			\widehat{ \left( \frac{2 \langle k^2 \rangle}{D} -1 \right)^{2} } \sigma_D^{2} 
			+ 
			\sigma_{\langle k^2 \rangle}^{2} \right]
	\end{aligned}
\end{equation}

Error propagation for $\Delta R_0 := R_0 - R_{c-net}$:
\begin{equation}
	\sigma_{\Delta R_0}^{2} = \sigma_{R_0}^{2} + \sigma_{R_{c-net}}^{2}
\end{equation}

Error propagation of secondary attack rate of COVID-19 $\beta_{COVID-19}$ for household restrictions: $\beta := 1 - (1-SAR)^{1/d} \textnormal{ , where } SAR := 19.3\% \, (95\% \, CI: 15.5\%-23.9\%)$ \cite{Jing:2020_betaCOVID-19_Houseldo_Sec_atta}. Thus,
\begin{equation}
	\sigma_{\beta} = \widehat{\left[ \frac{1}{d} \left( 1 - SAR \right)^{1/d - 1} \right]^{\frac{1}{2}}}\cdot \sigma_{ \mu } \sim 0.036
\end{equation}

\clearpage
The following table displays the conversion among the recovery rate $\mu$ and the day of recovery, which is the terminology used in medical articles. 

\begin{center}
	\begin{tabular}{||c|c||}
		\hline
		\multicolumn{2}{|c|}{Recovery Measures}\\  
		\hline
		\multicolumn{1}{|c|}{$ \mu$ } & \multicolumn{1}{c|}{days}\\
		\hline
		0.071 &  14\\ 
		\hline
		0.111 & 9 \\
		\hline
		0.167 & 6 \\
		\hline
		0.25 & 4 \\
		\hline
		1 & 1 \\
		\hline
	\end{tabular}
\end{center}


\bibliographystyle{plain}
\bibliography{../bib/my_bibliography.bib}
\end{document}	