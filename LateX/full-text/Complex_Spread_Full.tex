\documentclass[a4paper,12pt,twoside]{book} %%{book}

\usepackage[top=5cm,bottom=5cm,inner=5cm,outer=3cm]{geometry}
\usepackage{graphicx, subcaption} %per poter inserire le figure
\usepackage{csquotes} %handle " and ""
\MakeOuterQuote{"}  %handle opening/closing quotation
\usepackage{amssymb,amsmath,amsthm,amsfonts,bm}
\usepackage{bookmark}% http://ctan.org/pkg/bookmark
\usepackage{xspace}
\usepackage{tabularx}
\usepackage{indentfirst}
%\usepackage{subfigure}
%\usepackage[small]{caption}
\usepackage{caption}
\usepackage{eucal}
\usepackage{eso-pic}
\usepackage{url}
\usepackage{booktabs}
\usepackage{afterpage}
\usepackage{parskip}
\usepackage{listings}
\usepackage{fancyhdr}
\usepackage{textcomp}
\usepackage{cite}
\usepackage{multirow}
%\usepackage[utf8]{inputenc}   %per riuscire a scrivere gli accenti, but w/ this a error with csquote is raised. So, use \usep[english]{babel} only.
\usepackage[english]{babel}   %per riuscire a scrivere gli accenti
\usepackage{setspace}
\usepackage{etoc}
\usepackage{etoolbox}
\usepackage{setspace}
\usepackage{verbatim}
\usepackage{color}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{tocbibind}
\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{braket}
\usepackage{multirow}
\usepackage{float}
\usepackage{units}
\usepackage{siunitx}
\usepackage{bm}
\usepackage{bigints}

\usepackage{pgfplots} % to make plot inside LateX
\pgfplotsset{compat = 1.16}

\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{mathrsfs}
\usepackage{bbold}
\usepackage{mathtools}
\usepackage[style=ddmmyyyy]{datetime2}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\pagestyle{fancy} 

\graphicspath{{../images/}}

\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\Huge}

\definecolor{brown}{rgb}{0.28, 0.02, 0.03}
\hypersetup{
colorlinks,
citecolor=blue,
filecolor=blue,
linkcolor=blue,
urlcolor=blue
}

\makeatletter
\renewcommand\tableofcontents{%
    \section*{\Huge{\contentsname}}%
    \@starttoc{toc}%
}
\makeatother

\NewDocumentCommand{\evalat}{sO{\big}mm}{%
  \IfBooleanTF{#1}
   {\mleft. #3 \mright|_{#4}}
   {#3#2|_{#4}}%
}

\begin{document}

\newgeometry{top=2cm,bottom=2cm,left=2cm,right=2cm}

%%old for {book}: \frontmatter
%below: textsc = font "small caps"
\begin{titlepage}
\vspace{5mm}
\begin{figure}[hbtp]
\centering
\includegraphics[scale=.13]{../images/unipd_logo.png}
\end{figure}
\vspace{5mm}
\begin{center}
{{\huge{\textbf{\LARGE UNIVERSIT\'A DEGLI STUDI DI PADOVA}}}\\}
\vspace{20mm}
{\Large{\bf Dipartimento di Fisica e Astronomia "Galileo Galilei"}} \\
\vspace{5mm}
{\Large{\textsc{\bf Master Degree in Physics (LM-17)}}}\\  
\vspace{20mm}
{\Large{\textsc{\bf Final Dissertation}}}\\
\vspace{30mm}
{\Large{\textsc{\bf Modelling COVID-19 In a Network}}}\\
\vspace{45mm}
\end{center}

\begin{spacing}{2}
\begin{tabular}{lccccccccccl}
	{\Large{\bf Candidate}} &&&&&&&&&& {\Large{\bf Thesis supervisor}}\\
	{\Large{\bf Riccardo Milocco}} &&&&&&&&&& {\Large{\bf Prof./Dr. Marco Baiesi}}\\
\end{tabular}
\end{spacing}
\vspace{15 mm}

\begin{center}
{\Large{\bf Academic Year 2020/2021}}
\end{center}
\end{titlepage}

\restoregeometry

\clearpage{\pagestyle{empty}\cleardoublepage}

\pagestyle{empty}

\vspace*{\fill}
\tableofcontents
\vspace*{\fill}




%%old for {book}: 
%\mainmatter

%\newcommand{\changefont}{%
%    \fontsize{7}{12}\selectfont
%}
\fancyhf{}
%\fancyhead[LE,RO]{\changefont \slshape \rightmark} %section
%\fancyhead[RE,LO]{\changefont \slshape \leftmark} %chapter
%\fancyfoot[C]{\thepage}
\pagestyle{fancy}


%-----------------------------------------------------------------------

%\newgeometry{top=4cm,left=4cm}
\newgeometry{left = 2cm, bottom = 2cm, right = 2cm, top=2cm}

\chapter*{Abstract}
The usual simplified description of epidemic dynamics predicts an exponential growth. This is due to the mean field character of the dynamical equations. However, a recent paper (Thurner S, Klimek P and Hanel R 2020 Proc. Nat. Acad. Sci. 117, 22684) \cite{Thurner::NetBasedExpl} showed
that in a network with fixed connectivity, the nodes become infected at a rate that increases linearly rather than exponentially.
Experimental data for COVID-19 seem to validate this approach. In this thesis we plan to study this model by tuning its parameters.
In particular, we monitor the effect induced by a significant presence of hubs in the network.

\chapter*{Motivations}
Last Update: \today
"The COVID-19 pandemics has led to a dramatic loss of human life worldwide and presents an unprecedented challenge to public health, food systems and the world of work"\cite{Chriscaden::2021_ImpactCOVID19}. In this scenario, monitoring and forecasting the diffusion of the virus is a essential tool for policymakers to handle the health-care resources and restrictions among individuals. To this purpose, many academics, e.g. \href{https://web.unipd.it/covid19/en/}{UniPD against COVID-19}\footnote{https://web.unipd.it/covid19/en/}, have been trying to tackle the COVID-19 spreading as well as its side-effects, which have to be considered on the "path to normality". This drastic shift with respect to the "path to herd immunity" comes from a recent study \cite{GU::2021_SitePathToNormality} which shed light on the fact that "herd immunity", due to delay on vaccinations, could probably not be reached for the end of 2021. On the other hand, due to few practical points \cite{Nature:18.3.2021_NoHerdImmunity}, it is also probabile that the long-term prospects include COVID-19 becoming an endemic disease, much like influenza. 
Therefore, the recovery of the pre-COVID-19 situation could be slow even in the presence of effective vaccines.

So, an interesting path to cover is to describe the evolution of the COVID-19 pandemic, e.g. with the SIR model \cite{pizzuti::2020_ItalyCOVIDnetwork}, on different network topologies in order to simulate the different containment policies, i.e. "lock-downs".


\chapter[Introduction]{Introduction}
%\include{cap_Introduction}

\section{Complexity}
Since its beginning, Physics has been driven by a "Unification Principle"\footnote{from Newton with the "gravity"-relationship between an apple and the Moon; passing to Einstein, whom the effects of gravity are equal to the ones of an accelerated rocket for; ending to the \textit{Grand Unified Theory} as a unification of of general relativity with the particle physics},which aims at describing the reality as a "reduction" and simplification of its constituents and interactions. Nevertheless, this optimistic gaze, is (amazingly) challenged by the complexity that emerges in the world we are living in. This complexity could have broader origins but also different spatial scales: from ants' nest to storms, from the markets to human/animal migrations, from the epidemics to ecosystems. Therefore, Nature is capable of obeying simple laws but, on the other hand, to straightforwardly generate complex phenomena. A paradigmatic example is the flocking of birds: the core of the flock assumes different shapes due to a collective interaction among the birds; while an alone bird would proceed in a defined direction.
These facts were enough to establish the new branch of \textit{Physics Of the Complex Systems} which ultimate goal is to describe the emergence of the "complex" phenomena from the simple law of physics, e.g. "gravity model" \cite{GravityModelsandEmpiricalTrade}.
Having understood that \textit{"More is different"} \cite{Anderson:1972_MoreIsDifferent} and its ubiquity, it is natural to demand a formal definition of "Complexity". Nevertheless, quoting the Novel Price Murray Gell-Mann \cite{Gell-Man:1987_S&C}, "a variety of different measures would be required to capture all our intuitive ideas about what is meant by complexity and by its opposite, simplicity" . In fact, from computational complexity to other information measures, all such quantities are context-dependent and subjective, since they depend on many aspects, such as the detail level (coarse graining) of the entity to describe. Thus, a rough but clear definition could be drawn for the Complex System Society site \cite{CSS:2021_compsystdef}:

\begin{definition}[\textit{"Complexity"}]
Complex systems are systems where the collective behavior of their parts entails emergence of properties that can hardly, if not at all, be inferred from properties of the parts.
\end{definition}

In other words, the non-linear interactions among the parts of which a complex system is composed let emerge a behavior which could not be understood simply by applying the fundamental laws to its constituents.

The first representation of this weaved system is by considering entities and connections among them. Thus, a powerful tool to describe it is the Network Science.
In particular, the purpose of this thesis is to study the epidemic spreading of the COVID-19 over different topologies of the underlying social network of individuals. This phenomenon is regarded as a complex system through the analysis of the SIR model, which grasps the emergent features rather than the microscopic behavior of the disease.

\subsection{Models Ecosystem}
Infectious epidemics are responsible for a significant health and economic burden on society. New diseases appear frequently and old diseases persist. Therefore, many mathematical models have been used recently to guide policy makers to mitigate the impact of new infectious diseases (e.g. Polio, H1N1 influenza and Ebola) or established ones (such as HIV, cholera and seasonal influenza).
On the other hand, with the phrase \textit{All models are wrong; some models are useful} \cite{Box::2005_StatDesign}, George Box wanted to highlight that no model, no matter how complicated\footnote{Instead, the complexity of a model could challenge its possibility to be predictive in many scenarios. A related topic is the "over-fitted model" in statistical learning or "Occam's razor" in Philosophy}, is perfect. Thus, a way to evaluate a "successful" model has to be recovered.

Inside the "ecosystems" of models, there are the simple models which allow to analytically explain how the primary mechanisms influence important characteristics (e.g. epidemic threshold, epidemic size\footnote{how large it will be}, how long it will last). To be more accurate, it is also possible to develop models that incorporate much more details about both the disease and individual-level interactions \cite{Kiss::MathOfEpiOnNet}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width = .4\linewidth]{perfect_model.png}
	\caption{A caricature of the relation between model realism, complexity and the
	insight the model provides \cite{Kiss::MathOfEpiOnNet}.}
	\label{fig:perfect_model}
\end{figure}	

As suggested by the \autoref{fig:perfect_model}\footnote{\textit{"Est modus in rebus"} \label{cit:GM}} (\cite{Kiss::Ch1MathOfEpiOnNet}), models provide maximum insight where the right balance of realism (circle filled with dots, e.g. regarding epidemiology Graph-Neural-Network to predict $R_t$ \cite{Davahli::USA_predicting_COVID19}) and simplicity (circle with vertical lines e.g. on epidemics the deterministic SIR model ) is met\footnote{The equilibrium point may depend on the specific posed question} (central circle with oblique lines).

In this middle region, there could be find several works that use network analysis as the underlying structure for a phenomenon(\cite{Thurner::NetBasedExpl} \cite{VespignaniSatorras2001Epidemic} \cite{pizzuti::2020_ItalyCOVIDnetwork}); while other, more involved, that incorporate human mobility data in several hybrid models \cite{ZEROUAL::DL_COVID19, Stubinger::Incidence_Diff_Countries} in order to predict its diffusion. For the sake of simplicity, the present work of thesis is going to deepen the paper of \cite{Thurner::NetBasedExpl}. In particular, Thurner et al. \cite{Thurner::NetBasedExpl}, by focusing on the COVID-19 spreading in Austria and in the USA, are able to find a scenario, i.e. a combination of the spreading parameters\footnote{the infection rate $\beta$, recover rate $\mu$, the long-range attachment probability $p$}, such that the outbreak is (quasi-)linear for a value of the average number of contact $D$. This manifestation of the SIR model is, at least, qualitatively, showed by the curves reported by the JHUniversity. 
So, inspired by \cite{Thurner::NetBasedExpl}, the aim of this project is to analyze how the SIR model behave by changing its "inner" epidemic parameters but also the topology of the underlying social network, i.e. the susceptible people.

\section{A Technical Introduction}
\label{sec:ATechIntro}
At $8 \, \textnormal{May} \, 2020$, none of the affected COVID-19 states have reached "herd immunity" but still they reached the "epidemic peak" due to the containment restrictions on social contacts. One paradigmatic example is the case of Austria, for which the total infected cases at the first peak were $0.16\%$ of the total population, remarkably low with respect to the SARS-COV-2 "herd-immunity" level that are supposed to be at $0.8\%$ (\cite{Zingano:2021_HI_hom_pop}) of the entire (susceptible) population. A graphical representation of the cases could be find in \autoref{fig:NDI}, \autoref{fig:CNDI} produced by \href{https://ourworldindata.org/coronavirus/country/austria}{Austria New Daily Infected}.

\begin{figure}[htpb]
	\centering
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=\textwidth]{OWiD/Austria_NDI.png}
	  \caption{Austria until 8.5.2020}
	  %\label{fig:lab1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=\textwidth]{OWiD/USA_NDI.png}
	  \caption{USA until 8.5.2020}
	  %\label{fig:lab2}
	\end{subfigure}
	\caption{Daily Number Of Cases}
	\label{fig:NDI}
\end{figure}

\begin{figure}[htbp]
	\centering
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=\textwidth]{OWiD/Austria_CNDI.png}
	  \caption{Austria until 8.5.2020}
	  \label{fig:lab1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=\textwidth]{OWiD/USA_CNDI.png}
	  \caption{USA until 8.5.2020}
	  \label{fig:lab2}
	\end{subfigure}
	\caption{Cumulative Daily Number Of Cases}
	\label{fig:CNDI}
\end{figure}
The most striking observation is that the number of total cases (\autoref{fig:CNDI}), after early-stages when Non-Pharmaceutical-Interventions (NPIs), i.e. lockdown, were at work, exhibits a (quasi-)linear growth for an extended time interval in contrast with the "S-shaped" logistic curve predicted by the standard compartmental models. The extension of the linear regime depends on the onset of the measures; while for early stages, as it was the case for many countries \cite{Thurner::NetBasedExpl} ($8 \, \textnormal{May} \, 2020$), an exponential growth dominates the spreading of the disease.
By taking care of the modification of the underlying social network structure, in this present work of thesis, we want to (qualitatively) recover the spreading trends for the states that have or have not applied the NPIs measures. \newline
Specifically, an infection may occur for two reasons:
\begin{enumerate}
    \item interaction between an infected and a susceptible person;
    \item contact is "intense" (e.g. long, close,...) enough to lead to a disease transmission.
\end{enumerate}
So, the rationale behind the social distancing is that it takes to a reduction of both of these factors.
On the other hand, the analytically solvable SIR model assumes that, defined $N, D$ as the total number of individuals in the population and the average number of contacts respectively, there is the same probability that an individual encounters an infected person ("well-mixed population") and all the nodes have the same number of neighbors ($N-1$ or $D$). This approach allow to recover analytical results for a epidemic spreading but it annihilates the underlying network structure. Therefore, as claimed in \cite{VespignaniSatorras2001Epidemic}, there is the need of studying how the network affects the spreading of a disease, but still no focus is put on the spreading below the epidemic threshold \cite{Thurner::NetBasedExpl} as it is the case if nodes were separated through NPIs. 
In this thesis, the main goal is to grasp the relevant features of a complex social network in presence of NPIs and compare them with respect to a "well-mixed" population model.
In detail, by changing the epidemic parameters, we let the SIR Model evolve in different network topologies such as standard models of the graph-theory ("Erdös-Rényi","Scale-Free", "Caveman Model") but also a new involved topology, i.e. a poissonian Small-World Network. As a benchmark of a "well-mixed" population, it has been used an "annealed" mean-field (see ??) which is a natural realization of a mean-field network with a fixed average number of contacts per node.
Furthermore, $R_0 := \beta \cdot D/\mu$ is heuristically defined as the the average number of new infections caused by an individual in a completely susceptible population\footnote{The susceptible individuals are chosen according to "epidemic targets", e.g. only children for the measles} \cite{Kiss::MathOfEpiOnNet}. Moreover, it is well consolidated that for $R_0 > 1$ an epidemic is possible (not guaranteed); while for $R_0 < 1$ the disease will die out. 
Thus, these informations drive naturally to suppose that $R_0$ could also be a signal of the strength of an epidemic outbreak. Instead, this is not the case, since, as reported in the section ??, for equal $R_0$ values there could be different behavior of a disease. Thus, a new parameter has to be introduce that has to:
\begin{itemize}
	\item grasp the different behavior of epidemics;
	\item depend only on the initial condition, i.e. network metrics (e.g. "long-range" parameter $p$) and disease parameters ($\beta \textnormal{ and } \mu$).
\end{itemize}

A heuristics solution could be to characterize each spread via $R_{new} := \beta \cdot D/\mu * \mathbb{O}$ where $\mathbb{O} = \frac{D}{N-1}$, or equally $\mathbb{O} = D$. Thus, $R_0$ would still be considered as the secondary infected in a fully susceptible network; while $R_{new}$ as the "strength" a spread has due network limitations.

Finally, as in \cite{Thurner::NetBasedExpl}, a "epidemic" order parameter\footnote{Precisely (see ??), it is the standard deviation of the daily new infected} is computed for each spread to classify a light outbreak, i.e. an outbreak which infect less than the $30\%$ of the entire population, with a stronger one. In this way, a first-order phase transition could be obtained by plotting, for sufficiently high $p$\footnote{$p$ is the long-range rewiring probability.} the order parameter against the average degree of contacts.

On the top of the presentation of the work developed, it seems natural to deepen the area of the Network Science.


\section{Network Science}

Two aspects contribute to the popularity of Network Science: its universality nature of networks and the availability of large datasets\footnote{and the computational power to analyze them} which map huge networks of the furthest kind, e.g. "food-webs" or the Science of Science.
Its roots could be find in the Graph Theory, devoted to the analysis of the networks (or graphs), and Statistical Mechanics, committed to the formalization of the dynamical processes on network, such as self-organization and information theory.
For our purpose, the Graph Theory allows to formalize a "networky" - complex system such as the social network of the individuals over which a disease spreads; while Statistical Mechanics to develop a model focusing on its emergent characteristic.

\subsection{Graph Theory}
\label{sec:GraphTheory}
\begin{figure}[htbp]
	\includegraphics[scale = 0.7, trim = {0cm 17cm 4cm 4cm}, clip]{BSN-1}
	\caption{Contacts interaction Network. A contact is defined as the droplets that reach one individual. Therefore, a symmetric edge could represent a chat among two persons; where directed edges would be a "one-way" interaction such as in a room with pre-existent droplets. The number above each edge are the weights, which could be the minutes spent in the previous quoted room}
	\label{fig:basicSN}
\end{figure}

A graph is a tuple (V,E) where the "V" stands for "vertexes" (1,2,3,4) while "E" for "edges" (the links between the vertexes).
With the same notation, an edge connecting the node\footnote{In the context of a social network, a vertex represents a person/individual} $i$ to $j$ is, often, identified by $(i,j)$.
Formally, the labels of the vertexes are the image of any bijective embedding from the nodes to a chosen set, e.g. integer numbers. The relationship among vertexes are accounted by the edges, which could be unsymmetrical, as for $(1,4)$, and with different strength of the connection, such as $(1,4) \text{ and } (1,3)$ which. If all the edges are symmetric, i.e. if (A,B) implies (B,A), the graph is said undirected, e.g. the friendship network; otherwise it is said directed, e.g. the phone contacts network. 
With the given definitions, it is now clear that roughly everything could be divided in vertexes connected by am arbitrary relation.
The simplest example, is the \textit{simple graph}, which is a unweighted, undirected graph containing no graph loops, i.e. $(i,i)$ edges, or multiple edges such as multiples-$(i,j)$.
\begin{figure}[ht]
	\includegraphics[width = \linewidth]{SimpleGraph_950.png}
	\caption{Edges peculiarities. The labels are $1,2,3,4$ assigned anti-clockwise starting from the bottom}
	\label{fig:simple}
\end{figure}
\newpage

\subsection{Adjacency Matrix}
Typically for real networks, the number of involved nodes is big, e.g. the Google web-graph available at \href{https://snap.stanford.edu/data/#socnets}{Standford University Network Dataset} is of the order of the $800k$ nodes and $5milion$ edges. Thus, despite the clearness of \autoref{fig:basicSN}, the most suitable way of representing a graph is by grouping all its properties in a matrix called the adjacency matrix $A$. 
Starting from the basics, the (symmetric) adjacency matrix for the simple graph above is:
\[
A_{ij} :=
\begin{cases}
1 & \text{if exist an unweighted edge among i and j} \\
0 & \text{otherwise}
\end{cases}
\quad
\Leftrightarrow
\quad
A = 
\begin{bmatrix}
0 & 0 & 1 & 0 \\
0 & 0 & 1 & 1 \\
1 & 1 & 0 & 1 \\
0 & 1 & 1 & 0 \\
\end{bmatrix}
\]

Hereby, $A \in M_4(\mathbb{N})$ holds the nature of a simple graph:
\begin{itemize}[noitemsep]
	\item the rows represent the direct connections between the $i-th$ row and the $j-th$ column; while, the columns the edge $(j,i)$. Thus, there are only 4 nodes;
	\item unweighted: the value of the elements of $A$ is binar;
	\item without loops: the diagonal is $0$;
	\item without multi-edges: the value of the entries of $A$ are strictly $0$ or $1$, account for the presence of, at maximum, one edge.
\end{itemize}

A more involved, adjacency matrix is the one of the rightest graph which present multiples loops\footnote{As far as multi-edges are concerned, the $2$ would be off-diagonal}.
Formally,
\[
A = 
\begin{bmatrix}
0 & 0 & 1 & 0 \\
0 & 0 & 1 & 1 \\
1 & 1 & 0 & 1 \\
0 & 1 & 1 & \textbf{2} \\
\end{bmatrix}
\]

Thus, in \autoref{fig:basicSN} is report a (simple) weighted and directed graph with 4 nodes connected by a "central"\footnote{The "centrality" measure of the node is another important quantity which is context-dependent. In this case, the "centrality" refers to a degree centrality for which the most central node is the one with the highest number of contacts} node $1$.
In particular,
\[
A = 
\begin{bmatrix}
0 & 1 & \textbf{5} & 1 \\
1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\ 
\end{bmatrix}
\]
where it is encoded the direct edges since $A$ is not symmetric.

\subsection{Degree}

As quoted before, a (real) network could be very tangled. Thus, the degree distribution, i.e. the normalized number of vertexes with a certain degree, allows to check whether all the nodes have, or not, the same number of contacts even at a plain eye inspection. 

Formally, the degree of a vertex, in a (undirected) graph, is the number of edges connected to it. We denote the degree of node $i$ by $k_i$ and, by mean of $A$, $$k_i := \sum_{i=1}^{N} A_{ij},$$ where $N$ is the number of total nodes.
For directed graphs, as in \autoref{fig:basicSN}, a distinction between the entering and exiting edges to a node has to be defined. In particular, by a generalization of the previous $k_i$, 
\begin{equation}
	k_i^{in} := \sum_{j=1}^N A_{ij}, \quad k_j^{out} := \sum_{i=1}^N A_{ij}.
	\label{eq:kin_kout}	
\end{equation}
where $k_i^{in(out)}$ is the number of in-going (out-going) number of edges. 

\begin{figure}[t]
\begin{subfigure}{.5\textwidth}
	\begin{tikzpicture}
	\begin{axis}[ybar interval, 
		xtick align=inside,
		ymin = 0,%, ymax=55,ymin=0, minor y tick num = 3]
		ylabel = {Number of Nodes},
		xlabel = {In-degrees},]
	\addplot coordinates { (0, 0) (1, 3) (2, 0) (3, 0) (4, 0) (5, 1) (6, 0) };
	\end{axis}
	\end{tikzpicture}
\end{subfigure}%
\hfill
\begin{subfigure}{.5\textwidth}
	\begin{tikzpicture}
	\begin{axis}[ybar interval, 
		xtick align=inside,
		ymin = 0,%, ymax=55,ymin=0, minor y tick num = 3]
		ylabel = {Number of Nodes},
		xlabel = {Out-degrees},]
	\addplot coordinates { (0, 0) (1, 1) (2, 0) (3, 0) (4, 0) (5, 0) (6, 0) (7,1) (8,0) };
	\end{axis}
	\end{tikzpicture}
\end{subfigure}
\caption{Degree Distribution of the \autoref{fig:basicSN}}
\label{fig:degreed_basicSN}
\end{figure}

Therefore, by applying the \autoref{eq:kin_kout}\footnote{A practical check is to count the tails for the out-degree and arrows for the inner-degree. Ultimately, each sum has to equate the number of edges (a undirected edge has to be count doubled)} to \autoref{fig:basicSN}, it is possible to recover the degree distribution of the vertexes as in the \autoref{fig:degreed_basicSN}. Thus, a possibile way to build a graph would be to draw the nodes degrees from a fixed distribution and, then, connecting the nodes according to their degree. In fact, this approach is going to be exploited in the further chapters by using a poissonian degree distribution and connecting the vertexes only to nearest neighbors via an ad-hoc algorithm.

\section{Chapters Outline}
In chapter one, the models of this present work are going to be exposed. In particular, the 4 network models (Erdös-Rényi) will be treated in detail alongside with two kind of SIR models: the analytical SIR, which its ODEs are solvable; and the "monte-carlo" simulation one.

\section{To Do for Introduction}
\begin{itemize}	
	\item insert the Koenigsberg graph, the "adjacent definition for two vertexes" and the Theorem 1.1 (FedLor), deg(loop) = 2 as $2L = \sum k$ must hold;
	\item add a Social Network eloquent image, augment the simple network resolution or do them by yourself;
	\item World of War-craft had a serious bug which made epidemiologists devoting to it
	\item Cit: Dario:Ad quid? - a quale scopo ?	Rebus sic stantibus - stando così le cose
	\item Mene: Est modus in rebus
	\item Andre: Ca van san direct
	\item "A gratis"
\end{itemize}	

\chapter{The Network Models}
As could be perceived, modeling a complex (social) phenomenon\footnote{The "social" nature has been only evoked for concreteness. Nevertheless, this in-depth presentation of models is interdisciplinary as Network Science itself} with a network has two main tasks. The first is the identification of the nodes, which is a fairly simple task. The second is to pinpoint the emergent correlations in order to connect the vertexes and reproduce the phenomenon. The latter one is the real challenge in network theory.

%Thus, the goal is to artificially reproduce the main topological features of the real networks. 
Most of the real networks, however, do not show a comforting regularity such as the radial architecture of a spider net or the map of the main streets of Manhattan. Rather, their inner structure seems to be characterized by a certain degree of randomness, e.g. the internet network \cite{barabasi::2016networkbook}. Random network theory embraces this feature by constructing network models that are truly random. A review of these paradigmatic (Erdös-Rényi, Watts-Strogatz, Barabási-Albert) models is reported in the following sections. Ultimately, the newly proposed "Poissonian Small World" network (see \autoref{sec:ATechIntro}, \cite{Thurner::NetBasedExpl}) is presented.

\section{The Erdös-Rényi Model}
The Erdös-Rényi model (ER) is the first attempt to formally describe a random network. Thus, the primitive hypothesis is that the nodes are linked together with a probability $p$. Formally,

\begin{definition}[\textit{"Random Network"}]
A random network $G(N,p)$ has N nodes where two nodes are connected with probability $p$.
\end{definition}

The random model, thus, generated is called "Erdös-Rényi model". In fact, these two mathematicians are the fathers of the random graph theory, obtained by merging graph theory with probability theory.

Due to the random nature, this model may have different realizations even with $N$, $p$ fixed parameters. Therefore, the number of links and the nodes degree distribution (see \autoref{sec:GraphTheory}) becomes vital informations to be considered.
In particular, a specific realization could exhibit $L$ links with a probability
\begin{equation}
	\label{eq:probLrndNet}
	p_L = p^L \cdot (1-p)^{ \binom{N}{2} - L } \cdot \binom{\binom{N}{2}}{L}.
\end{equation}
The \autoref{eq:probLrndNet} is the product of 3 terms: the probability to have $L$ edges; the probability not the having the remaining ones knowing that the total possible node pairs are \[ \binom{N}{2} = \frac{N(N-1)}{2} \]; the total combinations in the choice of the $L$ edges, since what counts is only to pick $L$ edges and not which ones.
As expected, $p_L$ could be interpreted as $L$ successful connection attempt knowing that an edge is chosen with probability $p$ and the total pairs are $\frac{N(N-1)}{2}$. Thus, a Binomial distribution with the average number of links equal to
\[ <L> = \sum_{L = 0}^{\frac{N(N-1)}{2}} p_L L = p\frac{N(N-1)}{2} \label{eq:meanL} \] and, thus, the average per node contacts \[ <k> = \frac{2<L>}{N} = p(N-1) \label{eq:meank}. \]

The two equations above highlight that $<L>$ and $<k>$ could be obtained by multiplying $p$ by the total number of available links and neighbors resp.\footnote{This is accordance with the mathematical definition of an average quantity.}. Alongside, as the network becomes denser, i.e. increasing $p$, also $<L>$ and $<k>$ would be enhanced.
Moreover, by multiplying the expression by q, it is possible to obtain $\sigma_{<x>} = q \, \cdot <x>,$ where $x = L \textnormal{ or } k$.

Similarly, the probability\footnote{By means of $p_k$, $<k>$ and $\sigma_{<k>}$ could be recovered.} of having k number of contacts is 
\begin{equation}
	\label{eq:probkrndNet}
	p_k = p^k \cdot (1-p)^{ \binom{N-1}{2} - k } \cdot \binom{\binom{N-1}{2}}{k}.
\end{equation}
where $N-1$ are the total neighboring nodes differently from $\binom{N}{2}$, which were the total node pairs (cf. \autoref{eq:probLrndNet}).

Real network are sparse\footnote{Defining the "sparsity" of a network as the ratio $s : = <k>/N$, the internet-routers has $s \simeq 3.0 \cdot 10^{-5}$, since $N \simeq 200000 \text{ while } <k> \sim = 6$. Many others concrete example support this sparsity nature of real networks \cite{barabasi::2016networkbook}.}. Therefore, in the $<k> / N << 1$ limit\footnote{At least for $<k> \simeq 50 \text{ and } N \sim = 10^3$}, the \autoref{eq:probkrndNet} becomes the poissonian distribution with average $<k>$
\begin{equation}
	\label{eq:sparse_probk}
	p_k = e^{-<k>} \frac{<k>^k}{k!}.
\end{equation}

Therefore, keeping in mind that the \autoref{eq:sparse_probk} is only valid in the sparse regime,
\begin{itemize}
	\item the explicit expressions of $<k> \simeq p\cdot N \textnormal{ and } \sigma_k = <k>^{1/2}$ assume a simpler form for a poissonian distribution. Moreover, as the binomial case, $<k> \textnormal{ and } \sigma_k$ increase if p increases, since the the network is becoming denser and, so, more spread in the node degrees.
	\item it allows the same description for a pletora of networks with the same $<k>$, since the network characteristics does not depend on its size $N$.
\end{itemize}

\subsection{The "Small-World" Property}
\label{sec:SWProp}
A captivating argument, network science allows to study is the "Small-World phenomenon" or "six degrees of separation". The two quoted names comes, respectively, from a theme developed by the Hungarian writer Fridgyes Karinthy in a play called "Chains" ($1978$) and an empirical experiment injected by the psychologist S. Milgram in $1967$. In fact, the most famous version states that a person, e.g. Kevin Bacon, is linked to a random one, e.g. Andrea Scotti, through only few acquaintances: according to \href{https://oracleofbacon.org/movielinks.php}{Kevin Bacon Number}\footnote{https://oracleofbacon.org/movielinks.php} only $2$ common persons. More precisely, the distance between two randomly chosen nodes in a graph is small. Random networks account for this aspect as we are going to obtain shortly.

On average, the expected number of nodes $N$ at a certain distance $d$ is
\[N(d) \approx 1+ \langle k \rangle + \cdots + \langle k \rangle^d = \frac{\langle k \rangle^{d+1} -1 }{\langle k \rangle - 1},  \] 
where it has been taken advantage of the geometric series, after noting that $``1"$ is the selected nodes, $\langle k \rangle$ are its neighbors, $\langle k \rangle^2 \approx \langle k \rangle (\langle k \rangle - 1)$ the next neighbors and so on.
Thus, by considering that for a real network $\langle k \rangle << 1$ and that $N(d_{max}) \sim N$, 
\[
	d_{max} \sim \frac{\ln(N)}{\ln(\langle k \rangle)}
\]
On the other hand, the above equation is more suitable in describing $\langle k \rangle$, since $d_{max}$ is dominated by few distant paths from the core giant component. Hence, 
\begin{equation}
	\langle d \rangle \sim \frac{\ln(N)}{\ln(\langle k \rangle)} 
	\label{eq:SWrandnet}
\end{equation} 
is the formulation of the Karinthy's "Small-World phenomenon" obtained by considering that most of the nodes are contained within $\langle d \rangle$.
Indeed, for a real network, $\ln(N) << N$ drives to a "smaller" average path length compared to the scaling with $N$, which is the typical scaling for a regular lattice\footnote{The common notion of distance is based on a regular lattice which do not shows the "Small-World" effect. Thus, this result could be perceived as a uncomfortable}.
Moreover, the denominator of the \autoref{eq:SWrandnet} implies that the denser it becomes the network, the closer the nodes aggregate.
As a sociological example, the average distance of a random person, in the current century, would be derived by applying the \autoref{eq:SWrandnet} by fixing $N \sim 7\cdot 10^9$ and $\langle k \rangle \sim 10^3$ yielding $\langle k \rangle \sim 3.28$ which is a more reliable results than the "six degrees of separation".

\section{The Clustering Coefficient}
As far as a social network is considered, it seems natural to introduce a measure of community membership which capture the possibility to join a local community, e.g. an association or a family. More generally, real networks are characterized by the clustering formation, which "simplify" the network by aggregating the nodes, called clusters, which present common characteristics. Proceeding as the "coarse graining" technique of Statistical Mechanics, it becomes appealing to take care of the emergent properties exhibited by the "coarse-grained" network. In the following section, it is introduced the degree of "clusterization" of a network.

More precisely, the local clustering coefficient $C_i$ measures the density of link among the neighboring $i-th's$ nodes. To gain a rough intuition of its purpose, it worths to consider two opposite limits: 
\begin{itemize}
	\item $C_i = 0$ as there are no links among neighbors;
	\item $C_i = 1$ as there are fully connected neighbors.
\end{itemize}

Formally,
\begin{equation}
	C_i := \frac{\textnormal{average links of the i-th neigbors}}{\textnormal{total links among neighbors}} = \frac{\langle L_i \rangle}{\frac{k_i(k_i-1)}{2}} = p = \frac{\langle k \rangle}{N} = \langle C \rangle
	\label{eq:rnd_clustcoefficient}
\end{equation}

where $k_i \textnormal{ and } N$ are the amount of neighboring vertexes of $i$ and the total nodes respectively, \[ \langle L_i \rangle = p \cdot \frac{k_i(k_i-1)}{2} \quad \textnormal{ and } \quad \langle C \rangle := \frac{1}{N} \sum_{i = 1}^{N} C_i. \]

The presence of a link among the two neighbors closes a triangle with the focal node, $C$ could also be interpreted as the normalized number of triangles centered in the selected node.
This phenomenon is also known as "triadic closure".
Thus, $C_i = \langle C \rangle$ is independent by the $i-th$ node degree and, fixing the average degree of a network, it scales with $1/N$: for real networks $p = \langle k \rangle / N$ is small (real networks are sparse) and drives to a smaller $C$ that the one observed. A model that naturally generates "triangle", would be a good candidate to enhance the clustering coefficient even for small $p$.

\subsection{What Random Networks Can't Do}

The end of a general model development is to validate it against reality, as done in the concrete series of examples.
Thus, by looking at a real social network, e.g. at a cocktail party, there is the co-presence of highly connected nodes, called \textit{"hubs"}, alongside with the less interacting ones. In particular, the ER graph does not account for the presence of high-degree nodes\cite{barabasi::2016networkbook}, since they are strongly damped by the factor term $1/k!$ in \autoref{eq:probkrndNet}. Indeed, real networks are sparse but have also strongly heterogeneous degrees. In other words, the average degree per unit node is small, while preserving a high heterogeneity of the intrinsic node degrees. Thus, sparse links and amount of them per each node.

Secondly, by looking at the evolution of the contact network underlying the quoted cocktail party, there could be identified 4 topological regimes starting from isolated individuals for $\langle k \rangle = 0$ and ending with a fully connected graph for $\langle k \rangle = N-1$ where $N$ are the total invited persons. Therefore, these regimes scan the formation path of a fully connected network, passing trough a rapid emergence of a giant component. This is in a one-to-one relation with a First-Order-Phase Transition treated in Statistical Mechanics. In detail, the formation of a ordered giant component could be compared with ice formation: the passage from disordered water molecules to an ordered structure.
\newline Schematically,
\begin{itemize}[noitemsep]
	\item Sub-critical Regime for $0 < \langle k \rangle < 1 \quad (p < 1/N)$ 
	\item Critical point for $\langle k \rangle = 1 \quad (p = 1/N)$
	\item Supercritical Regime for $\langle k \rangle > 1 \quad (p > 1/N)$: Single giant component formation
	\item Connected Regime for $\langle k \rangle > \ln(N) \quad (p > \ln(N)/N)$
\end{itemize}
Thus, the ER model predicts the co-existence of a giant component, containing loops and cycles, and small components, which forms trees, for $1 < \langle k\rangle < \ln(N)$. This is clearly at odds with reality since, as a concrete example, in a random electric power-grid some consumers should not get electric power \cite{barabasi::2016networkbook}. Many network, as the actor network, are in the connected regime; most are in the supercritical one while still connected.

Thirdly, the real world networks exhibits a "Small-World" phenomenon not in agreement with the \autoref{eq:SWrandnet}. Hence, a new "Small-World" measure has to be proposed.

Fourthly, plotting $C(k)$ while fixing $N$, the theoretical expectation is to grasp the independent nature of $C$ with respect to $k$. Nevertheless, the results is unsatisfactory, since $C$ decreases with the increasing of $k$ for a pletora of the real networks.
Furthermore, $C/\langle k \rangle (N)$, for high $N$, is higher that its theoretical expectation. Thus, a refined model is needed, to explain the high $C$ even at small $p$ (or high $N$).

\section{Watts-Strogatz Model}
To face the problem of the "high $C$ at low edge probability (for now, redefined as $p_{ER}$)", while preserving the "Small-World" property, the mathematicians Duncan Watts and Steven Strogatz proposed in the $1998$ a hybrid model which interpolated between a regular lattice for $p = 0$ to a (pseudo-)random network model for $p = 1$. The new borne model is, thus, called "Watts-Strogatz (WS) model".
Practically, the idea is to start with a grid-like network, where all the nodes have $\langle k \rangle$ neighbors; then, with probability $p$, a link is chosen and one of the endpoints, such as the "rightmost", is replaced with a random vertex. The underlying degree distribution, hence, narrows the nodes degrees since it passes from a one-block histogram centered in $\langle k \rangle$ to a poissonian distribution. Thus, nodes have barely different neighbors degree and no hubs are allowed.
A further constraint is that no multiple edges can connect the same pair of vertexes, i.e. no multi-edges. In this way, the network obtained at $p = 1$ is not strictly random, since the degree of the each node $k_i = \geq  \langle k \rangle$ as a result "rightmost rewiring" technique \cite{Newman:2010_Net:AnIntro}. Indeed, in a true random graph the degrees span freely from $0$ to $N-1$. Furthermore, the presence of multiedges gives a correction of $1/N$ for big $N$. Thus, could be avoided for practical purposes.

The rationale behind this operation is that by diminishing a little the clustering, i.e. destroying triangles, corresponds a significant reduction of the average path length. Therefore, a "long-range" interaction is produced, dropping the average path length, while preserving the "triadic closures". 
It worths to highlight that the present $p$, called in this context the "rewiring parameter", is the "long-range" rewiring probability, while the ER model $p_{ER}$ is the probability of a edge formation either among neighbors ("short range" connection) as among distant nodes ("long range" connection). Thus, the "distant" connected vertexes are, on average, $p\cdot L$ where $L$ is the total number of links.
In particular, a regular lattice is characterized by a high clustering (neighboring nodes are fully connected) but a high average path length; while a random network, as seen before, exhibits manifestly the "Small-World" property alongside with a low clustering coefficient especially, since triangles are rare for small $p$.
Therefore, the parameter $p$ is introduce to fine tune the co-presence of the two kind of networks. In this way, it is possible to mimic for a range of $p$s the "Small-World" property, thanks to a low average path length, alongside with a high clustering coefficient.

As reported in \cite{Menczer:2020_1stCoursNetSci}, there is a range of rewiring probabilities between $p \in (0.01,0.1)$ in which $\langle l \rangle_p \approx \langle l \rangle_{p=0} \textnormal{ and } C_p \approx C_{p=0}$ encoding the fact that the average path length and clustering coefficient of the WS model are, respectively, near the one of a pseudo-random network and the one of a lattice.

Anyway, this model doesn't explain the presence of the hubs, since neglected by all the degree distributions, and the (inverse) dependence of $C(k)$ on the average network degree.

As a pedagogical spoiler, the (four) previous problems would be faced by modifying the WS model adding the emergent properties of the real networks, such as the "scale-free" property.
Hence, a useful approach would be to start from the degree distribution rather than on imposing a fixed probability of wiring as for ER and WS models.

TODO: Insert an image of a WS model and the clustering vs $\langle l \rangle$ property

\section{Configuration Model}
To face the problem previously exposed, a practical starting point could be to reproduce the degree distribution by taking advantage of the configuration model 
\cite{Menczer:2020_1stCoursNetSci}. Indeed, it is possible to built a network from an arbitrary degree sequence which either could be drawn by a distribution or by a real network of interest.
Precisely,
\begin{definition}[\textit{"Degree Sequence"}]
	The degree sequence is a list of $N$ numbers \newline $(k_0, \cdots, k_{N-1})$ where $k_i$ is the degree of node $i$.
\end{definition}
A degree sequence, thus, determines uniquely a degree distribution but the reverse is not true, since for a particular degree distribution there are $N!$ possibilities of labeling the nodes: $(1,2,3)$ has the same degree distribution of $(3,2,1)$.
Additionaly, a method to wire the nodes has to be chosen. Especially, the configuration model wires the nodes randomly; thus, building a random network. For these reasons, the generated graph is also called "random network with a pre-defined degree sequence". 

As done before, a practical way of constructing a network clarifies many ideas.
As a first step, consider to have a set of nodes with an assigned degree sequence. Pictorially, this could be depicted as a node with a number of dangling stub as its degree. Thus, the number of isolated stubs are twice the sum of all the degrees. 
The network is, further, composed by the following iterative steps:
\begin{itemize}
	\item A pair of stubs is selected at random;
	\item An edge is formed, by attaching together the selected stubs. 
\end{itemize}
This tying technique is replicated until all the stubs are saturated and it is random by construct. Therefore, this method generates random networks with a fixed degree distribution.

Since the number of stubs equals its degree, each node ends up having the desired degree. Thus, from the selected distribution, multiple (random) networks could be created; even, showing some peculiar properties, such as loops or multi-edges. Therefore, further constraints has to be imposed to obtain a specific topology, such as a connection only to "short-range" nodes. 

The network is characterized by mainly two propertios:
\begin{itemize}
	\item the probability that a fixed node $i$ may connect to another one $j$ is
		\begin{equation}
			p_{ij} = \frac{k_ik_j}{2L-1}.
		\end{equation}
		In fact, there are $k_i$ stubs (or rays) trying to match the $k_j$ ending of node $j$; while $2L -1$ are the overall rays available for the chosen one which produces the $"-1"$;
	\item the number of self-loops and multi-links decreases as $N$ increases, since it is less probable to connect with a specific node as $p \sim 1/N$. Typically it is not
	needed to exclude them \cite{Newman:2010_Net:AnIntro}. More importantly, arbitrarly rejecting self-loops or multi-links would result in a modification of the starting degree distribution, yielding the analytical calculations difficult. 
\end{itemize}

Obtaining different realizations of the same degree distribution could have a statistically interesting result. Indeed, let's suppose a certain feature, such as the clustering coefficient\footnote{The quoted measure is recalled only as an example, since for real network the clustering coefficient depends on different features than the degree distribution}, is independent on the other network characteristics rather than the degree distribution. By using this method, it would be possible to obtain an average and a standard deviation of all the predictions made by many configuration graphs. Thus, by comparing the result with the statistical predictions, it would be possible to validate the hypothesis that the feature of interest depends only on the degree distribution.

Subsequentely, a review of other approaches to artificially build a network is reported but not deepened, since not used in the following.
Another method to generate a network from a fixed degree distribution is the "hidden parameter ($\eta$) method" which, in addition, do not form multi-edges and loops \cite{barabasi::2016networkbook} and allows to tune the average degree $\langle k \rangle$ modifying $\eta$.
On the other hand, to recover a randomized version from the degree sequence, such as of a fixed real network, the "degree-preserving randomization" is the best help the theory provides. In this way, by just swapping two endpoint nodes at a time, it is possible to build an networks ensamble with the same degree distribution. So, check wheter a quantity depends only on the degree distribution itself or has a more involved structure.

\section{Poissonian Small-World Network}
Social network are highly non trivial exhibiting a involved structure, including multilevel organization; weak ties between communities; and temporal aspects that suggest a degree of fluidity with stable social cores \cite{Thurner::NetBasedExpl}.
As seen in the section \autoref{sec:ATechIntro}, however, the lockdown measures prevents the formation of highly connected nodes (hubs), which are representative of real network. Thus, a possible approach would be to use a poissonian small-world network with a SW-like rewiring parameter $p$ in order to grasp the heterogeneity in number of contacts (node degree); the "small-world phenomenon"; the clusterization of families alongside with their overlap and the $p$ fraction of individuals that are connected at "long-range" through leisure activities (by $p = 0$ these are prohibited).

In order to realize this peculiar network, the starting point is to use configuration model with the degree sequence drawn from a poissonian distribution. To simulate a closed population, the nodes are assumed to be put on a circle: the last and the first labeled nodes are neighboring nodes. Hence, both the "Small-World" property (see "The emergence of the Small-World property" \autopageref{sec:SWProp}) and the heterogeneity on the node degrees could be grasped at the same time. In this way, with low values of the poissonian parameter $\mu$ is possible to take into consideration the lock-down scenario where the average per node degree of contacts is the household size $D \sim 2.5$ \cite{Thurner::NetBasedExpl}. The configuration model generates a random wiring of the links, producing "long-range" connections which spoil the locality of the community structures (families). Therefore, by an ad-hoc algorithm, the stubs are forced to attach to the nearest nodes in the circle according to their degree. In practice, the wiring algorithm iterates over few steps\footnote{Recall that the degree sequence is already fixed and drawn by a poissonian distribution}:
\begin{itemize}
	\item choose the lowest degree $k_l$ node that has not already been saturated;
	\item a even number of times $k_l \% 2$ on the clockwise and anti-clockwise nodes;
	\item if the degree of the selected node is odd, wire one last time ($k_l - k_l \% 2 \, = 1$), say, anti-clockwise;
	\item delete the endpoint nodes which have saturated their degree.
\end{itemize}

More practically, fixed $[(1, 0), (6, 1), (0, 2), (2, 2), (4, 2), (7, 2), (8, 2), (9, 2), (5, 3), (3, 4)]$, i.e. a degree sequence labeled as the first entry with the reference node. 
The edges $(i,j)$ are created by connecting $i \textnormal{ to }j$ and diminishing the degree of $j$ at each step:
\begin{itemize}
	\item node $1$ is left alone as $deg(1)=0$;
	\item $(6,7)$;
	\item $(0,2)\textnormal{ and }(0,9)$
	\item $(2,0) \text{ (already present) and } (2,3)$;
	\item $(4,3)\textnormal{ and }(4,5)$;
	\item $(7,6)\textnormal{ (already present) and }(7,8)$; 
	\item $(8,7) \textnormal{ (already present) and } (8,9)$
	\item FINISH WHEN A PLOT OF A REALIZATION IS AVAILABLE
\end{itemize}

Hence, the algorithm preserves the poissonian degree distribution, while enhance the local clustering of the nodes. However, is it possible to have "long-range" connection, since the nearest available vertexes could be already saturated; therefore, resulting in a distant linking. This "tunneling" nodes are neither "super-spreaders" (or hubs of the infection), as they are forbidden by the poissonian degree distribution; nor "super-infectors", which would be characterized by an higher infectivity rate $\beta$.

\section{Connected Caveman Graph}
By analogy with the local interactions produced by the NPIs measures, it has been considered the "Caveman Model".
The rationale is to consider as if one node, of the previously modeled networks, would be a family composed by a fixed number of people. This idea drives to the "coarse-graining" approach seen in the previous sections.
In detail, the Caveman model enables to fix both the number of "caves"/families and the number of individual for each family. So, to connect each cave on a circle, one edge among two random nodes for each neighboring cave has been added and a WS-like "rewiring parameter" as been introduced to enable the "long-range" attachment.

A naive expectation is that the epidemic spread slower and more stochastically, since the nodes are poorly connected with the "outside" rather than in the family.

\section{Scale-Free Property}

\begin{definition}[Scale-Free Network \cite{Barabasi:1999_ScalRndNet}]
	A scale-free network is a network whose degree distribution follows a power law.
\end{definition}

Empirical evidence shows that the degree distribution of the majority of the real network, e.g. WWW, is poorly behaving as a poissonian distribution, due to the presence of high degrees nodes or "hubs" \cite{barabasi::2016networkbook}. Rather, a better approximation is obtained via a power law distribution of the kind $p_k \sim k^{-\gamma}$ which in log-log scale turns into the linear relation $\log(p_k) \sim -\gamma \log(k)$ where $\gamma$ is called the "degree exponent". As far a directed network is considered, as WWW, the $k$ has to be sub-divide into $k_{in} \textnormal{ and } k_{out}$, thus doubling the quoted relations.
Therefore, by exhibiting a power-law distributions of degree nodes, these kind of networks are endowed with "scale-free" property which, in turn, makes them call "scale-free networks". In detail, the power-laws are long-tailed function; hence, describing a phenomenon that has not a reference scale. For example, the heights distribution of a sample population displays a peak at the average height, which is the reference scale for that population. Instead, a power-law distribution does not have a peak and allows also for outliers, e.g. 100ft tall individuals. So, a reference scale for a power-law is not naturally present as in an exponential bounded distribution, i.e. when there is a exponential or faster decay for high $k$, such as binomial/poissonian/Gaussian distributions.

In the following part, the discrete approach, assuming $k = 0,1,2\cdots$, is going to be developed; while only the continuum result would be reported. In fact, the discrete approach provides the ready-to-use formulas handle by a calculator which, by construction, uses discrete operations. On the other hand, the continuum approach is useful for analytically support of the calculation and could be obtained similarly to the discrete case.
Formally, by exploiting the constraint on probabilities $p_k = C\cdot k^{-\gamma}$ \[\sum_{k=1}^{\infty} p_k = 1\], it is possible to recover the closed form
\begin{equation}
	p_k = \frac{1-p_0}{\zeta(\gamma)}k^{-\gamma}.
	\label{eq:p_scalefree}
\end{equation}
where $p_0 = p_{k=0}$ which has been separately considered as the power-law diverges for $k=0$.

On the another hand, the continuum formalism drives to \[p(k) = (\gamma-1)k_{min}^{\gamma-1}k^{-\gamma},\] since the the smallest value for the power law to hold is not $1$, as in the discrete approach, but $k_{min}$, the degree for which
\begin{equation}
	\int_0^{k_{min}-1} p(k) dk \stackrel{!}{=} 1/N \qquad \textnormal{or} \qquad P(k_{min}-1) \stackrel{!}{=} 1/N.
	\label{eq:kminrel}
\end{equation}

In the next part, the allowed maximum degree $k_max$ would be obtained either for a exponential bounded and a power-law distributions. Through this quantitative insight, it would be shown the dependence of $k_max$ on $N$, i.e. the size of the network. 

In particular, the exponential distribution for which \(\int_{k_min}^infty p(k) dk = 1\) holds, is \[ \lambda e^{\lambda k_{min}} e^{-\lambda k} \] and, using \autoref{eq:kminrel}, $k_min = 1$.
Furthermore, $k_max$ is defined to be such that the at most one node $i$ could have a degree $k_i$ larger than $k_max$. In other words, the cumulative distribution for degrees greater than $k_{max}$ is $1/N$; and, hence, $k_{max}$ (or the "natural cutoff") is expected to be the size of the bigger hub.
In formulas, 
\begin{equation}
	\int_{k_{max}}^{infty} p(k) dk = 1/N \implies k_{max} = k_{min} + \frac{\ln(N)}{\lambda}.
	\label{eq:Expkmax}	
\end{equation}
So, $k_{max}$ would not differ much with respect to $k_{min}$ since $\ln(N)$ would reduce much the size of the network $N$.
For a Poisson distribution, more effort is needed but the same interpretation holds.
Applying the same rationale to a scale-free network, 
\begin{equation}
	k_{max} = k_{min}\,N^{\frac{1}{\gamma-1}}.
	\label{eq:SFkmax}
\end{equation}
In this case, there could be order of magnitude of difference between $k_{max}$ and $k_{min}$.
Indeed, as a simple example, WWW forms a graph of on $N \sim 10^5$ documents with $\langle k \rangle \simeq 4.6$ and $\gamma = 2.1$ \cite{barabasi::2016networkbook}. 
By assuming $\lambda = 1 \textnormal{ and recovering that }, k_{min} = 1$ using \autoref{eq:kminrel}, $k_max \sim 14$; while $k_{max} = 95.000$ for a scale-free network with $\gamma = 2.1$, e.g. WWW network. This short results reinforces the quoted insight that for the class of "exponentially decaying" distributions the hubs are strongly forbidden since the nodes have comparably the same degree. Instead, the highly connected nodes are naturally arising by increasing $N$ with an underlying power-law distribution.
\label{sec:SFProperties}

Not all the network are scale-free since the hubs are present only if nodes have unlimited degree capacity.
In fact, there could be constraints, such as cost-benefit problem, which limit $k_max$ of the bigger node. In this overview, random network may be the best fit, e.g. national highways network, power grid of generator and switches, $\cdots$

With all the previous effort a more quantitative approach to the explanation on the "scale-free" etymology could be ultimate.
More precisely, defining the statistics momentums as
\begin{equation}
	\langle k^n \rangle := \int_{k_{min}}^{k_{max}} k^n\, p(k) dk
\end{equation}
it is possibile to recover the the mean and the variance for $n = 1,2$.
Thus, for a poissonian distribution, the nodes degree $k$ belong to $\langle k \rangle \pm  \langle k \rangle ^ {1/2}$ interval, which drives to the desired $\langle k \rangle$ scale. At the same time, for a power-law distribution holds	
\begin{equation}
	\langle k^n \rangle \sim \frac{k_{max}^{n-\gamma+1}-k_{min}^{n-\gamma+1}}{n-\gamma+1}
	\label{eq:kn_SFnets}
	.
\end{equation}
Therefore, knowing that real networks have typically $\gamma \in (2,3)$, e.g. WWW has $\gamma =  2.1$ \cite{barabasi::2016networkbook}
\begin{equation}
	k \in \langle k \rangle \pm \lim_{N \to \infty} \langle k^2 \rangle = \infty.
\end{equation}
where the hidden limit of \( k_{max} \to \infty \) is understood, since $k_{max}$ increases with the system size. 
So, no scale naturally arises.

Hubs represent a one-hop bridge for many nodes, by centralizing the otherwise-distant nodes. Indeed, due to hubs, the average distance $\langle d \rangle \sim \ln(\ln(N))$ for $\gamma \in (2,3)$, pointing out a ultra small-world nature of "scale-free" network. An empirical example is the airport lines, which drastically diminish the \(\langle d \rangle\) of the national highways by reducing the hops from one city to another. The result rapidly quoted before is surrounded by three other regimes 
\cite{Cohen:2003_SFUSW}:

\begin{equation}
	\langle d \rangle = 
	\begin{cases}
		constant & \gamma = 2 \\ \\
		\ln(\ln(N)) & \gamma \in (2,3) \\ \\
		\frac{\ln(N)}{\ln(\ln(N))} & \gamma = 3 \\ \\
		\ln(N) & \gamma > 3
	\end{cases}
	\label{eq:USWdistance}
\end{equation}
\newline
\textbf{Anomalous Regime ($\gamma = 2$)}: Due to $k_max \sim N$, the network is forced into a "wheel rim configuration", a central hub with spoke nodes. In this regime, the average path length is independent on the size of the graph.
\newline
\textbf{Ultra-Small World Regime ($2 < \gamma < 3$)}: By predicting a $\ln(\ln(N))$ trend of the average distance, \autoref{eq:USWdistance} guides to the concept of "ultra-small world phenomenon" as a slower growth than random network. Indeed, recalling the worldwide social network, \(\ln\ln(N) \sim 3 \text{ while } \ln(N) \sim 22\) more than few times higher than the expected "six degrees of separation".
\newline
\textbf{Critical Point ($\gamma = 3$)}: Looking back on the \autoref{eq:kn_SFnets}, the variance ($n=2$) is finite, marking the passage between the small-world ($\sim \ln(N)$) and the ultra-small world ($\sim \ln(\ln(N))$) classes of graphs.
\newline
\textbf{Small World ($\gamma > 3$)}:
In this regime, the finiteness of the variance is understood, due to the light aggregation that the high-degree nodes could perform in these kind of networks.

Moreover, recalling the root of "ultra-small world" \autoref{eq:SFkmax}, for $N \lesssim 10^2$ the path length distributions overlap in all the reported regimes and, so, converges to a Poisson distribution ($\gamma > 3)$ whose hubs are small to produce the "ultra-small world" phenomenon. In fact, to validate the power-law nature of a distribution, the nodes degree should be separated by 2-3 orders of magnitude. Hence, reverting the \autoref{eq:SFkmax},
\[N = (\frac{k_{max}}{k_{min}})^\frac{1}{\gamma -1},\]
which lucidly reproduce $N \sim 10^2$ for $\gamma = 2.1$ (WWW network).

Thus, \(\langle d \rangle\) is changing with $\gamma$, as the smaller it is, the shorter would be the average distance; but also with $N$, as it is the "ab ovo" hypothesis for having big hubs.

A sociological fact has to be remarked when trying to map the "ultra-small" world over social interactions. Indeed, an online experiment which aimed to replicate the "six degrees" concept find that individuals involved in complete chains, reaching their target, were less likely to send a message to a hub than individuals involved in incomplete chains \cite{Dodds:2003_GSonlineNet_6deg}. The reason may be self-imposed, since hubs are perceived as being busy, so contacted only in real need and avoid to concretely complete an "online experiment" challenge. Thus, network is not everything since actual success depends sensitively on individual incentives.

Moreover, for a balanced approach on the comparison of real degree distributions and a power-law, it has to be considered that the hosted phenomenon (-a) is complex and affects the graph topology, which, in turn, spoils the shape of the degree distribution. Rebus sic stantibus \label{cit:D.Massa}, it is sufficient to establish a fair starting point, rather then the meticulous fitting with a long-tail one. More concretely, start adding ad-hoc features over or an "exponentially bounded" or a "scale-free" distribution, by looking at $\langle k \rangle ^2$ \cite{barabasi::2016networkbook}.

As a final remark, many interesting features of scale-free networks, from their robustness to anomalous spreading phenomena, are linked to the $\gamma \in (2,3)$; thus, due to the small average distance which strengthen the considered phenomenon.

\subsection{Growth and Preferential Attachment}

Before deepening a model to naturally generate hubs (Albert-Barabási model), it is worth to review two important features without which it is impossible for hubs to emerge: growth and preferential attachment. Indeed, the explanation of the common similarities among graphs of broad origins (e.g. WWW, protein-protein interaction, social networks, $\cdots$), should be simple, but fundamental. The result of this investigation will change the methodology from describing a network’s topology to modeling the evolution of a complex system.

Growth is an active process driven by the sequential increasing of the size of the network $N$ as per the addition of new nodes. Rather, a random network assume $N$ to be fixed. 

Furthermore, most real networks new nodes prefer to link to the more connected ones, a process called "preferential attachment". On the other hand, random networks wire randomly with other nodes.

Ça va sans dire \label{cit:A.Marzo}, the constraints are that the model to be dynamical and encode a "preferential" probability which changes at every modification of the topology, called also "event time". 

\section{Barabási-Albert Model}
To take advantage of the coexistence of both growth and preferential attachment in the real networks, it has been proposed a minimal model, the Barabási-Albert model \cite{barabasi::2016networkbook}:
\begin{itemize}
	\item start with $m_0$ nodes randomly connected and a degree at least equal to $1$;
	\item \textbf{Growth:} at each event time add a new node with $m(\leq  m_0)$ stubs which are linked to different nodes;
	\item \textbf{Preferential Attachment:} the probability to wire with a node $i$ depends on the degree centrality, i.e. importance based on the degree:
	\begin{equation}
		\Pi(k_i) = \frac{k_i}{\sum_{j = 1}^N k_j}
	\end{equation}
\end{itemize}

So, preferential attachment has a stochastic nature not a deterministic one. Nevertheless, it increases the appeal of the (already) high-degree nodes, alias \label{cit:S.Sagone} "rich-gets-richer" phenomenon. As a result, while most of the nodes end up having few links, a bunch of nodes become highly connected; thus, performing the role of big hubs. A degree distribution which does not decay exponentially, i.e. a power law with $\gamma = 3$ \cite{barabasi::2016networkbook}, has been created. 

Empirical studies on networks, such as actor and scientific collaboration networks, have found that the exponent at the numeratore of $\Pi(k_i) \sim k_i^\alpha$ could differ from $1$.
Indeed, there are $3$ paradigmatic kinds of preferential attachments (pas) by varying the exponent $\alpha$: sublinear pas ($0<\alpha<1$), (linear\footnote{Commonly, the here-called "linear" pas is just called "preferential attachment" without further specification.}) pas ($\alpha = 1$), superlinear pas ($\alpha>1$). Briefly, the sublinear pas drives to an "stretched exponential distribution" (see \autoref{sec:SFProperties}); the linear to the standard scale-free network; while in the superlinear almost all nodes connects to few super-hubs (winner-takes-all phenomenon).
Hence, only for $\alpha = 1$, a power-law distribution could be recovered, recalling that knowing the high-degree behavior is just a starting point to model the underlying phenomenon. 
In addition, after $t$ event times, the network has gained $t$ new nodes for a total of $N = m_0 + t$ nodes and $m_0 + mt$ links.

\section{Linearized Chord Diagram}
Having in mind the general purpose and results, a more precise description would clarify two point left open, i.e. 
\begin{itemize}
	\item the arrangement of the initial $m_0$ nodes;
	\item whether the next $m$ links were added simultaneously or one at the time. This could drive to the formation of multi-links, i.e. conflicting edges, due to assumed independent nature of the new stubs. 
\end{itemize}

These problem are solved with the idea of sequentially adding nodes the past graph(s).
Fixing $m=1$ for simplicity, start with the empty graph $G_{m}^t = G_1^0$.
At each event time, choose an initial vertex $v_i$ of $G_1^{(t-1)}$ and connect it to the new vertex $v_t$ with probability $p$ such that
\begin{equation}
	p =
	\begin{cases}
		\frac{k_i}{2t-1} & if \quad i \in [1,t-1] \\
		\frac{1}{2t-1} & if \quad i = t
	\end{cases}
	.
\end{equation}

Rephrasing it, connect $v_t$ with $v_i$ with a probability of $\frac{k_i}{2t-1}$ if the nodes are different; otherwise, $\frac{1}{2t-1}$. This method does not forbid the formation of multi-edges and loops, but constraints their number to be negligible as the network grows.
The present technique referred to $m=1$ for an immediate perception. Whether $m > 1$ stubs are involved, add them one at the time, enabling their contribution to the hubs degree.

\section{Recovering Power-Law Distribution}
Due to a new $m-$stubs node, the degree of a existing node $i$ changes at a rate
\begin{equation}
	\frac{dk_i}{dt} = m \Pi(k_i) = m \frac{k_i}{\sum_j k_j}
	\label{eq:BA_dk_i/dt}
	.
\end{equation}
Hence, for $t >> 1$, 
\begin{equation}
	k_i(t) = m (\frac{t}{t_i})^\beta
	\label{eq:BA_modki(t)}
\end{equation}
where $\beta$, alias "dynamical exponent", has the value $\beta = 1/2$ \cite{barabasi::2016networkbook} and $t_i$ is the entering time for the node $i$.

The remarks, coming from the \autoref{eq:BA_modki(t)}, are multiple:
\begin{itemize}
	\item all the nodes follow the same power law, since $\beta$ is independent on the node label $i$. Furthermore, it is independent on $m$ and $m_0$ constructing parameters;
	\item the inverse dependence of $k_i \sim 1/t_i$ is a clear signal of the fact that old nodes are supposed to be larger, also called "first-mover advantage" in business;
	\item both $\beta = 1/2 < 1$ (sublinear growth) and the rate $dk_i/dt \sim 1/\sqrt{tt_i}$ underly a competing scenarios:
	the existing nodes compete for the $m-$new stubs with a growing bunch of nodes, resulting in a sublinear growth, which drives to a decreasing rate of acquaintances. 
\end{itemize}

To validate the power-law nature of the probability of having a certain node degree, by exploiting \autoref{eq:BA_modki(t)} it is possible note that the nodes $i$ with a degree higher than $k$ are such that \[t_i < t \left(\frac{m}{k} \right) ^{1/\beta}. \] Thus, for $t >> m_0$, the (cumulative) probability of having a degree less than $k$ is \[P(k) = 1 - \left(\frac{m}{k} \right)^{1/\beta}, \] from which the probability of having a degree $k$ could be recovered, by differentiating $P(k)$ and substituting $\beta = 1/2$, $p_k = 2m^2k^{-3}$ \cite{barabasi::2016networkbook}. As desired, the distribution exhibits a long-tail nature with $\gamma = 1/\beta +1 = 3$. Alongside with the BA model simulations, the dependence between $\gamma \textnormal{ and } \beta$ demonstrate the deep relationship among the graph topology and the temporal degree evolution, respectively contained in $\gamma$ and $\beta$ parameters.

Another feature present in \autoref{eq:BA_modki(t)} is that the degree distribution is independent on $t \textnormal{ and } N$ or a "stationary distribution". This result is the one expected, since the model aims at describing the real networks in general; thus, of rather different age and size.

\section{The Origins of Preferential Attachment}
Since preferential attachment plays a relevant role in guiding the dynamics of a real network, the goal is to recover the same $\Pi(k)$ as the one assumed in the Barabási-Albert model.
It worths to introduce two classes of microscopic processes that generate it naturally \cite{barabasi::2016networkbook}:
\textit{"local (or random) mechanisms"} representing the interplay between random events and some structural properties of the network; and
\textit{"global (or optimized) mechanisms"} which take advantages of a cost-benefit analysis on the whole graph. 
In the following, it would be reviewed the "Link Selection" and "Copying" Models which belongs to the random class; and, finally, the "Optimization" Model of the second one.

\subsection{Local Mechanisms}
\subsubsection{Link Selection}
Assuming growth is at work, i.e. at each event time a new node is added, select randomly a edge and connect the new node to a vertex belonging to that edge.
Thus, the probability $q_k$ that the selected node, already present in the graph, has degree $k$ is 
\begin{equation}
	q_k = \frac{1}{\langle k \rangle} \cdot kp_k
\end{equation}
where the first factor exploiting the normalization constraint $\sum_k q_k = 1$.
Hence, it has the hoped form, being linear in the number of stubs ($k$) and in the frequence of the degree-k nodes ($p_k$). This displays an increasing of the chance of wiring to a degree $k$ node, by enhanching either the number of degrees or the presence of degree-k nodes itselves.

\subsubsection{Copying Model}
The idea is compute the probability of obtaining a connection of a degree-k node by mimicing the phenomenon of authors of a new webpage which copy links from existing webpages on the related topic \cite{Kleinberg:1999_WebAsAGraph}.

By introducing a new node $n$ in the network, as before, the difference stands in the link selection.
More in detail, selecting an existing node $u$ regardless of its degree, e.g. picking an arbitrary web document which topic is related to $n$, there are two possible way of connections:
\begin{itemize}
	\item Random (direct) connection: with a probability $p$, link $n$ to $u$;
	\item Copying (undirect) connection: with likelihood $1-p$, wire $n$ to an neighboring node $v$ of $u$. Reprising the web example, the new webpage $n$, copying the link of $u$, connects with its target $v$.
\end{itemize}

As a constraint to recover a dependence on $k$ of $\Pi(k)$, fix that $v$ should have degree $k$, by considering $p_v = k/2L$.
Hence, 
\begin{equation}
	\Pi(k) = \frac{p}{N} + \frac{k}{2L}(1-p)
	\label{eq:Pik_recovering}
\end{equation}
which is linear in k, yielding the desired preferential attachment.
The power of this approach is its "empirical" root which could be detected from citation networks to the social network of acquaintances.

\subsection{Global Mechanisms}
\subsection{Optimisation Mechanism}
The principle of minimization of cost-benefits analysis in a economic market
\cite{Shively:2012_OverviewOfB-CAnalysis} can lead to preferential attachment wheter a proper cost function is considered.

For simplicity assume that the vertexes are laying in a unit square, e.g. routers in a square continent. As before, at each event time a new router $i$ is added, while an edge with an existing node $j$ is created according to the cost function \cite{barabasi::2016networkbook}
\[
	C_i = min_{j\in G(V,E)} \left(\delta d_{ij}+h_j\right).
\]
where $d_{ij}$ is the euclidean distance between the vertex $i$ and $j$; while $h_j$ is the number of hops from $j$ to a pre-defined "center" which embodies the best network performance, e.g. net ditribution hub. More clearly, $d_{ij}$ and $h_j$ represent the physical distance and "network-based" distance. Their summation describes the cost to be minimized for real results.

Depending on $\delta \textnormal{ and } N$, there are $3$ network topologies:
\begin{itemize}
	\item \textbf{Star Network $(\delta < (1/2)^{1/2})$}: only $\delta d_{ij}$ drives the growth, thus, the new nodes connects with the central one forming a star;
	\item \textbf{Random Network $(\delta \geq N^{1/2})$}: $h_j$ is the minimum quantity to be minimized. So, the new nodes connect to the nearest node;
	\item \textbf{Scale-free Network $4(\leq \delta \leq N^{1/2})$}: in this intermediate regime scale-free topologies may be recovered as a result of Optimisation and Randomness. In particular, Optimisation creates a basin of attraction with size $s \sim k_j$ and centered in $j$, within all the appearing nodes $i$ wire with $j$. On the other hand, Randomness is the chance of choosing a specific basin.
\end{itemize}
 

s precisely because it can come from both rational choice and random
actions [25]. Most complex systems are driven by processes that have a bit
of both. Hence luck or reason, preferential attachment wins either way.




\chapter{The Epidemiological Models}
\chapter{Results}
\chapter{Conclusions}


\bibliographystyle{plain}
\bibliography{../bib/my_bibliography.bib}

\end{document}




\chapter{The Models}
\subsection{The Network Models}
To this end, we proceeded with the following steps.
At first, we pruned a fully connected graph of $N = 1000$ nodes, by keeping fixed $N$ while having $D$, i.e. the average of nearest neighbors. In parallel, since $R_0 = \frac{\beta \cdot D}{\mu}$, we doubled the $\beta$ \footnote{$\beta \in [N,2]$} parameter to keep the power of the epidemic $R_0$ fixed (\ref{fig::pruning_p0.0}). 

Since the aim is to understand the difference with respect to the "well-mixed" population model, we put in the same plot the number of total infected cases with the epidemics that spreads either on a specific network (in green) or in a "mean-field" fashion (in orange)\footnote{Moreover, the daily infected of the network (in blue) and of the mean-field (in magenta)}. More precisely, the "mean-field" approximation is a "D-neighbors-mean-field" where every node has $D$ nearest neighbors picked at random on the whole set of nodes. In this way, the hypothesis of the "well-mixed" population is recovered, i.e. everyone has the same probability to get in touch with an infected, and the spreading could be modeled with the exact same parameters ($\beta \textnormal{ and } \mu$) as the network epidemics.

Then, we introduced the "rewiring probability" $p$ which enables the distant nodes to connect faster, thus, obtaining a Erdös-Rényi (E-R) graph with non-zero $p$. In this case, we could recover the "(pseudo) linearity" of the SIR infection, as it is shown in \ref{fig::pruning_p0.1}, using two similar methods of choosing the spreading parameters $\beta \textnormal{ and } \mu$:
\begin{itemize}
	\item pick a series of $\beta \in [0,1]$ and $\mu \in [0,1]$ coefficients at random;
	\item choosing $\mu \in [0,1]$ and fixing $p=0.1$. Then, as in the previous point, while halving D, the $\beta$ is doubled.
\end{itemize}
For conciseness, in \ref{fig::pruning_p0.1}, it is reported only the epidemics obtained by the "pruning procedure".
\newline
To further test the recovered (pseudo) linearity with respect to the mean-field benchmark, we used a Poissonian "small-world" network which enabled us to taking care of degree heterogeneity (i.e. different social contacts), family-clusters and their overlaps, small-world feature, given the fact that "leisure" activities may connect nodes that are diametrically opposite. Moreover, by changing the number of the average degree $D$ we may pass from light ("exponential spreading" for $D\simeq5$) to severe ("linear spreading" for  $D\simeq2.5$) NPIs measures, i.e. a severe lock-down.
The Poissonian "small-world" network could be realized by imposing, at first, a Poissonian degree sequence on all nodes ("Configuration Model"); then, re-link the nodes to their closest neighboring nodes, i.e. a nearest neighbors rewiring (NNR) procedure is applied. Thus, obtaining a network with poissonian degree distribution and the small-world property.
The (pseudo) linearity obtain with this graph is reported in the \ref{fig::NNR_Conf_Model}.

Furthermore, the work focused on analyzing the network spreading on a fully-connected cavemen graph, where the families cores could be taken into consideration; and also enabling the possibility of a "long-range" connection via the rewiring probability $p = 0.1$ (cfr. \ref{fig::Caveman_Model}). Finally for what concerns the different network models, it has been studied the case of "(pseudo) linear" epidemics on a Barabási-Albert (B-A) Model, i.e. with the presence of hubs (cfr. \ref{fig::Barabási_AlbertSIR}).

Moreover, the critical average degree $D_c$ \footnote{$D_c$ as reported in \cite{Thurner::NetBasedExpl} describes a "first-order-phase transition" between exponential and linear growth of an epidemic} , is going to be recovered for the quoted network topologies. A final up-to-date point, would be to use a Graph Neural Network architecture, based on the previously described scenarios and networks, in order the make epidemics forecasting. For this purpose, a relevant reference, is \cite{Davahli::USA_predicting_COVID19}




\section{The Epidemiological Models}

\section{Goals Outline}

\begin{enumerate}
	\item Assuming that a typical social network has an average number of contact (degree) $D\simeq16.5$(\cite{Liu::2021_Review_SContactPattern}\footnote{A "contact" is defined as per touching or talking at a certain distance. For a spreading of a SARS disease, this open the discussed topic on the validity of the "2 meters" distance policy \cite{Jones::2020_2metersSDIstance}}); while lockdown measures reduce it to the household size ($D\simeq3.5$\cite{Liu::2021_Review_SContactPattern}), our goal is to reproduce the dynamics of an epidemic spreading on a sparser network due to the Non-Pharmaceutical Interventions (NPIs) generally known as lockdown\footnote{For brevity, the word "lockdown" is an alias for all the "lockdown-like" measures undertaken to prevent the diffusion of COVID-19 without requiring Pharmaceutical drug treatment, e.g. confinement (the direct translation of "lockdown") but also contact tracing, higher hygiene, face masks,}. A direct comparison of the obtained curves with the states of Austria and the USA \cite{Thurner::NetBasedExpl} is satisfactory even at a plain-eye inspection. However, the project aims at studying of how the topology of a network could characterize a paradigmatic model as the SIR one. 
    So, no statistical fit has been performed, since a relevant work on the available data has to be performed, e.g. removing out-of-range values, impossible data combinations, and missing values on \href{https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series}{\textit{GitHub John Hopkins University}}\footnote{https://github.com/CSSEGISandData/COVID-19/tree/master/csse\_covid\_19\_data/csse\_covid\_19\_time\_series}. In addition, for the fitting procedure, it would be possible to extend the SIR model in accordance to the other reported compartments, e.g. death (SIRD), exposed (SEIR),$\dots$
	Anyway, these states express the (quasi-)linearity in the cumulative number of cases similarly to the simulated ones.
    Moreover, it has to be pointed out that the previous quoted states are representative of two opposite political policies, undertaken by many others states, in facing the COVID-19 pandemic: a immediate "severe lockdown" (Austria) and "light restriction" on contacts (USA).
	\item As claimed in \cite{Thurner::NetBasedExpl}, we want to find that for any fixed transmission rate, recovery rate and "long-range" probability ($\beta, \, \mu, \, p$) there exists a critical number of social contacts ($D_c$) which describes a first-order phase transition of the disease growth obtained with the SIR model:
	\begin{itemize}
		\item for $D<D_c$, linear growth, i.e. low infection prevalence, occurs;
		\item for $D>D_c$, "classical" exponential growth as predicted by the SIR model.
	\end{itemize}

    The rationale behind is that, since $R_0 := \beta\, D/\mu$, by augmenting the average degree of the underlying network also the $R_0$ is enhanced, resulting in a more powerful spreading.
    \item On the way to obtain $D_c$, the simulated plots show a strongly dependence on $p$, i.e. the probability of a node to have a distant neighbor ("long-range" attachment), which is as crucial as $\beta, \mu \textnormal{ and } D$ in the characterization of a epidemic spreading. Therefore, a new definition of $R_0 := R_0 $ is proposed, which integrate the possibility that same effective parameter but with different long-range attachment (e.g. one local and the other random attached) could drive to different scenarios.
	Finally, by setting our parameters to the empirical estimates of $\beta$ and $\mu$, we are going to estimate $D_c$.
\end{enumerate}

\newpage
\section{Relevant Figures}
\begin{verbatim}
	FIGURES ARE AFTER "\textbf{\end{document}}"
\end{verbatim}


\chapter[Model]{Model}
The main assumptions of the model are:
\begin{itemize}
    \item fixed number of total individuals $N$ connected by a social link;
    \item "undirected" adjacency matrix $A$, where $A_{i,j}=1$ if there's a link between the $i-th$- and $j-th-$node or $A_{i,j}=0$ otherwise;
    \item transmission per day probability $r$ and $d$ days for an infected individual to be recovered, i.e. immune or naturally dead;
    \item average degree $D$ and shortcut (or rewiring) probability $\epsilon$
\end{itemize}

As one may expect, the $r$ and $d$ parameters are in a one-to-one map with to the SIR ones, respectively, for the recovery and transmission rates:
\begin{equation}
    \label{eq::d/r to gamma/beta}
    \gamma=1/d, \quad \beta=rD/N.
\end{equation}

At each time step (day), the infection curve of positive cases, $P(t)$, is the cumulative sum of the number of new cases $C(t)$.
So, if $D>D_c$ the "well-mixed" (or homogeneous/mean-field) approximation may be applied, with the result that $C(t)$ is associated to $R(t)$ up to a time-shift of $d$, since all the infected are predicted to become recovered after $d$ days.

\chapter[Results]{Results}
\section{Infection Dynamics}
To gain some intuition about the importance of modeling the underlying network, we've made the disease spread for two different values of $D$ but fixing $N_{total}=1000,\,D=8, \, \epsilon=0.1, d=6 \, \textnormal{days},r=0.1$ and $10$ initially infected nodes as seeds ($N_{seeds}=10$). \newline
The results are the following:

\begin{itemize}
    \item In the large $D$ and $\epsilon$ limit, the mean-field condition is fulfilled and the model resembles the compartmental  SIR model. By mapping the parameters as done in \ref{eq::d/r to gamma/beta}, we may compare $P(t)$ from the network model with the compartmental $R(t)$ by shifting it of $-d$ days.
    As a relevant reference for the comparison to the "small $D$" evolution, we've plotted the histogram of the daily cases which shows the typical peak in the early-exponential stages and, then, a decrease towards herd immunity, which for \cite{Thurner::NetBasedExpl} has been reached at the $98\%$ of the overall population. 
    \item By reducing the average degree to $D=3$, we've find a changing in the spreading behavior, since $P(t)$ increases almost linearly for a remarkable timespan. At the end of the infection, nearly $1/5$ of the entire population has been infected which is noticeably smaller than the SIR predicted herd immunity nearly at $80\%$.
\end{itemize}

With this rough perception of the main results by considering the underlying network structure, we've further analyzed the parameter dependence.

\section{Parameter Dependence and Phase Transition}
The aim of this section is to obtain the value of the "critical average degree" $D_c$, such that, for $D<D_c$, the infection curve is (roughly) linear. In order to take care of the two growth trends, we've defined an order parameter 
\begin{equation}
    \label{def::sdc}
    \mathbb{O}:=SD(C(t))
\end{equation}{}
as the standard deviation of the daily new cases, having removed the days without new infected. In this way,     $\mathbb{O}!=0$ signals the presence of a nonlinear increase of the daily new cases and, therefore, of its cumulative sum $P(t)$. Other way around, $\mathbb{O}=0$ corresponds to a linear growth of $P(t)$.


\chapter{Appendix}

\section{Notes on Network book of A.Barabási}

\subsection{Compartmental Models 10.2}
\subsubsection{SI model}
At $(10.1)$ $\beta$ is the probability of 1 spread only. So, $\frac{\beta \, \langle k \rangle S(t)}{N}$ expresses the probability of 1 spread over $\langle k\rangle S(t)/N$ neighbors. \\
$C:=\ln{\frac{i_0}{1-i_0}}$

\subsubsection{SIS model}

In $(10.7)$, define $v_{SIS}:=1-\frac{\mu}{\beta*\langle k\rangle }=1-R_0^{-1}$ the "characteristic velocity". If $v>0$ spread; otherwise it dies out. So, $\tau^{SIS}=\frac{1}{\beta\langle k\rangle v_{SIS}}=\frac{\tau^{SI}}{v_{SIS}}$ In, SI model $v_{SIS}=1$ \cite{barabasi::2016networkbook}
\newline
The endemic fraction of infected is given by $\frac{\beta-\mu}{\beta}$ slides 16 of the LDE course.

\subsection{Epidemics on Nets 10.3}

\subsubsection{SI Model}
$10.16:$ $$\tau_n^{SI}=\frac{\tau_{c}^{SI}}{\frac{\langle k^2 \rangle -\langle k\rangle}{\langle k\rangle^2}}$$
\newline
$10.17:$ is obtained by imposing the initial condition $i(t=0)=i_0$. Anyway, $i_k=i_0+i_0f(t)$

\subsubsection{SIS}

$$\tau_N^{SIS}=\frac{\tilde{k}^2}{\beta\langle k\rangle-\mu\tilde{k}^2}$$ 
\newline
where $\tilde{k}^2:=\frac{\langle k\rangle^2}{\langle k^2 \rangle }$ and ???? if the network is homogeneous
$\langle k\rangle^2=\langle k^2 \rangle .$
Check better what's the behavior of $\langle k^2 \rangle $ with respect to the considered network.

\subsection{10.B}

If a nets lacks degree of correlation, i.e. $e_{ij}=q_iq_j$, the probability $p_{kk'}$ is independent on k. Thus, $$p_{kk'}=\frac{k'p_k'}{\langle k\rangle}.$$

Typo under $(10.50)$ multiply by $kp_k$ not $k-1)p_k$

\chapter{Results}


\chapter{Conclusion}





\begin{figure}[h]
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\linewidth, height=8cm]{WS_Pruned_AdjMat_N1000_D14.0_p0.0.png}
        \centering
        \caption{Network information for the regular "D-halving" graph}
        \label{fig::adjmat_pruning_p0.0}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth, height=8cm]{WS_Pruned_SIR_p0_9}
        \caption{(pseudo) linearity of the total number of infected for the "halving-D" method with $p = 0$}
        \label{fig::pruning_p0.0}
    \end{subfigure}
\end{figure}

\newpage
\begin{figure}[h]
	\begin{subfigure}{0.4\textwidth}
		\includegraphics[width=\textwidth, height=9cm]{WS_Pruned_AdjMat_N1000_D6.0_p0.1.png}
		\centering
		\caption{Network information for the E-R model with $p = 0.1$}
		\label{fig::adjmat_pruning_p0.1}
	\end{subfigure}
	\begin{subfigure}{0.4\textwidth}
		\includegraphics[width=\textwidth, height=8cm]{WS_Pruned_SIR_p0.1_4_2.png}
		\caption{(pseudo) linearity of the total number of infected for the E-R model with $p = 0.1$}
		\label{fig::pruning_p0.1}
	\end{subfigure}
\end{figure}

\newpage
\begin{figure}[h]
	\begin{subfigure}{0.4\textwidth}
		\includegraphics[width=\textwidth, height=9cm]{NNR_Conf_Model_AdjMat_N1000_D6_p0.0.png}
		\centering
		\caption{Network information for the "Poissonian" small-world network with $p = 0$}
		\label{fig::adjmat_NNR_Conf_Model}
	\end{subfigure}
	\begin{subfigure}{0.4\textwidth}
		\includegraphics[width=\textwidth, height=8cm]{NNR_R0_1-2_p0.0_27.png}
		\caption{(pseudo) linearity for the nearest neighbors rewiring network with $p = 0$}
		\label{fig::NNR_Conf_Model}
	\end{subfigure}
\end{figure}

\newpage
\begin{figure}[h]
	\begin{subfigure}{0.4\textwidth}
		\includegraphics[width=\textwidth, height=9cm]{Caveman_Model_AdjMat_N1000_D5.0_p0.1.png}
		\centering
		\caption{Network information for the Caveman network model with $p = 0.1$}
		\label{fig::adjmat_Caveman_Model}
	\end{subfigure}
	\begin{subfigure}{0.4\textwidth}
		\includegraphics[width=\textwidth, height=8cm]{CavemanMod_R0_1-3_p0.1_18.png}
		\caption{"(pseudo) linearity" for the Caveman network model with $p = 0.1$}
		\label{fig::Caveman_Model}
	\end{subfigure}
\end{figure}

\newpage
\begin{figure}[ht]
	\begin{subfigure}{0.4\textwidth}
		\includegraphics[width=\textwidth, height=9cm]{B-A_Model_AdjMat_N1000_D4.0_p0.0_m2_N0_2.png}
		\centering
		\caption{Network information for the B-A graph with $p = 0$}
		\label{fig::adjmat_B-A_Model}
	\end{subfigure}
	\begin{subfigure}{0.4\textwidth}
		\includegraphics[width=\textwidth, height=8cm]{B-A_Mod_R0_0-1_p0_6.png}
		\caption{"(pseudo) linearity" for the B-A graph with $p = 0$}
		\label{fig::Barabási_AlbertSIR}
	\end{subfigure}
\end{figure}