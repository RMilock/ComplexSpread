\documentclass[a4paper,10pt,twoside]{book} %%{book}

\usepackage[top=5cm,bottom=5cm,inner=5cm,outer=3cm]{geometry}
\usepackage{graphicx, subcaption} %per poter inserire le figure
\usepackage{csquotes} %handle " and ""
\MakeOuterQuote{"}  %handle opening/closing quotation
\usepackage{amssymb,amsmath,amsthm,amsfonts,bm}
\usepackage{bookmark}% http://ctan.org/pkg/bookmark
\usepackage{appendix}
\usepackage{xspace}
\usepackage{tabularx}
\usepackage{indentfirst}
%\usepackage{subfigure}
%\usepackage[small]{caption}
\usepackage{caption}
\usepackage{eucal}
\usepackage{eso-pic}
\usepackage{url}
\usepackage{booktabs}
\usepackage{afterpage}
\usepackage{parskip}
\usepackage{listings}
\usepackage{fancyhdr}
\usepackage{textcomp}
\usepackage{cite}
\usepackage{multirow}
%\usepackage[utf8]{inputenc}   %per riuscire a scrivere gli accenti, but w/ this an error with csquote is raised. So, use \usep[english]{babel} only.
\usepackage[english]{babel}   %per riuscire a scrivere gli accenti
\usepackage{setspace}
\usepackage{etoc}
\usepackage{etoolbox}
\usepackage{setspace}
\usepackage{verbatim}
\usepackage{color}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{tocbibind}
\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{braket}
\usepackage{multirow}
\usepackage{float}
\usepackage{units}
\usepackage{siunitx}
\usepackage{bm}
\usepackage{bigints}

\usepackage{pgfplots} % to make plot inside LateX
\pgfplotsset{compat = 1.16}

\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{mathrsfs}
\usepackage{bbold}
\usepackage{mathtools}
\usepackage[style=ddmmyyyy]{datetime2}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\pagestyle{fancy} 

\graphicspath{{../images/}}

\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\Huge}

\definecolor{brown}{rgb}{0.28, 0.02, 0.03}
\hypersetup{
colorlinks,
citecolor=blue,
filecolor=blue,
linkcolor=blue,
urlcolor=blue
}

%\makeatletter
%\renewcommand\tableofcontents{%
%    \section*{\Huge{\contentsname}}%
%    \@starttoc{toc}%
%}
%\makeatother

%\NewDocumentCommand{\evalat}{sO{\big}mm}{%
%  \IfBooleanTF{#1}
%   {\mleft. #3 \mright|_{#4}}
%   {#3#2|_{#4}}%
%}

\begin{document}
\frontmatter


\newgeometry{top=2cm,bottom=2cm,left=2cm,right=2cm}
%%old for {book}: \frontmatter
%below: textsc = font "small caps"
\begin{titlepage}
\vspace{5mm}
\begin{figure}[hbtp]
\centering
\includegraphics[scale=.13]{../images/unipd_logo.png}
\end{figure}
\vspace{5mm}
\begin{center}
{{\huge{\textbf{\LARGE UNIVERSIT\`A DEGLI STUDI DI PADOVA}}}\\}
\vspace{20mm}
{\Large{\bf Dipartimento di Fisica e Astronomia "Galileo Galilei"}} \\
\vspace{5mm}
{\Large{\textsc{\bf Master Degree in Physics (LM-17)}}}\\  
\vspace{20mm}
{\Large{\textsc{\bf Final Dissertation}}}\\
\vspace{30mm}
{\Large{\textsc{\bf Modelling COVID-19 In a Network}}}\\
\vspace{45mm}
\end{center}

\begin{spacing}{2}
\begin{tabular}{lccccccccccl}
	{\Large{\bf Candidate}} &&&&&&&&&& {\Large{\bf Thesis supervisor}}\\
	{\Large{\bf Riccardo Milocco}} &&&&&&&&&& {\Large{\bf Prof./Dr. Marco Baiesi}}\\
\end{tabular}
\end{spacing}
\vspace{15 mm}

\begin{center}
{\Large{\bf Academic Year 2020/2021}}
\end{center}
\end{titlepage}

\restoregeometry

\clearpage{\pagestyle{empty}\cleardoublepage}

\pagestyle{empty}

\vspace*{\fill}
\tableofcontents
\vspace*{\fill}

	

%-----------------------------------------------------------------------

%\newgeometry{top=4cm,left=4cm}
\newgeometry{left = 4cm, bottom = 2cm, right = 2cm, top=4cm}
\chapter*{Abstract}
The usual simplified description of epidemic dynamics predicts an exponential growth. This is due to the mean field character of the dynamical equations. However, a recent paper (Thurner S, Klimek P and Hanel R 2020 Proc. Nat. Acad. Sci. 117, 22684) \cite{Thurner::NetBasedExpl} showed
that in a network with fixed connectivity, the nodes become infected at a rate that increases linearly rather than exponentially.
Experimental data for COVID-19 seem to validate this approach. In this thesis we plan to study this model by tuning its parameters.
In particular, we monitor the effect induced by a significant presence of hubs in the network.

\chapter*{Motivations}
Last Update: \today
"The COVID-19 pandemics has led to a dramatic loss of human life worldwide and presents an unprecedented challenge to public health, food systems and the world of work"\cite{Chriscaden::2021_ImpactCOVID19}. In this scenario, monitoring and forecasting the diffusion of the virus is an essential tool for policymakers to handle the health-care resources and restrictions among individuals. To this purpose, many academics, e.g. \href{https://web.unipd.it/covid19/en/}{UniPD against COVID-19}\footnote{https://web.unipd.it/covid19/en/}, have been trying to tackle the COVID-19 spreading as well as its side-effects, which have to be considered on the "path to normality". This drastic shift with respect to the "path to herd immunity" comes from a recent study \cite{GU::2021_SitePathToNormality} which shed light on the fact that "herd immunity", due to delay on vaccinations, could probably not be reached for the end of 2021. On the other hand, due to few practical points \cite{Nature:18.3.2021_NoHerdImmunity}, it is also probabile that the long-term prospects include COVID-19 becoming an endemic disease, much like influenza. 
Therefore, the recovery of the pre-COVID-19 situation could be slow even in the presence of effective vaccines.

So, an interesting path to cover is to describe the evolution of the COVID-19 pandemic, e.g. with the SIR model \cite{pizzuti::2020_ItalyCOVIDnetwork}, on different network topologies in order to simulate the different containment policies, i.e. "lock-downs".

%%old for {book}: 
\mainmatter

\newcommand{\changefont}{%
    \fontsize{12}{12}
	%\selectfont
}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{\changefont \slshape \nouppercase{\rightmark}} %section
\fancyhead[RE,LO]{\changefont \slshape \nouppercase{\leftmark}} %chapter
\fancyfoot[C]{\thepage}

\chapter[Introduction]{Introduction}
%\include{cap_Introduction}

\section{Complexity}
Since its beginning, Physics has been driven by a "Unification Principle"\footnote{from Newton with the "gravity"-relationship between an apple and the Moon; passing to Einstein, whom the effects of gravity are equal to the ones of an accelerated rocket for; ending to the \textit{Grand Unified Theory} as an unification of of general relativity with the particle physics},which aims at describing the reality as a "reduction" and simplification of its constituents and interactions. Nevertheless, this optimistic gaze, is (amazingly) challenged by the complexity that emerges in the world we are living in. This complexity could have broader origins but also different spatial scales: from ants' nest to storms, from the markets to human/animal migrations, from the epidemics to ecosystems. Therefore, Nature is capable of obeying simple laws but, on the other hand, to straightforwardly generate complex phenomena. A paradigmatic example is the flocking of birds: the core of the flock assumes different shapes due to a collective interaction among the birds; while an alone bird would proceed in a defined direction.
These facts were enough to establish the new branch of \textit{Physics Of the Complex Systems} which ultimate goal is to describe the emergence of the "complex" phenomena from the simple law of physics, e.g. "gravity model" \cite{GravityModelsandEmpiricalTrade}.
Having understood that \textit{"More is different"} \cite{Anderson:1972_MoreIsDifferent} and its ubiquity, it is natural to demand a formal definition of "Complexity". Nevertheless, quoting the Nobel Price Murray Gell-Mann \cite{Gell-Man:1987_S&C}, "a variety of different measures would be required to capture all our intuitive ideas about what is meant by complexity and by its opposite, simplicity" . In fact, from computational complexity to other information measures, all such quantities are context-dependent and subjective, since they depend on many aspects, such as the detail level (coarse graining) of the entity to describe. Thus, a rough but clear definition could be drawn for the Complex System Society site \cite{CSS:2021_compsystdef}:

\begin{definition}[\textit{"Complexity"}]
Complex systems are systems where the collective behavior of their parts entails emergence of properties that can hardly, if not at all, be inferred from properties of the parts.
\end{definition}

In other words, the non-linear interactions among the parts of which a complex system is composed let emerge a behavior which could not be understood simply by applying the fundamental laws to its constituents.

The first representation of this weaved system is by considering entities and connections among them. Thus, a powerful tool to describe it is the Network Science.
In particular, the purpose of this thesis is to study the epidemic spreading of the COVID-19 over different topologies of the underlying social network of individuals. This phenomenon is regarded as a complex system through the analysis of the SIR model, which grasps the emergent features rather than the microscopic behavior of the disease.

\subsection{Models Ecosystem}
Infectious epidemics are responsible for a significant health and economic burden on society. New diseases appear frequently and old diseases persist. Therefore, many mathematical models have been used recently to guide policy makers to mitigate the impact of new infectious diseases (e.g. Polio, H1N1 influenza and Ebola) or established ones (such as HIV, cholera and seasonal influenza).
On the other hand, with the phrase \textit{All models are wrong; some models are useful} \cite{Box::2005_StatDesign}, George Box wanted to highlight that no model, no matter how complicated\footnote{Instead, the complexity of a model could challenge its possibility to be predictive in many scenarios. A related topic is the "over-fitted model" in statistical learning or "Occam's razor" in Philosophy}, is perfect. Thus, a way to evaluate a "successful" model has to be recovered.

Inside the "ecosystems" of models, there are the simple models which allow to analytically explain how the primary mechanisms influence important characteristics (e.g. epidemic threshold, epidemic size\footnote{how large it will be}, how long it will last). To be more accurate, it is also possible to develop models that incorporate much more details about both the disease and individual-level interactions \cite{Kiss::MathOfEpiOnNet}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width = .4\linewidth]{perfect_model.png}
	\caption{A caricature of the relation between model realism, complexity and the
	insight the model provides \cite{Kiss::MathOfEpiOnNet}.}
	\label{fig:perfect_model}
\end{figure}	

As suggested by the \autoref{fig:perfect_model}\footnote{\textit{"Est modus in rebus"} \label{cit:GM}} (\cite{Kiss::Ch1MathOfEpiOnNet}), models provide maximum insight where the right balance of realism (circle filled with dots, e.g. regarding epidemiology Graph-Neural-Network to predict $R_t$ \cite{Davahli::USA_predicting_COVID19}) and simplicity (circle with vertical lines e.g. on epidemics the deterministic SIR model ) is met\footnote{The equilibrium point may depend on the specific posed question} (central circle with oblique lines).

In this middle region, there could be found several works that use network analysis as the underlying structure for a phenomenon(\cite{Thurner::NetBasedExpl} \cite{VespignaniSatorras2001Epidemic} \cite{pizzuti::2020_ItalyCOVIDnetwork}); while other, more involved, that incorporate human mobility data in several hybrid models \cite{ZEROUAL::DL_COVID19, Stubinger::Incidence_Diff_Countries} in order to predict its diffusion. For the sake of simplicity, the present thesis is going to deepen the paper of \cite{Thurner::NetBasedExpl}. In particular, Thurner et al. \cite{Thurner::NetBasedExpl}, by focusing on the COVID-19 spreading in Austria and in the USA, are able to find a scenario, i.e. a combination of the SIR spreading parameters\footnote{the infection rate $\beta$, recover rate $\mu$, the long-range attachment probability $p$}, such that the outbreak is (quasi-)linear for a value of the average number of contact $D$. This behavior of the SIR model is qualitatively comparable with the curves reported by the JHUniversity reported below. 
So, inspired by \cite{Thurner::NetBasedExpl}, the aim of this project is to analyze how the SIR model behaves by changing its "inner" epidemic parameters but also the topology of the underlying social network, i.e. the susceptible people.

\section{A Technical Introduction}
\label{sec:ATechIntro}
At $8 \, \textnormal{May} \, 2020$, none of the affected COVID-19 states have reached "herd immunity" but still they reached the "epidemic peak" due to the containment restrictions on social contacts. One paradigmatic example is the case of Austria, for which the total infected cases at the first peak were $0.16\%$ of the total population, remarkably low with respect to the SARS-COV-2 "herd-immunity" level that are supposed to be at $0.8\%$ (\cite{Zingano:2021_HI_hom_pop}) of the entire (susceptible) population. A graphical representation of the cases could be found in \autoref{fig:USA-AUT-ITAtotalcasesOWID}, \autoref{fig:USA-AUT-ITAtotalcasesOWID} produced by \href{https://ourworldindata.org/coronavirus/country/austria}{Austria New Daily Infecte}.
\begin{figure}[htbp]
	\centering
	\includegraphics[width = \textwidth]{Introduction/COVID-RealStates.png}
	\caption{Normalized number of total cases until 8.5.2020. In the legend, there are reported the percentage of "$max / totp$" where "$max$" is the total cases at the end, while "$totp$" is the total population for each state according to \cite{PopulationEstimate}}
	\label{fig:USA-AUT-ITAtotalcasesOWID}
\end{figure}
The most striking observation is that the number of total cases (\autoref{fig:USA-AUT-ITAtotalcasesOWID}), after early-stages when Non-Pharmaceutical-Interventions (NPIs), i.e. lockdown, were at work, exhibits a (quasi-)linear growth for an extended time interval in contrast with the "S-shaped" logistic curve predicted by the standard compartmental models. The extension of the linear regime depends on the onset of the measures; while for early stages, as it was the case for many countries \cite{Thurner::NetBasedExpl} ($8 \, \textnormal{May} \, 2020$), an exponential growth dominates the spreading of the disease.
By taking care of the modification of the underlying social network structure, in this present thesis, we want to (qualitatively) recover the spreading trends for the states that have or have not applied the NPIs measures. \newline
Specifically, an infection may occur for two reasons:
\begin{enumerate}
    \item interaction between an infected and a susceptible person;
    \item contact is "intense" (e.g. long, close,...) enough to lead to a disease transmission.
\end{enumerate}
So, the rationale behind the social distancing is that it takes to a reduction of both of these factors.
On the other hand, the analytically solvable SIR model assumes that, defined $N, D$ as the total number of individuals in the population and the average number of contacts respectively, there is the same probability that an individual encounters an infected person ("well-mixed population") and all the nodes have the same number of neighbors ($N-1$ or $D$). This approach allow to recover analytical results for an epidemic spreading but it annihilates the underlying network structure. Therefore, as claimed in \cite{VespignaniSatorras2001Epidemic}, there is the need of studying how the network affects the spreading of a disease, but still no focus is put on the spreading below the epidemic threshold \cite{Thurner::NetBasedExpl} as it is the case if nodes were separated through NPIs. 
In this thesis, the main goal is to grasp the relevant features of a complex social network in presence of NPIs and compare them with respect to a "well-mixed" population model.
In detail, by changing the epidemic parameters, we let the SIR Model evolve in different network topologies such as standard models of the graph-theory ("Erdös-Rényi","Scale-Free", "Caveman Model") but also a new involved topology, i.e. a poissonian Small-World Network. As a benchmark of a "well-mixed" population, it has been used an "annealed" mean-field (see ??) which is a natural realization of a mean-field network with a fixed average number of contacts per node.
Furthermore, $R_0 := \beta \cdot D/\mu$ is heuristically defined as the the average number of new infections caused by an individual in a completely susceptible population\footnote{The susceptible individuals are chosen according to "epidemic targets", e.g. only children for the measles} \cite{Kiss::MathOfEpiOnNet}. Moreover, it is well consolidated that for $R_0 > 1$ an epidemic is possible (not guaranteed); while for $R_0 < 1$ the disease will die out. 
Thus, these informations drive naturally to suppose that $R_0$ could also be a signal of the strength of an epidemic outbreak. Instead, this is not the case, since, as reported in the section ??, for equal $R_0$ values there could be different behavior of a disease. Thus, a new parameter has to be introduced that has to:
\begin{itemize}
	\item grasp the different behavior of epidemics;
	\item depend only on the initial condition, i.e. network metrics (e.g. "long-range" parameter $p$) and disease parameters ($\beta \textnormal{ and } \mu$).
\end{itemize}

A heuristics solution could be to characterize each spread via $R_{new} := \beta \cdot D/\mu * \mathbb{O}$ where $\mathbb{O} = \frac{D}{N-1}$, or equally $\mathbb{O} = D$. Thus, $R_0$ would still be considered as the secondary infected in a fully susceptible network; while $R_{new}$ as the "strength" a spread has due network limitations.

Finally, as in \cite{Thurner::NetBasedExpl}, a "epidemic" order parameter\footnote{Precisely (see ??), it is the standard deviation of the daily new infected} is computed for each spread to classify a light outbreak, i.e. an outbreak which infect less than the $30\%$ of the entire population, with a stronger one. In this way, a first-order phase transition could be obtained by plotting, for sufficiently high $p$\footnote{$p$ is the long-range rewiring probability.} the order parameter against the average degree of contacts.

On the top of the presentation of the work developed, it seems natural to deepen the area of the Network Science.


\section{Network Science}

Two aspects contribute to the popularity of Network Science: its universality nature of networks and the availability of large datasets\footnote{and the computational power to analyze them} which map huge networks of the furthest kind, e.g. "food-webs" or the Science of Science.
Its roots could be found in the Graph Theory, devoted to the analysis of the networks (or graphs), and Statistical Mechanics, committed to the formalization of the dynamical processes on network, such as self-organization and information theory.
For our purpose, the Graph Theory allows to formalize a "networky" - complex system such as the social network of the individuals over which a disease spreads; while Statistical Mechanics to develop a model focusing on its emergent characteristic.

\subsection{Graph Theory}
\label{sec:GraphTheory}
\begin{figure}[htbp]
	\centering
	\includegraphics[scale = 0.7, trim = {3cm 2cm 4cm 5cm}, clip]{Basic_SN}
	\caption{Contacts interaction Network. A contact is defined as the droplets that reach one individual. Therefore, a symmetric edge could represent a chat among two persons; where directed edges would be a "one-way" interaction such as in a room with pre-existent droplets. The number above each edge are the weights, which could be the minutes spent in the previous quoted room}
	\label{fig:basicSN}
\end{figure}

A graph is a tuple (V,E) where the "V" stands for "vertexes" (1,2,3,4) while "E" for "edges" (the links between the vertexes).
With the same notation, an edge connecting the node\footnote{In the context of a social network, a vertex represents a person/individual} $i$ to $j$ is, often, identified by $(i,j)$.
Formally, the labels of the vertexes are the image of any bijective embedding from the nodes to a chosen set, e.g. integer numbers. The relationship among vertexes are accounted by the edges, which could be unsymmetrical, as for $(1,4)$, and with different strength of the connection, such as $(1,4) \text{ and } (1,3)$ which. If all the edges are symmetric, i.e. if (A,B) implies (B,A), the graph is said undirected, e.g. the friendship network; otherwise it is said directed, e.g. the phone contacts network. 
With the given definitions, it is now clear that roughly everything could be divided in vertexes connected by am arbitrary relation.
The simplest example, is the \textit{simple graph}, which is an unweighted, undirected graph containing no graph loops, i.e. $(i,i)$ edges, or multiple edges such as multiples-$(i,j)$.
\begin{figure}[ht]
	\includegraphics[width = \linewidth]{SimpleGraph_950.png}
	\caption{Edges peculiarities. The labels are $1,2,3,4$ assigned anti-clockwise starting from the bottom}
	\label{fig:simple}
\end{figure}
\newpage

\subsection{Adjacency Matrix}
Typically for real networks, the number of involved nodes is big, e.g. the Google web-graph available at \href{https://snap.stanford.edu/data/#socnets}{Standford University Network Dataset} is of the order of the $800k$ nodes and $5milion$ edges. Thus, despite the clearness of \autoref{fig:basicSN}, the most suitable way of representing a graph is by grouping all its properties in a matrix called the adjacency matrix $A$. 
Starting from the basics, the (symmetric) adjacency matrix for the simple graph above is:
\[
A_{ij} :=
\begin{cases}
1 & \text{if exist an unweighted edge among i and j} \\
0 & \text{otherwise}
\end{cases}
\quad
\Leftrightarrow
\quad
A = 
\begin{bmatrix}
0 & 0 & 1 & 0 \\
0 & 0 & 1 & 1 \\
1 & 1 & 0 & 1 \\
0 & 1 & 1 & 0 \\
\end{bmatrix}
\]

Hereby, $A \in M_4(\mathbb{N})$ holds the nature of a simple graph:
\begin{itemize}[noitemsep]
	\item the rows represent the direct connections between the $i-th$ row and the $j-th$ column; while, the columns the edge $(j,i)$. Thus, there are only 4 nodes;
	\item unweighted: the value of the elements of $A$ is binar;
	\item without loops: the diagonal is $0$;
	\item without multi-edges: the value of the entries of $A$ are strictly $0$ or $1$, account for the presence of, at maximum, one edge.
\end{itemize}

A more involved, adjacency matrix is the one of the rightest graph which present multiples loops\footnote{As far as multi-edges are concerned, the $2$ would be off-diagonal}.
Formally,
\[
A = 
\begin{bmatrix}
0 & 0 & 1 & 0 \\
0 & 0 & 1 & 1 \\
1 & 1 & 0 & 1 \\
0 & 1 & 1 & \textbf{2} \\
\end{bmatrix}
\]

Thus, in \autoref{fig:basicSN} is report a (simple) weighted and directed graph with 4 nodes connected by a "central"\footnote{The "centrality" measure of the node is another important quantity which is context-dependent. In this case, the "centrality" refers to a degree centrality for which the most central node is the one with the highest number of contacts} node $1$.
In particular,
\[
A = 
\begin{bmatrix}
0 & 1 & \textbf{5} & 1 \\
1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\ 
\end{bmatrix}
\]
where it is encoded the direct edges since $A$ is not symmetric.

\subsection{Degree}

\begin{figure}[ht]
    \begin{subfigure}{\textwidth}
        \includegraphics[scale = 0.4]{../images/Networks/DegreeClustCoeffPathLeng}
        \centering
        \caption{Network measures \cite{Olaf:2011_NonRandomBrain}}
        \label{fig:degree_clustcoefficient_pathlength}
    \end{subfigure}
\end{figure}


As quoted before, a (real) network could be very tangled. Thus, the degree distribution, i.e. the normalized number of vertexes with a certain degree, allows to check whether all the nodes have, or not, the same number of contacts even at a plain eye inspection. 

Formally, the degree of a vertex, in a (undirected) graph, is the number of edges connected to it. We denote the degree of node $i$ by $k_i$ and, by mean of $A$, $$k_i := \sum_{i=1}^{N} A_{ij},$$ where $N$ is the number of total nodes.
For directed graphs, as in \autoref{fig:basicSN}, a distinction between the entering and exiting edges to a node has to be defined. In particular, by a generalization of the previous $k_i$, 
\begin{equation}
	k_i^{in} := \sum_{j=1}^N A_{ij}, \quad k_j^{out} := \sum_{i=1}^N A_{ij}.
	\label{eq:kin_kout}	
\end{equation}
where $k_i^{in(out)}$ is the number of in-going (out-going) number of edges. 

\begin{figure}[t]
\begin{subfigure}{.5\linewidth}
	\begin{tikzpicture}
	\begin{axis}[ybar interval, 
		width = \linewidth,
		xtick align=inside,
		ymin = 0,%, ymax=55,ymin=0, minor y tick num = 3]
		ylabel = {Number of Nodes},
		xlabel = {In-degrees},]
	\addplot coordinates { (0, 0) (1, 3) (2, 0) (3, 0) (4, 0) (5, 1) (6, 0) };
	\end{axis}
	\end{tikzpicture}
\end{subfigure}%
\hfill
\begin{subfigure}{.5\linewidth}
	\begin{tikzpicture}
	\begin{axis}[ybar interval, 
		width = \linewidth,
		xtick align=inside,
		ymin = 0,%, ymax=55,ymin=0, minor y tick num = 3]
		ylabel = {Number of Nodes},
		xlabel = {Out-degrees},]
	\addplot coordinates { (0, 0) (1, 1) (2, 0) (3, 0) (4, 0) (5, 0) (6, 0) (7,1) (8,0) };
	\end{axis}
	\end{tikzpicture}
\end{subfigure}
\caption{Degree Distribution of the \autoref{fig:basicSN}}
\label{fig:degreed_basicSN}
\end{figure}

Therefore, by applying the \autoref{eq:kin_kout}\footnote{A practical check is to count the tails for the out-degree and arrows for the inner-degree. Ultimately, each sum has to equate the number of edges (a undirected edge has to be count doubled)} to \autoref{fig:basicSN}, it is possible to recover the degree distribution of the vertexes as in the \autoref{fig:degreed_basicSN}. Thus, a possibile way to build a graph would be to draw the nodes degrees from a fixed distribution and, then, connecting the nodes according to their degree. In fact, this approach is going to be exploited in the further chapters by using a poissonian degree distribution and connecting the vertexes only to nearest neighbors via an ad-hoc algorithm.

\section{Chapters Outline}
In \autoref{ch:network-models}, the models used in this thesis are going to be exposed: starting from a "regular" Watts-Strogatz and Caveman models, passing to the random ones (Erdös-Rényi, Watts-Strogatz, Poissonian-Small-World models); then, ending with the scale-free ones, such as the Barabási-Albert model. 

In \autoref{ch:sir-models}, two kind of SIR models are presented: the mean-field SIR model and the network one, which combine the mean-field SIR with the network structure.

\chapter{The Network Models}
\label{ch:network-models}
As it could be perceived, modeling a complex (social) phenomenon\footnote{The "social" nature has been only evoked for concreteness. Nevertheless, this in-depth presentation of models is interdisciplinary as Network Science itself.}, as the epidemic spreading, in a network has two main tasks. The first is the identification of the nodes, which is a fairly simple task. The second is to pinpoint the emergent correlations in order to connect the vertexes and reproduce the phenomenon. The latter one is the real challenge in network theory.

%Thus, the goal is to artificially reproduce the main topological features of the real networks. 
Many networks are characterized by strong constraints on the node degree which forces a satisfying regularity to be present, e.g. the radial architecture of a spider net or the map of the main streets of Manhattan.
Inspired by this, a first insight, the pathogen is left to act on a regular society, which individual contact are differing at most by one, e.g. "regular" Watts-Strogatz and Caveman model.

Most of the real networks, however, do not show a comforting orderliness. 
%add an image of spider net, Manhattan and Internet Network
Rather, their inner structure seems to be characterized by a certain degree of randomness, e.g. the internet network \cite{barabasi::2016networkbook}. Random network theory embraces this feature by constructing network models that are truly random, e.g. Erdös-Rényi. Meanwhile, they are not able to capture the presence of hubs, alias the "Scale-Free" property. Thus, new models as to be proposed to generate those highly connected degree nodes, such as the Watts-Strogatz and Barabási-Albert models.
A review of these is reported in the following sections; alongside with the newly proposed "Poissonian Small World" network (see \autoref{sec:ATechIntro}, \cite{Thurner::NetBasedExpl}).

\section{Regular Networks}
\subsection{Lattice Graph}
\begin{figure}[ht]
    \begin{subfigure}{\textwidth}
        \includegraphics[scale = 0.4]{../images/Networks/RegGraphs}
        \centering
        \caption{In (a) - fully-connected graph; In (b) - $D = 4$ regular graph \cite{Zelazo:2011_RSensNet_images}}
        \label{fig:RegGraph}
    \end{subfigure}
\end{figure}

As briefly reported, the "Regular Networks" are characterized by a fixed number of neighbors per node $D = \langle k \rangle$. Thus, they are expected as the constituent graphs for the class of systems which constraints $D$ to a certain value, e.g. solids in the physics of matter.
Ultimately, by trying to study a new phenomenon, it is fundamental to test its behavior on a controlled environment as the one provided by the regular networks.

In \autoref{sec:WS_Model}, there will be presented the "Watts-Strogatz" model which depending by an inner parameter $p$ can interpolate between a regular graph for $p = 0$(\autoref{fig:RegGraph}) and a random network ($p=1$).
So, setting $N$ as the number of nodes, chosen $p=0$ and starting from a fully-connected network ( (a) figure in \autoref{fig:RegGraph}), the average degree $D = N-1$ is halved at each time-step. Thus, to preserve the secondary cases $R_0$, the infectivity of the pathogen $\beta$ is doubled at each time-step.

\subsection{Connected Caveman Graph}
\begin{figure}[ht]
    \begin{subfigure}{\textwidth}
        \includegraphics[trim={2cm 3cm 2cm 4cm}, clip, scale = 0.6]{../images/Networks/CavemanMod_Rew}
        \centering
        \caption{Disconnected and Connected with rewiring Caveman Model \cite{Taube:2005_IndianSoftwIndustry}}
        \label{fig:CavemanMod}
    \end{subfigure}
\end{figure}


By analogy with the NPIs measures which allowed only for the household interactions, it has been considered the "Caveman Model".

The rationale is to consider as if one node, of the previously modeled networks, would be a family composed by a fixed number of people. This idea drives to the "coarse-graining" approach seen in the previous sections.
In detail, the Caveman model enables to fix both the number of "caves"/families and the number of individual for each family. 

In the present thesis, it is going to be developed an intermediate version among the Caveman models shown in \autoref{fig:CavemanMod}. Indeed, connect the nearest caves of the left figure by creating a new edge on randomly choosing nodes within each cave. Then, introduce a "rewiring parameter" to enable "long-range" connections, resulting in "tunneling cuts" among distant nodes as in the right rewired graph. The ending graph is comparable to a Watts-Strogatz model, whose nodes are substituted with an entire cave.

A naive expectation is that the epidemic spread slower and more stochastically, since the nodes are poorly connected with the "outside" rather than in the family.

\section{Random Networks}
Not all the networks are characterized by hubs as WWW or the airports graphs.
In fact, there could be constraints, such as cost-benefit problem, which limit the maximum degree of the bigger node. In those cases, random networks may be the best (thus, not conclusive) fit, e.g. national highways network, power grid of generators and switches, $\cdots$

\subsection{The Erdös-Rényi Model}
%insert a reference image for ER model

\begin{figure}[ht]
    \begin{subfigure}{\textwidth}
        \includegraphics[trim={14cm 0 0 1cm}, clip, scale = 0.6]{../images/Networks/ERmodel}
        \centering
        \caption{Erdös-Rényi Model for $p = 0.2$ \cite{Baronchelli:2017_EpidSpreadCompNets}}
        \label{fig:ERmodel}
    \end{subfigure}
\end{figure}

The Erdös-Rényi (ER - \autoref{fig:ERmodel}) model was the first attempt to formally describe a random network. 
In particular, these two mathematicians were the fathers of the random graph theory, obtained by merging graph theory with probability theory.

The primitive hypothesis, for a random wiring, is that the nodes are linked together with a probability $p$. Formally,

\begin{definition}[\textit{"Random Network"}]
A random network $G(N,p)$ has N nodes where two nodes are connected with probability $p$.
\end{definition} 

Due to its random nature, this model may have different realizations even with $N$, $p$ fixed parameters. Therefore, the number of links and the nodes degree distribution (see \autoref{sec:GraphTheory}) becomes vital informations to be considered.
In particular, a specific realization could exhibit $L$ links with a probability
\begin{equation}
	\label{eq:probLrndNet}
	p_L = p^L \cdot (1-p)^{ \binom{N}{2} - L } \cdot \binom{\binom{N}{2}}{L}.
\end{equation}
The \autoref{eq:probLrndNet} is the product of $3$ terms: the probability to have $L$ edges; the probability not to have the remaining ones knowing that the total possible node pairs are \[ \binom{N}{2} = \frac{N(N-1)}{2}; \] the total combinations in the choice of the $L$ edges, since what counts is only to pick a number $L$ of edges and not which ones.
As expected, $p_L$ could be interpreted as $L$ successful results knowing that an edge is chosen with probability $p$ and the total trials are $\frac{N(N-1)}{2}$ pairs. Thus, a Binomial distribution with the average number of links equal to
\[ \langle L \rangle = \sum_{L = 0}^{\frac{N(N-1)}{2}} p_L L = p\frac{N(N-1)}{2} \label{eq:meanL} \] and, thus, the average per node contacts \[ \langle k\rangle = \frac{2\langle L \rangle}{N} = p(N-1) \label{eq:meank}. \]

The two equations above highlight that $\langle L \rangle$ and $\langle k\rangle$ could be obtained by multiplying $p$ by the total number of available links and neighbors resp.\footnote{This is accordance with the formal definition of the "average"}. Alongside, as the network becomes denser, i.e. increasing $p$, also $\langle L \rangle$ and $\langle k\rangle$ would be enhanced.
Moreover, by multiplying the expression by $q$, it is possible to obtain $\sigma_{\langle x\rangle} = q \, \cdot \langle x\rangle,$ where $x = L \textnormal{ or } k$.

Similarly, the probability of having k number of contacts is 
\begin{equation}
	\label{eq:probkrndNet}
	p_k = p^k \cdot (1-p)^{ \binom{N-1}{2} - k } \cdot \binom{\binom{N-1}{2}}{k}.
\end{equation}
where $N-1$ are the total neighboring nodes differently from $\binom{N}{2}$, which were the total node pairs (cf. \autoref{eq:probLrndNet}).
Another way to recover $\langle k\rangle$ and $\sigma_{k}$ is by means of $p_k$.

Real network are sparse. In fact, by defining the "sparsity" of a network as the ratio $s : = \langle k\rangle/N$, the internet-routers has $s \simeq 3.0 \cdot 10^{-5}$, since $N \simeq 200000 \text{ while } \langle k\rangle \simeq 6$. Many others concrete example support this sparsity nature of real networks \cite{barabasi::2016networkbook}. Therefore, in the $\langle k\rangle / N \ll 1$ limit\footnote{At least for $\langle k\rangle \simeq 50 \text{ and } N \simeq 10^3$}, the \autoref{eq:probkrndNet} becomes the poissonian distribution with average $\langle k\rangle$
\begin{equation}
	\label{eq:sparse_probk}
	p_k = e^{-\langle k\rangle} \frac{\langle k\rangle^k}{k!}.
\end{equation}

Therefore, keeping in mind that the \autoref{eq:sparse_probk} is only valid in the sparse regime,
\begin{itemize}
	\item the explicit expressions of $\langle k\rangle \simeq p\cdot N \textnormal{ and } \sigma_k = \langle k\rangle^{1/2}$ assume a simpler form for a poissonian distribution. Moreover, as the binomial case, $\langle k\rangle \textnormal{ and } \sigma_k$ increases if p increases, since the network is becoming denser and, so, more spread in the node degrees.
	\item it allows the same description for a pletora of networks with the same $\langle k\rangle$, since the network characteristics does not depend on its size $N$.
\end{itemize}

\subsection{The "Small-World" Property}
\label{sec:SWProp}
A captivating argument, network science allows to study is the "Small-World (SW) phenomenon" or "six degrees of separation". The two quoted names comes, respectively, from a theme developed by the Hungarian writer Fridgyes Karinthy in a play called "Chains" ($1978$) and an empirical experiment injected by the psychologist S. Milgram in $1967$. In fact, the most famous version states that a person, e.g. Kevin Bacon, is linked to a random one, e.g. Andrea Scotti, through only few acquaintances: according to \href{https://oracleofbacon.org/movielinks.php}{Kevin Bacon Number}\footnote{https://oracleofbacon.org/movielinks.php} only by $2$ persons in common. So, the distance between two randomly chosen nodes in a graph is small. Random networks account for this phenomenon as we are going to deepen.

On average, the expected number of nodes $N$ at a certain distance $d$ is
\[N(d) \approx 1+ \langle k \rangle + \cdots + \langle k \rangle^d = \frac{\langle k \rangle^{d+1} -1 }{\langle k \rangle - 1},  \] 
where it has been taken advantage of the geometric series, after noting that $``1"$ is the selected nodes, $\langle k \rangle$ are its neighbors, $\langle k \rangle^2 \approx \langle k \rangle (\langle k \rangle - 1)$ the next neighbors and so on.
Thus, by considering that for a real network $\langle k \rangle \ll 1$ and that $N(d_{max}) \sim N$, 
\[
	d_{max} \sim \frac{\ln(N)}{\ln(\langle k \rangle)}
\]
On the other hand, the above equation is more suitable in describing $\langle k \rangle$, since $d_{max}$ is dominated by few distant paths from the core giant component. Hence, 
\begin{equation}
	\langle d \rangle \sim \frac{\ln(N)}{\ln(\langle k \rangle)} 
	\label{eq:SWrandnet}
\end{equation} 
is the formulation of the Karinthy's "Small-World phenomenon" obtained by considering that most of the nodes are contained within $\langle d \rangle$.
Indeed, for a real network, $\ln(N) \ll N$ drives to a "smaller" average path length compared to $N$, which is the typical scaling for a regular lattice\footnote{The common notion of physical distance is based on a regular lattice which do not shows the "Small-World" effect. At first glance, this result could be perceived as an uncomfortable.}
Moreover, the denominator of the \autoref{eq:SWrandnet} implies that the denser it becomes the network, the closer the nodes aggregate.
As a sociological example, the average distance of a random person, in the current century, would be derived by applying the \autoref{eq:SWrandnet} by fixing $N \sim 7\cdot 10^9$ and $\langle k \rangle \sim 10^3$ yielding $\langle k \rangle \sim 3.28$ which is a more reliable results than the "six degrees of separation" \cite{barabasi::2016networkbook}.

\newpage
\subsection{The Clustering Coefficient}
As far as a social network is considered, it seems natural to introduce a measure of community membership (\autoref{fig:degree_clustcoefficient_pathlength}) which capture the possibility to join a local community, e.g. an association or a family. More generally, real networks are characterized by the clustering formation, which "simplify" the network by aggregating the nodes, called clusters, which present common characteristics. Proceeding as the "coarse graining" technique of Statistical Mechanics, it becomes appealing to take care of the emergent properties exhibited by the "coarse-grained" network. In the following section, it is introduced the degree of "clusterization" of a network.

More precisely, the local clustering coefficient $C_i$ measures the chance of having a link among the neighbors of the $i-th$ node. To gain a rough intuition of its purpose, it worths to consider two opposite limits: 
\begin{itemize}
	\item $C_i = 0$ as there are no links among neighbors of $i$;
	\item $C_i = 1$ as there are fully connected neighbors of $i$.
\end{itemize}

Formally,
\begin{equation}
	C_i := \frac{\textnormal{average links of the i-th neigbors}}{\textnormal{total links among neighbors}} = \frac{\langle L_i \rangle}{\frac{k_i(k_i-1)}{2}} = p = \frac{\langle k \rangle}{N} = \langle C \rangle
	\label{eq:rnd_clustcoefficient}
\end{equation}

where $k_i \textnormal{ and } N$ are the amount of neighboring vertexes of $i$ and the total nodes respectively, \[ \langle L_i \rangle = p \cdot \frac{k_i(k_i-1)}{2} \quad \textnormal{ and } \quad \langle C \rangle := \frac{1}{N} \sum_{i = 1}^{N} C_i. \]

The presence of a link among the two neighbors closes a triangle with the focal node $i$ ( "triadic closure"). Hence, $C_i$ could also be interpreted as the normalized number of triangles centered in the selected node. In addition, $C_i = \langle C \rangle$ is independent by the $i-th$ node degree and, fixing the average degree of a network, it scales with $1/N$: for real networks $p = \langle k \rangle / N$ is small (real networks are sparse) and drives to a smaller $C$ that the one observed. A model that naturally generates "triangle", would be a good candidate to enhance the clustering coefficient even for small $p$.

\subsection{Problems in Random Networks}

By looking at a real social network, e.g. at a cocktail party, there is the co-presence of highly connected nodes, called \textit{"hubs"}, alongside with the less interacting ones. The ER graph does not account for the presence of high-degree nodes\cite{barabasi::2016networkbook}, since they are strongly damped by the  factor term $1/k!$ in \autoref{eq:probkrndNet}. Indeed, real networks are sparse but have also strongly heterogeneous degrees, showing a high variance through node degrees.

Secondly, by looking at the evolution of the contact network, e.g. the one underlying the previous cocktail party, there could be identified 4 topological regimes starting from isolated individuals for $\langle k \rangle = 0$ and ending with a fully connected graph for $\langle k \rangle = N-1$, where $N$ are the total invited persons. The ER model predicts the co-existence of a giant component, containing loops and cycles, and small components, which forms trees, for $1 < \langle k\rangle < \ln(N)$ - find a better description below. However, this is clearly at odds with reality since, as a concrete example, in a random electric power-grid some consumers should not get electric power \cite{barabasi::2016networkbook}. Many network, as the actor network, are in the connected regime; most are in the supercritical one while still connected.
In particular, these regimes scan the formation path of a fully connected network, passing trough a rapid emergence of a giant component.
\newline Schematically,
\begin{itemize}[noitemsep]
	\item Sub-critical Regime for $0 < \langle k \rangle < 1 \quad (p < 1/N)$ 
	\item Critical point for $\langle k \rangle = 1 \quad (p = 1/N)$
	\item Supercritical Regime for $\langle k \rangle > 1 \quad (p > 1/N)$: Single giant component formation
	\item Connected Regime for $\langle k \rangle > \ln(N) \quad (p > \ln(N)/N)$
\end{itemize}
This is in an one-to-one relation with a First-Order-Phase Transition treated in Statistical Mechanics. The formation of an ordered giant component could be compared with ice formation: the passage from disordered water molecules to an ordered structure.

Thirdly, the real world networks exhibits a "Small-World" phenomenon not in agreement with the \autoref{eq:SWrandnet}. Hence, a new "Small-World" measure has to be proposed.

Fourthly, fixing $N$, the theoretical expectation is to grasp the independent nature of $C$ with respect to $k$. Nevertheless, the results is unsatisfactory, since $C$ decreases with the increasing of $k$ for a pletora of the real networks.
%put an image of this
Furthermore, $C/\langle k \rangle (N)$, for high $N$, is higher that its theoretical expectation. Thus, a refined model is needed, to explain the high $C$ even at small $p$ (or high $N$).

\subsection{Watts-Strogatz Model}
\label{sec:WS_Model}
%insert a reference image
\begin{figure}[ht]
    \begin{subfigure}{\textwidth}
        \includegraphics[scale = 0.4]{../images/Networks/WS_Model}
        \centering
        \caption{In the top panel, the changing topology from lattice to random-like network; In the bottom panel, the behavior of average path length and clustering coefficient with $p$ \cite{Olaf:2011_NonRandomBrain}}
        \label{fig:WSmodel}
    \end{subfigure}
\end{figure}

To face the problem of the "high $C$ at low edge probability (for now, redefined as $p_{ER}$)", while preserving the "Small-World" property, the mathematicians Duncan Watts and Steven Strogatz proposed in the $1998$ a hybrid model which interpolated between a regular lattice for $p = 0$ to a random network model for $p = 1$. The new borne model is, thus, called "Watts-Strogatz (WS) model".
Practically, the idea is to start with a grid-like network, where all the nodes have $\langle k \rangle$ neighbors; then, with probability $p$, a link is chosen and one of the endpoints, e.g. the "rightmost", is replaced with a random vertex within the graph. The underlying degree distribution, hence, narrows the nodes degrees since it passes from an one-block histogram centered in $\langle k \rangle$ to a poissonian distribution. Thus, nodes have barely different neighbors degree and no hubs are allowed.
A further constraint is that no multiple edges can connect the same pair of vertexes, i.e. no multi-edges. The presence of multiedges gives a correction of $1/N$ for big $N$, negligible in a real modeling scenario with big $N$.

The rationale of ER model is that by a little decrease of clustering (i.e. destroying triangles) corresponds a significant reduction in the average path length $\langle d \rangle$. Other way around, by connecting nodes in a "long-range" fashion, the overall "triadic closures" are still relevant. 

It worths to highlight that the present $p$, called in this context the "rewiring parameter", is the "long-range" rewiring probability, while the ER model $p_{ER}$ is the probability of an edge formation either among neighbors ("short range" connection) or among distant nodes ("long range" connection). Thus, the "distant" connected vertexes are, on average, $p\cdot L$ where $L$ is the total number of links.
In particular, a regular lattice is characterized by a high clustering (neighboring nodes are fully connected) but a high average path length; while a random network, as seen before, exhibits manifestly the "Small-World" property alongside with a low clustering coefficient especially, since triangles are rare for small $p$.
Therefore, the parameter $p$ is introduce to fine tune the co-presence of the two kind of networks. As reported in \cite{Menczer:2020_1stCoursNetSci}, there is a range of rewiring probabilities $p \in (0.01,0.1)$ in which $\langle d \rangle_p \approx \langle d \rangle_{p=0} \textnormal{ and } C_p \approx C_{p=0}$ encoding the fact that the average path length and clustering coefficient of the WS model are, respectively, near the one of a random network and the one of a lattice.

Anyway, this model doesn't explain the presence of the hubs, since neglected by all the degree distributions, and the (inverse) dependence of $C(k)$ on the average network degree.

As a pedagogical spoiler, the (four) previous problems would be faced by modifying the WS model adding the emergent properties of the real networks, such as the "scale-free" property.
Hence, an useful approach would be to start from the degree distribution rather than on imposing a fixed probability of wiring as for ER and WS models.

\section{Configuration Model}
To face the problems previously exposed, a practical starting point could be to reproduce the degree distribution by taking advantage of the configuration model 
\cite{Menczer:2020_1stCoursNetSci}. Indeed, it is possible to built a network from an arbitrary degree sequence which either could be drawn by a distribution or by a real network of interest.
Precisely,
\begin{definition}[\textit{"Degree Sequence"}]
	The degree sequence is a list of $N$ numbers \newline $(k_0, \cdots, k_{N-1})$ where $k_i$ is the degree of node $i$.
\end{definition}
A degree sequence, thus, determines uniquely a degree distribution but the reverse is not true, since for a particular degree distribution there are $N!$ possibilities of labeling the nodes: $(1,2,3)$ has the same degree distribution of $(3,2,1)$.
Additionally, a method to wire the nodes has to be chosen. Especially, the configuration model wires the nodes randomly; thus, building a random network. For these reasons, the generated graph is also called "random network with a pre-defined degree sequence". 

As done before, a practical way of constructing a network clarifies many ideas.
As a first step, consider to have a set of nodes with an assigned degree sequence. Pictorially, this could be depicted as a node with a number of dangling stubs as its degree. Thus, the number of isolated stubs have to be twice the sum of all the degrees. 
The network is, further, composed by the following iterative steps:
\begin{itemize}
	\item A pair of stubs is selected at random;
	\item An edge is formed, by attaching together the selected stubs. 
\end{itemize}
This tying technique is replicated until all the stubs are saturated and it is random by construct. Therefore, this method generates random networks with a fixed degree distribution.

Since the number of stubs equals the node degree, each node ends up having the desired degree. Thus, from the selected distribution, multiple (random) networks could be created; even, showing some peculiar properties, such as loops or multi-edges. Therefore, further constraints has to be imposed to obtain a specific topology, such as a connection only to "short-range" nodes. 

The network is characterized by mainly two properties:
\begin{itemize}
	\item the probability that a fixed node $i$ may connect to another one $j$ is
		\begin{equation}
			p_{ij} = \frac{k_ik_j}{2L-1}.
		\end{equation}
		In fact, there are $k_i$ stubs (or rays) trying to match the $k_j$ ending of node $j$; while $2L -1$ are the overall rays available for the chosen one which produces the $"-1"$;
	\item the number of self-loops and multi-links decreases as $N$ increases, since it is less probable to connect with a specific node as $p \sim 1/N$. Typically it is not
	needed to exclude them \cite{Newman:2010_Net:AnIntro}. More importantly, arbitrarily rejecting self-loops or multi-links would result in a modification of the starting degree distribution, yielding the analytical calculations difficult. 
\end{itemize}

The different realizations of the same degree distribution have a statistically objective. Indeed, let's suppose a certain feature, such as the presence of hubs, is independent on the other network characteristics rather than the degree distribution. By exploiting an ensemble of graphs, it would be possible to obtain an average and a standard deviation of that feature. Thus, by comparison with the real value, evaluate the unique dependence on the degree distribution.

Another method to generate a network from a fixed degree distribution is the "hidden parameter ($\eta$) method" which, in addition, do not form multi-edges and loops \cite{barabasi::2016networkbook} and allows to tune the average degree $\langle k \rangle$ modifying $\eta$.
On the other hand, to recover a randomized version from the degree sequence, such as of a fixed real network, the "degree-preserving randomization" is the best help the theory provides. In this way, by just swapping two endpoint nodes at a time, it is possible to build an networks ensemble with the same degree distribution. So, check whether a quantity depends only on the degree distribution itself or has a more involved structure.
In the following sections, only the Configuration Model is going to be used.

\clearpage
\subsection{Random Configuration Model: Poissonian Small-World Network}
\label{sec:PSW_network}
Social network are highly non trivial structure, by including a multilevel organization; weak ties between communities; and temporal aspects that suggest a degree of fluidity with stable social cores \cite{Thurner::NetBasedExpl}.
As seen in the \autoref{sec:ATechIntro}, however, the lockdown measures prevents the formation of highly connected nodes (hubs), expected in an unconstrained (social) scenario. Thus, a possible approach would be to use a Watts-Strogatz network, which has a degree sequence drawn from a poissonian distribution, regardless on the magnitude of $\langle k \rangle / N$ ratio (differently from \autoref{sec:WS_Model}); while edges only connecting nearest neighboring nodes.
This involved structure, called "Poissonian Small-World (PSW) Network" as in \cite{Thurner::NetBasedExpl}, enables to capture the heterogeneity in the number of contacts (node degree); the SW phenomenon and the clusterization of families alongside with their overlap. Moreover, the WS parameter $p$ represents the fraction of individuals that are connected at "long-range" through leisure activities ($p = 0$ these are prohibited).

In order to built a PSW network, the starting point is by using the configuration model with a degree sequence drawn from a poissonian distribution. To simulate a closed population, the nodes are assumed to be put on a circle: the last and the first of nodes of the degree sequence are assumed as neighbors. Hence, both the "SW" property (see "The emergence of the Small-World property" \autopageref{sec:SWProp}) and the heterogeneity on the node degrees could be embodied at the same time. 

With low values of the poissonian parameter $\mu$ is possible to take into consideration the lock-down scenario where the average per node degree of contacts is the household size $D \sim 2.5$ \cite{Thurner::NetBasedExpl}. The configuration model generates a random wiring of the links, producing "long-range" connections which spoil the locality of the community structures (families). Therefore, by an ad-hoc algorithm, the stubs are forced to attach to the nearest nodes in the circle according to their degree. In practice, the wiring algorithm iterates over few steps\footnote{Recall that the degree sequence is already fixed and drawn by a poissonian distribution}:
\begin{itemize}
	\item choose the lowest degree $k_l$ node that has not already been saturated;
	\item link an even number of times $\lfloor k_l /2 \rfloor$ on the clockwise and anti-clockwise nodes, where $\lfloor * \rfloor$ is the floor function of $*$;
	\item if the degree of the selected node is odd, wire one last time ($k_l - \lfloor k_l /2 \rfloor \, = k_l \bmod(2) =  1$), e.g. anti-clockwise;
	\item delete, among the available nodes, both the chosen node and the target nodes which have saturated their degree.
\end{itemize}

More practically, fixed $[(1, 0), (6, 1), (0, 2), (2, 2), (4, 2), (7, 2), (8, 2), (9, 3), (5, 3), (3, 4)]$, i.e. a degree sequence composed by (node,degree) tuples. 
The edges $(i,j)$ are created by connecting $i \textnormal{ to }j$ and diminishing the degree of $j$ at each step:
\begin{itemize}
	\item node $1$ is left alone as $deg(1)=0$;
	\item $(6,7)$;
	\item $(0,2)\textnormal{ and }(0,9)$
	\item $(2,0) \text{ (already present) and } (2,3)$;
	\item $(4,3)\textnormal{ and }(4,5)$;
	\item $(7,6)\textnormal{ (already present) and }(7,8)$; 
	\item $(8,7) \textnormal{ (already present) and } (8,9)$;
	\item $(9,3), (9,8) \textnormal{ (already present) and } (9,5)$ - "tunneling" nodes ;
	\item $(5,4), (5,9) \textnormal{ (already present) and } (5,3)$;
	\item node $3$ is left with $deg(3) = 1$, but this issue reduces with sparsity $\langle k \rangle \ll 1$.
\end{itemize}

Hence, the algorithm preserves the poissonian degree distribution, while enhancing the local clustering of the nodes. However, it is possible to have "long-range" connections ($(9,5)$), since the nearest available vertexes could be already saturated; therefore, resulting in a distant linking. This "tunneling" nodes are neither "super-spreaders" (or hubs of the infection), as they are forbidden by the poissonian degree distribution; nor "super-infectors", which would be characterized by an higher infectivity rate $\beta$.

\newpage
\section{Scale-Free Networks}
\subsection{Scale-Free Property}

\begin{figure}[h]
    \begin{subfigure}{\textwidth}
        \includegraphics[scale = 0.5]{../images/Networks/Scale-free-network-and-power-law-distribution-A-B-The-US-highway-system-has-a.png}
        \centering
        \caption{On the left: the comparison of a random network(top panel) and a scale-free one(bottom panel; On the right: their degree distributions on log-log scale) \cite{barabasi::2016networkbook}}
        \label{fig:PLDsVSEBDs}
    \end{subfigure}
\end{figure}

\begin{definition}[Scale-Free Network \cite{Barabasi:1999_ScalRndNet}]
	A scale-free network is a network whose degree distribution follows a power law.
\end{definition}

By plotting the degree distribution of the real networks, due to "hubs", a better approximation is obtained via a power-law distributions (PLDs) of the kind\footnote{Usually, reported in the log-log scale linear version: $\ln(p_k) \sim -\gamma \ln(k)$.} $p_k \sim k^{-\gamma}$  where $\gamma$ is called the "degree exponent". 

{\large \textbf{How a scale does (not) emerge}} \\
The PLDs are long-tailed functions; hence, describing a phenomenon that has not a reference scale.
For example, the heights distribution of a sample population displays a peak at the average height, which is the reference scale for that population. Instead, PLDs do not exhibit a peak but allows also for outliers, e.g. 100ft tall individuals.
More precisely, the variance, for a poissonian distribution, is $\sigma_k = \langle k \rangle ^ {1/2}$, limited as the network grows. So, the $\langle k \rangle$ scale is trustworthy. 

At the same time, for a PLDs the variance diverges since \cite{barabasi::2016networkbook}	
\begin{equation}
	\lim_{N \to \infty} \sigma_k^2 \approx k_{max}^{2-\gamma+1} = \infty.
	\label{eq:kn_SFnets}
\end{equation}
since real networks have typically $\gamma \in (2,3)$ (WWW has $\gamma =  2.1$ \cite{barabasi::2016networkbook}) and $k_{max} \sim N$. 

So, a reference scale for PLDs is not naturally present as, extending the poissonian result, for the "Exponential Bounded Distributions" (EBDs) \cite{barabasi::2016networkbook}, i.e. when there is a exponential or faster decay for high $k$, such as binomial/poissonian/Gaussian distributions.

{\large \textbf{The Degree(s) of Hubs}} \\
In the following, it is developed a quantitative comparison between the maximum degree allowed by EBDs and by PLDs to gain a numerical perception of their difference.
In detail, $k_{max}$ (or the "natural cutoff") is expected to be, roughly speaking, the size of the bigger hub; while $k_{min}$ approximates the smallest degree.
So, for an exponential distribution \cite{barabasi::2016networkbook}, 
\begin{equation}
	k_{max} = k_{min} + \frac{\ln(N)}{\lambda}.
	\label{eq:Expkmax_up}	
\end{equation}
where $\lambda = 1$ for convenience. The same $\ln(N)$ dependence would be recovered of any other EBD.

On the other hand, applying the same rationale to a scale-free network, 
\begin{equation}
	k_{max} = k_{min}\,N^{\frac{1}{\gamma-1}}.
	\label{eq:SFkmax_up}
\end{equation}

The results are somehow expected, since for:
\begin{itemize}
	\item EBDs: $k_{max} \sim k_{min}$ since $\ln(N)$ strongly reduces contribution with $N$;
	\item PLs: there could be order of magnitude of difference between $k_{max}$ and $k_{min}$.
\end{itemize} 
Indeed, as a simple example, WWW forms a graph of on $N \sim 10^5$ documents with $\langle k \rangle \simeq 4.6$ and $\gamma = 2.1$ \cite{barabasi::2016networkbook}. 
Knowing that $k_{min} = 1$ for EBDs, $k_{max} \sim 14$; while $k_{max} = 95.000$. 
This reinforces the insight that big hubs are naturally arising by increasing $N$ only for PLDs.
\label{sec:SFProperties_up}

{\large \textbf{Ultra Small-World}} \\
Hubs represent a one-hop bridge for many nodes, by centralizing the otherwise-distant nodes. Indeed, the average distance $\langle d \rangle \sim \ln(\ln(N))$ for $\gamma \in (2,3)$, formally describing the so called "ultra small-world phenomenon". An heuristic example is the airport lines, which diminish the \(\langle d \rangle\) among (far) cities with respect the national highways.

Surrounding the "Ultra Small-World", there are $3$ other regimes
\cite{Cohen:2003_SFUSW}:
\begin{enumerate}
	\item \underline{\textit{Anomalous Regime ($\gamma = 2$)}} producing a "wheel rim configuration", 
	a central hub with spoke nodes. In this regime, $\langle d \rangle = constant$;
	\item \underline{\textit{Ultra-Small World Regime ($2 < \gamma < 3$)}}: $\langle d \rangle \sim \ln(\ln(N))$ gives rise to the "ultra-small world phenomenon" as a slower growth in $\langle d \rangle$ than random network;
	\item \underline{\textit{Critical Point ($\gamma = 3$)}} for which $\langle d \rangle \sim \frac{\ln(N)}{\ln(\ln(N))}$, marking the passage between the small-world and the ultra-small world;
	\item \underline{\textit{Small World ($\gamma > 3$)}} random networks are recovered, resulting in $\langle d \rangle \sim \ln(N)$.
\end{enumerate}
Thus, \(\langle d \rangle\) is changing with $\gamma$, as the smaller it is, the shorter would be the average distance; but also with $N$, as it is the "ab ovo" hypothesis for having big hubs.

{\large \textbf{Growth and Preferential Attachment}} 

Even with broad different origins, most real networks display two important features without which it is impossible for hubs to emerge \cite{barabasi::2016networkbook}: growth and preferential attachment (see \autoref{App:RecPLD},\autoref{App:OriginOfPA}). 

In particular, growth is an active process driven by the sequential increasing of the size of the network $N$ as per the addition of new nodes. Rather, a random network assume $N$ to be fixed. \newline
Furthermore, most real networks new nodes prefer to link to the more connected ones, a process called "preferential attachment". On the other hand, random networks wire randomly with other nodes.

Ça va sans dire \label{cit:A.Marzo}, the derided model, developed in \autoref{sec:BA_model}, has to be dynamical and encode a "preferential" probability which changes every time ("event time") there is a modification of the topology. 

{\large \textbf{Final Remarks on Scale-Free}} \\
It worths to recall that, in general, the hosted phenomenon (-a) is complex and affects the graph topology and, in turn, the degree distribution. Rebus sic stantibus \label{cit:D.Massa}, it is sufficient to establish a fair starting point, or EBDs or PLDs class, over which developing gradually ad-hoc features, rather then the meticulous fitting.

As a final remark, many interesting features of scale-free networks, from their robustness to anomalous spreading phenomena, are linked to the $\gamma \in (2,3)$; thus, due to the small average distance which strengthen the considered phenomenon.
For a more detailed dissertation, see \autoref{sec:SFD_details}.

\newpage
\subsection{Barabási-Albert Model}
\begin{figure}[ht]
	\includegraphics[scale = 0.4]{../images/Networks/Barabasi_Albert_Model}
	\centering
	\caption{Formation of a scale-free network \cite{Barabasi:2009_SF_DecadeBeyond}}
	\label{fig:LCD_growth}
\end{figure}

\label{sec:BA_model}
To take advantage of the coexistence of both growth and preferential attachment in the real networks, it has been proposed a minimal model, the Barabási-Albert model \cite{barabasi::2016networkbook}:
\begin{itemize}
	\item start with $m_0$ nodes randomly connected and a degree at least equal to $1$;
	\item \textbf{Growth:} at each event time add a new node with $m(\leq  m_0)$ stubs which are linked to different nodes;
	\item \textbf{Preferential Attachment:} the probability to wire with a node $i$ depends on the degree centrality, i.e. importance based on the degree:
	\begin{equation}
		\Pi(k_i) = \frac{k_i}{\sum_{j = 1}^N k_j}
	\end{equation}
\end{itemize}

So, preferential attachment has a stochastic nature not a deterministic one. Nevertheless, it increases the appeal of the (already) high-degree nodes, alias \label{cit:S.Sagone} "rich-gets-richer" phenomenon. As a result, while most of the nodes end up having few links, a bunch of nodes become highly connected; thus, performing the role of big hubs. In turn, this translates into a degree distribution which does not decay exponentially, i.e. a power law with $\gamma = 3$ \cite{barabasi::2016networkbook}. 

Empirical studies on networks, such as actor and scientific collaboration networks, have found that the exponent at the numeratore of $\Pi(k_i) \sim k_i^\alpha$ could differ from $1$.
Indeed, there are $3$ paradigmatic kinds of Preferential Attachments (PAs) by varying the exponent $\alpha$: sublinear PAs ($0<\alpha<1$), (linear\footnote{Commonly, the here-called "linear" PAs is just called "preferential attachment" without further specification.}) PAs ($\alpha = 1$), superlinear PAs ($\alpha>1$). Briefly, the sublinear PAs drives to an "stretched exponential distribution" (see \autoref{sec:SFProperties}); the linear to the standard scale-free network; while in the superlinear almost all nodes connects to few super-hubs (winner-takes-all phenomenon).
Hence, only for $\alpha = 1$, a power-law distribution could be recovered, recalling that knowing the high-degree behavior is just a starting point to model the underlying phenomenon. 
In addition, after $t$ event times, the network has gained $t$ new nodes for a total of $N = m_0 + t$ nodes and $m_0 + mt$ links.

\newpage
\subsection{Linearized Chord Diagram}
Having in mind the general purpose and results, a more precise description on the building process would clarify two point left open, i.e. 
\begin{itemize}
	\item the arrangement of the initial $m_0$ nodes;
	\item whether the next $m$ links were added simultaneously or one at the time. This could drive to the formation of multi-links, i.e. conflicting edges, due to assumed independent nature of the new stubs. 
\end{itemize}

These problem are solved with the idea of sequentially adding nodes to the graph(s) created at a previous times.

Fixing $m=1$ for simplicity, start with the empty graph $G_{m}^t = G_1^0$.
At each event time, choose an initial vertex $v_i$ of \\ $G_1^{(t-1)}$ and connect it to the new vertex $v_t$ with probability $p$ such that
\begin{equation}
	p =
	\begin{cases}
		\frac{k_i}{2t-1} & if \quad i \in [1,t-1] \\
		\frac{1}{2t-1} & if \quad i = t
	\end{cases}
	.
\end{equation}

Rephrasing it, connect $v_t$ with $v_i$ with a probability of $\frac{k_i}{2t-1}$ if the nodes are different; otherwise, $\frac{1}{2t-1}$. This method does not forbid the formation of multi-edges and loops, but constraints their number to be negligible as the network grows.
The present technique referred to $m=1$ for an immediate perception. Whether $m > 1$ stubs are involved, add them one at the time, enabling their contribution to the hubs degree.

\chapter{The Epidemiological Models}
\label{ch:sir-models}	
The most studied dynamical process on networks is the spreading of an agent. Rather than its common interpretation as a pathogen, this agent could also be considered as an information ("knowledge spreading"), an ask for collaboration, a mobile/computer virus, a business practice, a rumor or a meme, etc. These phenomena acts on different types of networks; are characterized by peculiar time-scales and follow various microscopic-transmission mechanisms. Despite all of these differences, they obey to a common pattern conveniently captured by analyzing their spreading on a network: the quoted graphs (cf. \autoref{ch:network-models}) provides different insights on how links could be created and preserved. Thus, although it has been starting from the biological sector \cite{VespignaniSatorras2001Epidemic}, it is used also in many other human-related fields, as the social sciences and digital security.

This thesis is devoted to the infective epidemiology: a paradigmatic model (SIR model) is let spread on different topologies simulating the COVID-19. In particular, infectious diseases are also called contagious, due to their transmission by a contacts with a secretion of an ill person, e.g. droplets. They account for $43\%$ of the global burden of diseases alongside with other noninfectious diseases (for example obesity, gambling or smoking) which are equally affected by the graph of contacts \cite{barabasi::2016networkbook}.

The epidemiological frameworks, in general, find their roots in two fundamental hypotheses:
\begin{itemize}
	\item Compartmentalization: Each individuals is classified according to a state, or compartment, allowed by the considered model. In particular, the simplest compartments are the following:
	\begin{itemize}
		\item Susceptible (S): vulnerable people which have not been in contact with the disease;
		\item Infectious (I): individuals, which are able the spread the pathogen to their neighbors;
		\item Recovered (R): infected individuals which has are removed from the I-state. No difference among deaths or recovered has been settled\footnote{In reality, this is not the case since death individuals are no more present in the society and leaves space for new contacts; while immunize nodes shields the more inner nodes, by arranging in a sun-like configuration (a susceptible core and spoken "recovered" nodes)}. Thus, many texts use the fair word "Removed" as a definition of the "R".
	\end{itemize}
	
	Driven by some transition probabilities, individuals are able to move among compartments: $S \rightarrow I$ is a random event regulated by the transmission rate ($\beta$); while $I \rightarrow R$ by the recovery rate ($\mu$). So, as for COVID-19, in the early stages a large fraction of the population was susceptible, e.g. $99\%$, called "naive population". Then, the pathogen drives some nodes into the infectious state; while, at the end, to the "Recovered" compartment.

	For convenience, it is also possible to introduce many other compartments to allow for additional states, such as the "asymptomatic" which are able to spread the disease without exhibit symptoms, the first signal of the infection.
	The epidemiological frameworks are generally called as their constitutive compartments characterizing the model, e.g. "SIR" stands for the node path among the susceptible-infectious-recovered states.
	The division in compartments has to be considered at a macroscopic scale, since the aim is to improve the prediction of the number of infected; while not caring about the microscopic infection within the host body; all these details, indeed, are encoded in $\beta \text{ and } \mu$;
	\item Mixing Level: People live and mixes but has a fixed number of total individuals $N$, i.e. no death or birth are taken into account (see \cite{Newman:2010_Net:AnIntro}). Hence, two possible hypothesis on how the individuals, restricted to their compartments, could get exposed to the disease are the:
	\begin{itemize}
		\item \textit{homogeneous mixing}: this hypothesis assumes that each individual has the same chance of interact with another one, e.g. an infectious. For this reason, it is also called the "fully-mixed", "mass-action" or "mean-field" approximation. Indeed, the specific degree of a single node is not taken into account; rather, the number of infectious-contacts per node is fixed. In turns, this could be seen as a "field" of mean infectious spikes which attack the susceptible.
		Only the "global" scale is present;
		\item \textit{heterogeneous (degree-based) mixing}: this approach hypothesizes that the individuals are grouped by their degree of contacts $k$ and considered "statistically" equivalent within that class: this is a finer mean-field approximation and could be improved by considering an "individual-based" approximation. 
	\end{itemize}
\end{itemize}
It seems pretty natural that the "fully-mixed" hypothesis is false at some extent.
In particular, it would be possible to observe that for small "epidemic strength" ($R_0$) the network structure deeply plays a role (see \autoref{ch:Results}), since individuals may transmit a pathogen only to (local) neighbors. Nevertheless, if a pathogen is "strong" enough the total number of infected are comparable to the one of a "mean-field" approach.

The model which may be composed with the quoted compartments are the $SI$, the $SIS$ and the $SIR$ (the case of study). In detail, the $SI$ model accounts for the fact that it would be impossible to recover from the disease, e.g. herpes; the $SIS$ model for diseases where it would be not possible to waning immunity, thus, returning susceptible, e.g. the HPV, generic flu; the $SIR$ where the last state is the gained immunity, e.g. the most strain of seasonal influenza or chickenpox.

Before gaining the details on the two mixing hypothesis, it is worth define the variables involved in the two approach.
In particular, by denoting the total number of individuals $N$, $s(t) := S(t)/N$ is the number of susceptible (healthy) normalized with $N$; $i(t):=I(t)/N$ is the fraction of already infectious persons; while $r(t) := R(t)/N$ the recovered one. In this way, the "closed population" constraint translates into $s(t)+i(t)+r(t)=1$ at any time $t$. 
Thus, at the initial time ($t = 0$), a seed of infected has to be injected to start the infection, e.g. $I(0) := 10$. So, $s(0) = 1 - i(0)$ since the recovered are $r(0) = 0$.
Furthermore, let assume that the likelihood of transmitting the disease to a susceptible per unit time is $\beta$, i.e. the "transmission rate"; while the "recovery rate" is $\mu$. Hence, it worths define the \textit{basic reproduction number}  $R_0 := \beta \langle k \rangle / \mu$, embodying the number of secondary infected in a naive population.

The final goal would be to find the evolving number of new daily infected at each time (a slight modification of $I(t)$). Moreover, since the disease-spreading is a stochastic process, it would be the average of the various scenarios--in our case 50.

\newpage
\section{Homogeneous Mixing}

\begin{figure}[ht]
	\includegraphics[scale = 0.38]{../images/Ch_TheEpidModels_Methodology/complete_graph_sir}
	\caption{Mean-Field SIR using a Complete Graph with $N = 10^{3}, \langle k \rangle = 999, \beta = 0.001,  \mu = 0.2$}
	\label{fig:MF_SIR}
\end{figure}

Within this class of hypotheses, people mingle and meet completely at random: an infected person can be in contact with {\large $\frac{\langle k \rangle \, S(t)}{N}$} susceptible individuals. Taking into account that at a time $t$ there are $I(t)$ infected which spread the disease at a rate $\beta$\footnote{In many text books, e.g. \cite{Newman:2010_Net:AnIntro}, the $\beta \langle k \rangle$ total rate considered in the following is just $\beta$.} and recovers at a rate $\mu$,
the SIR equations are the following \cite{Newman:2010_Net:AnIntro}

\begin{equation}
	\begin{cases}
		\frac{ds}{dt} = -\beta \langle k \rangle s i \\ \\ 
		\frac{di}{dt} = \beta \langle k \rangle s i - \mu i = \beta \langle k \rangle \, (1-i-r) \, i -\mu i = \mu( R_0 s(t) - 1) i(t)  \\ \\
		\frac{dr}{dt} = +\mu i
	\end{cases}
	\label{eq:SIR_MF}
\end{equation} 
where it has been exploited the fact that the population is closed, i.e. no death/births are considered. This translates into $s+i+r=1 \quad  \forall t$ or even $ ds/dt + di/dt + dr/dt = 0 \quad \forall t$. In addition, the right most equality shows how the definition of $R_0$ could be recovered directly from the equations. Finally, note the removal of the time dependence since $s(t), i(t), r(t)$ is understood.

To solve \autoref{eq:SIR_MF}, by integrating "division" of the third equation and the first equation
\begin{equation}
	s(r) = s_0 e^{-\beta \langle k \rangle r / \mu}\
\end{equation}
where it has been required that $s(t=0)=s_0$ arbitrarily. 
Then, substituting $s(r)$ into the second equation \cite{Newman:2010_Net:AnIntro}, 
\begin{equation}
	\frac{dr}{dt} = \mu \, (1-r-s_0e^{-\beta \langle k \rangle  r/\mu}).
	\label{eq:r_SIR_MF}
\end{equation}

Now, solving this equation for $r$ gives, in turn, the possibility to find $s$ and, thus, $i$.
In principle,
\[
	t = \frac{1}{\mu} \int_0^r \frac{du}{1-u-s_0e^{-\beta \langle k \rangle  u/\mu}}.	
\]
The initial conditions can be chosen arbitrarily but the most suited is to fix a small number $c << N$ of infected individuals such that $s_0=1-c/N\approx1$, $i_0:=i(0)=c/N\approx0$ and $r_0\approx0$.

Unfortunately, this equations have no closed form, but special insights cloud be reached by looking at the plot of the numerical solutions of \autoref{eq:SIR_MF}.


The case where $R_0 < 1$ represents the scenario when the pathogen will die out fast from the population, i.e. a limited spreading occurs. 
\\Indeed, at early times $t\approx 0 \implies i(0)\sim 0, r(0)\sim 0$. 
\\So, \cite{barabasi::2016networkbook} \[i(t\approx 0) \sim i_0 \, e^{(\mu(R_0 -1)t)}.\]
On the other hand, the SIR features are more involved if $R_0 > 1$:
\begin{enumerate}
	\item $s(t)$ is monotonically decreasing; while $r(t)$ is always increasing; 
	\item $i(t)$ increases depending on the availability of susceptible: at early times the increasing is exponential; then, deflect and decreases on the way to the end of the epidemic, i.e. $i(t_{max})=0$;
	\item $s(t_{max}) > 0$ since when $i \to 0$ there are no infected individuals left to infect the remaining susceptible. Formally, $r(\infty):=\lim_{t \to \infty} r(t)$ is the total size of the outbreak, i.e. the total number of individuals who ever catch the disease. 
	
	Setting $dr/dt \stackrel{!}{=}0$, $s_0\simeq1$ ("naive population") and using \autoref{eq:r_SIR_MF}, the solution of 
	\begin{equation}
		r = 1 - e^{-\beta \langle k \rangle  r /\mu}
		\label{eq:r_SIR_MF_GiantComp}
	\end{equation}
	is precisely $r(\infty)$. Therefore, whenever $r(\infty)\neq1$ is a sign that "herd-immunity" is reached, alongside with $1-r(\infty)$ susceptible left;
	\item the characteristic time \[ \tau  = \frac{1}{\mu(R_0-1)}\] is the time needed to reach about the $36\%$ of the population and it is the inverse of "disease velocity", which further depends both on the possibility to infect and the average nearest neighbors. With this new measure of epidemics, it would be possible to recover the previous result observing that there is a spread iff $\tau > 0$, i.e. $R_0 > 1$.
\end{enumerate}

Interestingly, it would be possible to recognize $S \leftrightarrow r$ and $D:= \langle k \rangle \leftrightarrow \beta \langle k \rangle/\mu$ (\autoref{eq:r_SIR_MF_GiantComp}) where the "left" quantities are describing the percolation phenomenon and the "right" ones the SIR spreading describes. In this way, the equation \autoref{eq:r_SIR_MF_GiantComp} regulates the size $S$ of the giant component of a Poisson random graph. In particular, the SIR infection of neighboring nodes could be seen as a bond percolation process of an agent, e.g. water, which propagates according to the same probability distribution \cite{Newman:2010_Net:AnIntro}, \cite{barabasi::2016networkbook}. 
This analogy allows to double the interpretations of the results of \autoref{eq:r_SIR_MF_GiantComp}.
Indeed, in percolation theory, the giant component size $S$ depends on the $D$ average degree of links per node when $N \to \infty$ . 
\\So, the final outbreak size $r(\infty)$ depends on $\beta \langle k \rangle / \mu$ and the \textit{basic reproduction number} is, conveniently, defined as $R_0 := \beta \langle k \rangle / \mu$. 
\\Moreover, $r(\infty)$ goes continuously to $I(0)/N \sim 0$ as $1^{+} \leftarrow R_0$; while it is constantly $I(0)/N$ for $R_0 \to 1^{-}$. Analogously of the appearance of the giant component, the point where (first-order) phase transition occurs, i.e. $R_{0c} = 1$, is called epidemic transition. Thus, $R_{0c}$ separates the "epidemic" ($R_0 > R_{0c}$) from the "non-epidemic" ($R_0 < R_{0c}$) regime.
The $R_0$ dependence of $r(\infty)$ has to be somehow expected since if an infection has less transmissibility than recovery ($R_0 <1$), the pathogen would die out fast. On the way to $R_{0c}$, an increasing of the $\beta / \mu$ ratio would describe a less strong epidemic, which consecutively provide a minor $r(\infty)$.

To be precise, $R_0$ is the number of second infections in a naive population, i.e. at early times $t \approx 0$: $\beta/\mu = \beta \tau$ is the probability to infect over a time-interval of $\tau$. Thus, $R_0$ is an estimate of the slope of the curve describing the infection at the start of the epidemic. Furthermore, it is a rough marker of an exponential growth\footnote{For SI model, $\lim_{\mu \to 0} R(t) = \infty$ since every infected could arbitrarily infect all the other nodes. In a real scenario, $R_0 \, (\leq N')$ is finite, as the size $N'$ of a population is finite.}. (for $R_0 > 1$) or vanishing of the pathogen (for $R_0 < 1$) since $I(t) \approx R_0^{t/d}$. On the other hand, $R_0 =1$ is not indicative of whether this slope is going to increase until $r=1$ (pandemic scenario), decreases fast guiding into $r(\infty)<<1$, or both resulting in $r(\infty)<1$. Indeed, for a more precise signal on the spreading behavior depending only by the initial conditions, higher-order features of the underlying network should be taken into account. A new reproduction number is going to be proposed in the next section.

\newpage
\section{Heterogeneous Mixing}
\begin{figure}[ht]
	\includegraphics[scale = 0.38]{../images/Ch_TheEpidModels_Methodology/er_graph_sir}
	\caption{Degree-Based SIR using Erdös-Rényi Graph with $N = 10^{3}, p = 0.5, \langle k \rangle = 500, \beta = 0.001,  \mu = 0.2$}
	\label{fig:DB_SIR}
\end{figure}
As quoted before, within the well-mixed population it has been considered that every node could infect, at any time, a fixed amount of susceptible neighbors $\langle k \rangle$. This is not the case, e.g., for a pletora of real networks (Barabási-Albert or Linearized Chord Diagram models) as they are "complex"/"scale-free" in the sense that, for $\gamma \leq 3, \, \langle k^2 \rangle$ typically diverges. Thus, fixing the number of contacts to $\langle k \rangle$, it drives to underestimate the spreading, which could have worser effect than overestimating it. 

An improvement would be to consider that if a pathogen spreads on a network, individuals with more links are more likely to be infected. Therefore, the nodes are further classified according to their degree and assumed statistically equivalent within their class. This method may, thus, called degree-based mean-field approach. Indeed, there would be $3 \cdot k_{max}$ classes, i.e. $3$ compartments for each degree, but there are no individual differences among nodes with the same degree $k$. 

The total fraction of infected $i(t) = \sum_{k=0}^{k_{max}} p_k i_k(t)$. Alongside with the other decomposition of susceptible and recovered, substituting them in the \autoref{eq:SIR_MF} it would be possible to recover for each $k = 0,\cdots,k_{max}$
\begin{equation}
	\begin{cases}
		\frac{ds_k}{dt} = -\beta  k  s_k \Theta_k \\ \\ 
		\frac{di_k}{dt} = \beta  k  s_k \Theta_k - \mu i_k = \beta  k  \, (1-i_k-r_k) \, \Theta_k -\mu i_k \\ \\
		\frac{dr_k}{dt} = +\mu i_k
	\end{cases}
	\label{eqs:SIR_degree-based}
\end{equation}
where $\Theta_k(t) = i_0 \frac{\langle k \rangle - 1}{\langle k \rangle} e^{t/\tau}$ is the density function, representing the fraction of infected nodes surrounding a susceptible individual with degree $k$. It would be recovered, and commented, shortly but note that, a priori, it can depend both on $k$ and $t$.

The above equations depend, by construction, on $k$, the $\langle k \rangle$ dependence is no more present; while network structure is encoded into the $\Theta_k(t)$ factor, which in the homogeneous case was $i$.
Before tackling the problem of solving them, it is necessary to explicitly recover the $\Theta_k(t)$ factor.

\subsection*{The Density Function}
The density function, as reported before, provides the fraction of infected nodes neighboring a susceptible degree-k node. In fact, as we are going to prove, this estimation account for all the infectors connected with an arbitrary number of hops, since it has no upper-limit. 

Formally, it has been assumed that the network lacks the degree correlation: nodes are connected at random, thus an hub could equally connect with another hub or a lower-degree node; so, called, a "neutral network". Hence, the probability of a vertex to point to a degree $k'$ node is the excess degree
\begin{equation}
	q_{k'} = \frac{k' p_{k'}}{\langle k \rangle}
	\label{eq:q_excess_degree}
	.
\end{equation}
Indeed, the probability of choosing a degree-$k'$ node is $p_{k'}$ but, since it has been $k'$ possibilities with one stub to connect with it, this has been enhanced by a factor of $k'$. As $q_{k'}$ shoudl be a probability, the denominator comes to match the $\sum_k q_k \stackrel{!}{=} 1$ constraint.
The \autoref{eq:q_excess_degree} is local, random and linear in the "bias term" $k$. So, as reported in the \autoref{sec:linear_pa_link_selection}, it is the starting point to build the (linear) preferential attachment. Moreover, \autoref{eq:q_excess_degree} grows with $k'$ or $p_{k'}$, i.e. the "presence" of $k'$ degree nodes.

For a disease to spread, it is required at least one of the spoke of the infected node is connected to the one target node: only $k'-1$ links would be available.
Thus,
\begin{equation}
	\Theta_k(t) = \frac{\sum_{k'} (k'-1)p_{k'}i_{k'}(t)}{\langle k \rangle} = \Theta(t)
	\label{eq:Theta_generic}
\end{equation}
where the rightmost equation explicitly display the independence on $k$.

In order to capture its time-dependence, it would be take advantage of the specific model of spreading, e.g. SIR, following these steps: differenciate the \autoref{eq:Theta_generic}, insert \autoref{eqs:SIR_degree-based} and consider $1-i_k-r_k \approx 1$, which it is the case at early times. In this way, it could be obtained that \cite{barabasi::2016networkbook}

\begin{equation}
	\begin{cases}
		\frac{d\Theta}{dt} = \left( \beta\frac{\langle k^2 \rangle - \langle k \rangle}{\langle k \rangle} - \mu \right) \Theta \\ \\
		\Theta(0) \stackrel{!}{=} i_0 \frac{\langle k \rangle -1}{\langle k \rangle } \qquad \text{I.C.}
	\end{cases}
\end{equation}
which solving by $\Theta$ gives 
\begin{equation}
	\Theta(t) = C e^{t/\tau}
	\label{eq:Theta_SIR_network}
\end{equation} 
where
\begin{equation}
	C := i_0\frac{\langle k \rangle -1}{\langle k \rangle}  \qquad \text{and} \qquad
	\frac{1}{\tau} := \beta \left(\frac{\langle k^2 \rangle}{\langle k \rangle}-1\right)
	-\mu.
	\label{eq:tau_SIR_networks}
\end{equation}

\begin{figure}[t]
	\includegraphics[scale = 0.38]{../images/Ch_TheEpidModels_Methodology/ER_Epid_Threshold.jpg}
	\centering
	\caption{Epidemic Threshold in a ER Graph with $N = 10^{4}, \beta \in [0,0.4], \langle k \rangle \sim 10, \mu = 0.8$. Note that the departure from zero happens near 0.1, due to finite size effects. Thus, $R_0 < 0.1 \cdot 10 = 1$ drives to a fast-decay of the infection}
	\label{fig:ER_Epidem_Thr}
\end{figure}

A global outbreak is possible if, and only if, $\tau > 0$, which implies (cf. \autoref{eq:Theta_SIR_network}), that the number of infected nodes grows exponentially with time. This allows to recover the condition for a global outbreak
\begin{equation}
	\lambda > 
	\lambda_c:=\left[ \frac{\langle k^2 \rangle}{\langle k \rangle} - 1\right]^{-1}
	\Rightarrow
	R_0 > R_{c-net}:=\frac{ \langle k \rangle^2 }{\langle k^2 \rangle-\langle k \rangle} \geq 0
	\label{eq:lambdac_Rc_netSIR}
\end{equation}
where it is specified the interdependence of the degree-heterogeneity ($\langle k^2 \rangle$), $\mu$ and $\langle k \rangle$ for a decay in the presence of the pathogen. For these reasons, $\lambda$ is called the (biological) "spreading rate", since include all the biological factor of the disease, and $\lambda_c$ the "epidemic threshold". Indeed, the number of the contagions is not linear with the enhancement of the spreading rate; rather, it is highly enhanced by passing $\lambda_c$. 

By specializing \autoref{eq:tau_SIR_networks}, it is possible to recover that, in the case of random networks, the final size of an outbreak displays a first-order-phase transition; while for scale-free network $\lambda_c$ vanishes meaning that every epidemics could spread, regardless its biological weakness.
\\In particular, for a \underline{\textit{poissonian network}} $\langle k^2 \rangle = \langle k \rangle (\langle k \rangle + 1 )$\footnote{As a rule of thumb, for a Poisson-distributed random variable $k$ holds $\sigma_k^2 = \langle k^2 \rangle - \langle k \rangle^2 = <k>$. So, $\langle k^2 \rangle = \langle k \rangle^2 + \langle k \rangle$}, 
resulting in the "epidemic threshold of a random network" \cite{barabasi::2016networkbook}
\[ \lambda_c = 
	\frac{1}{\langle k \rangle} \Rightarrow R_{c-net} = 1.
\]
Being $\langle k \rangle$ always finite, and the reference scale for a random network, $\lambda_c$ play the role of a true threshold, which separates the epidemic regime to the non-endemic regime for as recovered in the previous section.
\\For a \underline{\textit{scale-free network}}, $\langle k^2 \rangle$ diverges for large networks ($N\to \infty$), since $\gamma < 3$ and the probability of having big hubs does not vanish. In this limit, $\lambda_c, \tau \to 0$ showing, respectevely, the fact that even low-transmissibility disease are successfully spreading in the population and are instantaneous.
An heuristic explaination of this is that, as for the "network-distance" contraction, hubs are super-infectors of every disease. Thus, even if a pathogen has a weak load of infectiveness, when a hub is infected, it could be able to pass it to a larger number of nodes, enabling the pathogen to persist within the population. In the present context, hubs are responsible of a "time-length" contraction.
\\The exposed examples are at the extrema of the possible scenarios. For most networks in the middle, e.g. the ones described in \autoref{ch:network-models}, the enanchement of $\langle k^2 \rangle > \langle k \rangle (\langle k \rangle +1)$ implies a reduction both of the epidemic threshold and the characteristic time of the disease. Thus, in contrast with the "mean-field" predictions, it is possible to refine the calculations for a specific network.

To sum up, for $\lambda>\lambda_c$ the total epidemic outbreak size $i_{\infty} \neq0$; while for $\lambda < \lambda_c$, $i_{\infty} = 0$, since the pathogen is not strong enough to reach an endemic state.
Moreover, $\lambda_c = 0$ for a "typical"($\gamma<3$) large scale-free network; while $\lambda_c \neq0$ for the random networks.

\subsection*{About the SIR equations}

No analytic solution exists for the equations regulating the SIR epidemic phenomenon. Furthermore, even the early times analysis exhibits some technical difficulties which demands for the numerical simulations of the ODEs of \autoref{eqs:SIR_degree-based}.

\subsection*{Early Times}
At early times ($t\approx 0 \implies r_k, s_k\approx0$), according to \autoref{eqs:SIR_degree-based} and \autoref{eq:Theta_SIR_network}, the fraction of infected is \vspace{3mm}
\begin{equation}
	\begin{cases}
		\frac{di_k}{dt} = - \mu i_k + i_0 \beta k \frac{\langle k \rangle -1}{\langle k \rangle } e^{t/\tau} \\ \\
		i(0) \stackrel{!}{=} i_0 \qquad \text{I.C.}
	\end{cases}
\end{equation}
whose, first equation, could be straightforwardly rewritten as
\begin{equation}
	\frac{d i_k}{dt} + \mu i_k - i_0 B e^{t/\tau} = 0
\end{equation}
where $i_0 B(k) := i_0 \beta k \frac{\langle k \rangle -1}{\langle k \rangle }$.

Hence, the solution of the last equation assumes the form
\begin{equation}
	i_k(t)=i_0 \left[C_1(k)e^{t/\tau} + C_2(k) e^{-\mu t}\right]
\end{equation}
where it has been defined as \vspace{3mm}
\begin{equation*}
	\begin{cases}
		C_1(k):= \frac{B(k) \tau}{1+ \mu \tau} = \lambda \cdot \left[k\left(\frac{\langle k \rangle -1}{\langle k \rangle }\right)\right]
		\cdot \frac{1}{\tau} \\ \\
		%\frac{\beta}{\mu+\tau^{-1}} \left(\frac{\langle k \rangle -1}{\langle k \rangle }\right) k \\ \\
		C_2(k):= 1 - C_1(k)
	\end{cases}
\end{equation*}
where there have been isolated, in $C_1(k)$,  the contribution from the pathogen ($\lambda:=\frac{\beta}{\mu}$), the network and the interaction among the two ($\tau^{-1}$). In all the previous equations, only the dependence was made explicit, since normally the epidemics parameter are fixed, while the degrees $k$ vary.

In this way, the initial exponential-phase has two contribution: the first, which grows in time, due to an interplay among the disease and the underlying network; the second is diminishing the number of infected according to the recovery of infected individuals.

For \textit{real networks}, the "degree exponent" $\gamma<3$ allowing, in the large networks limit, for $k$ and $\tau^{-1}$ to diverge. This, in turns, gives birth to a peculiar limit of $C_1(k)$: \vspace{3mm}
\begin{equation}
	C_1(k)\propto \tau k \propto \frac{1}{\int k^2 p_k dk}k  \to 0 \cdot \infty.
\end{equation}
where the last proportionality displays that $k$ diverge alongside with $\langle k^2 \rangle$. More specific insights could be reached only by considering the underlying probability degree distribution $p_k$.

On the other hand, for \textit{limited $\langle k \rangle$ networks} which find a good approximation in the random limit, i.e. when the individual contacts are limited by a cost-benefit constraint (e.g. power-grids), $\langle k^2 \rangle $ is limited and, therefore,
\begin{equation}
	i_k(t) \approx i(0)  + \left(C - C_2(\langle k \rangle )\mu \right) t
\end{equation}
where $C:= \frac{\beta (\langle k \rangle -1)}{\tau}$ since $\langle k \rangle \approx k$ and $e^{at}\approx1+at$ as either $t\approx0$ or $a \approx 0$. \vspace{3mm} The dependence on $k$ is, thus, lost: at the beginning of an epidemic all the nodes are statistically equivalent.

Ultimately, the degree-block approximation is itself a "mean-field" framework, since all the nodes, endowed with the same degree, are equivalent. However, this approach, although simplifying the calculations, it is not completely correct. Indeed, for the SIS model (with reinfection), the formal mathematical calculations drive to a vanishing epidemic threshold even for $\gamma>3$ \cite{barabasi::2016networkbook}. 

\chapter{Methodology}
\label{ch:Methodology}
As reported in \autoref{sec:ATechIntro}, the goal of this thesis is to study the "(quasi-)linear contagion" scenario, where the number of total infected individuals grows linearly in time rather than exponentially, as it could be the case for early times (cf. \autoref{ch:sir-models}). Indeed, as NPIs measures drives the social network to be sparse, there would no more be the presence of hubs but a "poorer" random model could capture the essence of the lockdown.

To this purpose, it has been sequentially selected a network model (\autoref{ch:network-models}) over which the (degree-based) SIR simulates a disease spreading. Moreover, for every plot, it has been reported the mean-field SIR approximation to appreciate the effects of the underlying topologies (cf. \autoref{ch:sir-models}).

So, as in \cite{Thurner::NetBasedExpl}, initialize the $N$ total individuals as susceptible and randomly pick $i(0) = 10$ vertexes as the starting infectors. Moreover, choose an "epidemic", i.e. fix $\beta, \mu$, alongside with the average number of contacts $\langle k \rangle =:D$ and $p$\footnote{The fraction of long-range contacts} in the underlying network. 
\\At every time-step $t$, find all the infected nodes and infect the susceptible neighbors with probability $\beta$ (transmissibility or microscopic spreading rate). After the "infectious" phase, at the same time-step $t$, simulate their recovery with probability $\mu = 1/d$ (recovery rate), where $d$ are the average days an individual stays infected.

\noindent\fbox{%
    \parbox{0.97\textwidth}{%
	\textit{To implement the stochasticity nature of an epidemic, generate a random number between $q \in [0,1]$ and infect the chose susceptible neighbors as $q\leq \beta$. Similarly, for the recovery transition with $\mu$.}
    }%
}

Proceed iterating the exposed infection chain, until the dynamic comes to a halt, i.e. $i(\infty)=0$. In general, it could be find also that $s(\infty)\neq0, r(\infty) \neq0$. 
\\In particular, for every $t$, collect the "new daily cases" $C(t)$ alongside with their (normalized\footnote{It has been chosen not to normalize the "new daily cases" since are $O(1)$ while $N\sim O(3)$}) "cumulative sum" $P(t):=\sum_{\bar{t}\leq t} C(t)/N$, as shown in the panels in \autoref{sec:ATechIntro} reporting the real collected data. In details, $C(t) = N(i(t) - i(t-1) + r(t) - r(t-1))$: for this two states spreading $(I(t-1),S(t-1)) \to (R(t),S(t))$, $C(t) = 1$ as $i(t)-i(t-1)=0$ but $r(t)-r(t-1) = 1$.

After having collected $C(t) \text{ and } P(t)$ also for the "Annealed Mean-Field Network (AMFN)" (see \autoref{sec:Annealed_MF_Network}), it is produced a plot with the joined $C(t) and P(t)$ from a specific network, e.g. Watts-Strogatz, and the AMFN. Moreover, there would be a series of plots for each model, since there are some free parameters to be fixed: $ \langle k \rangle, p, \beta,\mu$. Thanks to this method, it would be much clear to look for the departure of the network spread from the mean-field SIR.

\section*{Annealed Mean-Field Network}
\label{sec:Annealed_MF_Network}
As a benchmark of the network results, it has been introduced the "annealed-mean-field" network to properly simulate a homogeneous mixing with a fixed degree per node $D$. %For now, it is enough to know that the SIR model allows the susceptible nodes to be infected at a rate $\beta$ and the infected to recover at a rate $\mu$ (see \autoref{ch:sir-models}).
The definition of a AMFN will be given through the practical implementation of a SIR spreading over it, i.e. using a bottom-up approach.
Indeed, the mean-field SIR model is obtain, at every time-step $t$, with the following scheme: 
\begin{itemize}
	\item select an individual $I_1$ over the pool of infected;
	\item chose $\langle k \rangle$ neighbors at random among all the nodes\footnote{A fair spreading should consider as available contacts all the nodes, i.e. the susceptible but also the already infected or recovered ones. Indeed, all nodes are possible count; while only the susceptible ones could be infected.}, e.g. $I_2$,$I_3$;
	\item try to infect them with probability $\beta$ and to recover with probability $\mu$.
\end{itemize}
With this procedure, whenever the target infected node, e.g. $I_2$, is chosen to pass the pathogen at a future time $t'$, it is highly improbable to re-select its past infector $I_1$, since the probability goes as $D/N$, which typically is much less than one. 
\\Moreover, the number of neighbors is fixed to $\langle k \rangle$, giving to this model the "annealed" characteristic name.

As promised, a AMFN is defined as a (directed) random network whose $D$ links are randomly chosen by each node where the agent lays down. With this technique, the phenomenon is well mixed emulating a "mean-field" approximation.

The spreading on network resembles the mean-field model, alias AMFN in our case, whenever either $R_0$ or the long-range connections $p \cdot N$ are high enough to allow the disease, reaching the epidemic state rapidly. Indeed, the comparison between mean-field and network spreading regards the fraction of total infected nodes and not how the nodes are infected: for the mean-field case the neighbors are chosen randomly along the circle; while they are fixed for the chosen network case. For this reason, there could an accordance in the measures considered.

\section*{(Quasi-)Linear Contagion}
In order to analyze a linear behavior of $P(t)$, it worths recall that $R_{0c} := \beta D_c / \mu \sim 1$ is the critical basic reproduction number, which separates the exponential growth ($R_0 > 1$) from the exponential decay ($R_0 < 1$) of the number of infected. The critical degree $D_c = \frac{\mu}{\beta \langle k \rangle}$ and $C(t) \propto R^{t/d}$. So, as the epidemic is fixed, the above behavior are recovered, respectively, for $D > D_c$ and $D<D_c$. For $D=D_c$ \cite{Thurner::NetBasedExpl}, the number of infected will behave critically, i.e. $P(t) \propto t^{\alpha}$, where $\alpha \in (1,2)$ for a simply connected (no multiedges or loops) short-range network embedded in dimension $2$. More in detail, $\alpha \sim 2$ for a network resembling a regular lattice in $2$ dimensions, since $C(t) \propto t^2$ is proportional to the covered area of the infection front; while $\alpha \sim 1$ for a tree or a collection of 1-dimensional chains, as the front spreads linearly as a flame along a fuse, i.e. $C(t) \propto t$. In between these regimes, the specific $\alpha$ is influenced and shaped by non-pharmaceutical interventions (NPIs).

For a sub-exponential growth, it has to be expected that the network features, e.g. $\langle k^2 \rangle$, play a significant role in this context. In particular, for a fixed pathogen with $\lambda = \beta/ \mu$, $R_0 \propto D$ and it would be natural to search for a critical average degree $D_c$ (\cite{Thurner::NetBasedExpl}), when it is informative (e.g. not in scale-free networks), such that, fixing $p, \beta, \mu$, 
\begin{itemize}
	\item for $D>D_c$, the situation resemble a mean-field with $P(t) = R(t+d)$, where the shift in time is due to the fact that the average time of one infected to recover is $d$ days. In addition, the number of infected assumes a typical bump on the immediate days after the starting, which is an evident mark of an exponential growth both in $C(t) \text{ and } P(t)$.
	\item for $D<D_c$, the sub-exponential behavior is recovered with the consequence that the final outbreak size is lower than the mean-field one.
\end{itemize}
A proper way to characterize this transition is per the definition of the order parameter \cite{Thurner::NetBasedExpl} as the standard deviation (SD) of the new daily cases, after excluding all the days without new cases.
\\Formally,
\begin{equation}
	O := SD(C(t)) = 
	\begin{cases}
		0 \qquad \text{if $C(t) = constant$}\\\\
		\neq 0 \qquad \text{otherwise} 
	\end{cases}
\end{equation}
where $SD$ is the "standard deviation".
\\As anticipated, a linear growth is possible only if the new daily cases are constants, since $P(t) \sim t^{\alpha}$. For the "S-shaped" curve, resembling a logistic curve, a bump in the daily infected occurs producing an enhancement of the $SD$. Hence, a SD deviating from zero signals the presence of a nonlinear increase of the cumulative positive cases, $P(t)$ \cite{Thurner::NetBasedExpl}. Finally, $D_c$ depends on the set of parameters $p, \beta, \mu$ \cite{Thurner::NetBasedExpl} and, eventually $N$.

Also $R_0$ should be modified as it represents only the initial strength of the pathogen without considering the underlying topology, e.g. the "long-range" contacts $pN$. In other words, equal $R_0$s could drive to completely different scenarios (see the next chapter).

To sum up, it is going to be analyzed the graphic of the epidemiological quantities, such as $P(t)$ and $C(t)$, simulated via an heterogeneous SIR model on networks (Regular graph, Watts-Strogatz model, Poissonian SW Network, Caveman model and a Barabási-Albert Model) and benchmarked with the homologous $P(t)$ and $C(t)$ on a AMFN. In these plots, there are also reported the standard $R_0 := \beta D / \mu$ alongside with the $R_0(\delta) := R_0 /\delta$ where $\delta:=\langle d \rangle $ is the average path length.

Aside of this, it is reported the dependence of $SD(C(t))$ on $D$, in order to characterize the (quasi-linear) behavior of $P(t)$ with an $SD(C(t)) \sim 0$; while the exponential curves with a standard deviation $SD$ sensibly different from $0$.

\subsection*{Analytical critical degree}
In Statistical Mechanics, the dimensionality of a system is crucial to legitimate the mean-field approximation: the Ising Model is well modeled by a mean-field in a dimension $d \geq 4$ as it captures its phase transition.

In the following, $D_c$ is derived for an ER model and a ring-shaped poissonian small-world network (see \autoref{sec:PSW_network}) with $D$ average degree, $p$ rewiring probability, $D(1-p)$ short-range per node links, e.g. family, and $Dp$ long-range connections, e.g. leisure activities. So, the probability that an infector will infect a susceptible person in the next d days is $\beta_d:= 1-(1-\beta)^d \sim \beta d = \beta/\mu$, i.e. the complementary probability of not having a "risky" interaction for $d$ days.

Taking care of the dimensional effects, it worths to define the curvature of the front as $ \mathcal{K}:= \pm 1/\mathcal{R}_{front}$ where $\mathcal{R}_{front}$ is the radius of the fitting circle of the front: $\mathcal{K}>0$ for concave case; while $\mathcal{K}<0$ if the front surrounds an island of susceptible. For the poissonian small-world network, $\mathcal{K} = 0$, since the front is flat.    

Moreover, $R_0$ is exact only for a naive population, i.e. at early times. In fact, if a node is later infected, it would find at least one neighbor infected, alias its infector. So, taking care of the epidemic spreading, $R := (D-n)rd$, where $n$ is the average number of not-susceptible neighbors which, further, implies that $\nu := 1 - n/D$ is the fraction of susceptible. This latter quantity has to be interpreted as an general estimate of how infectious is a disease, i.e. for low infectivity, $\nu\sim1$ which implies $D \lesssim D_c$.
To be precise, for the "fuse-mode" $\nu$ represents only the fraction of susceptible in the neighborhood of the infection front; while for the ER model, $n$ embodies the entire population.
\\Whenever, $n$ is known, 
\begin{equation}
	D_c = n + \frac{1}{rd}.
	\label{eq:D_c_analytical_discussion}
\end{equation}   

Moving forward with the calculations, the network models have to be exploited. In particular, it is going to be discussed the ER model (see \autoref{ch:network-models}) and the tree- or chain-like "fuse model" \cite{Thurner::NetBasedExpl}. For low infection level, i.e. $\beta<<1$ or $\nu\sim1$, the claim is that $n\sim 1$ for a random graph, while $n \sim (1-\nu)D$ with $\nu\sim 1/2$ for a "fuse-model": $\nu\sim 1/2$ since the disease spreads on half of the tree, living the remaining nodes as susceptible. 

\subsection*{Erdös-Rényi Graph}
For a Erdös-Rényi random graph with "rewiring probability" $p$, the average degree of contacts is $D = Np$. So, the number of un-infectable nodes from an infected is $n \sim 1 + (D-1)(1-\nu)$, as $1$ is the past infector of the fixed node summed with the remaining neighbors which are infected or recovered \cite{Thurner::Appendix_NetBasedExpl}. 
\\Inserting the obtained $n$ in \autoref{eq:D_c_analytical_discussion}, 
\begin{equation}
	D_c^{ER} = 1 + \frac{1}{\nu rd}.
\end{equation}
which grasps the fact that for low infection levels $n\sim1$. 
In addition, $D_c^{ER}$ increases as $\nu$ drops: it has been needed more contacts to spread a disease if there are less susceptible available.  
%\\Other way around, for a fixed $D$ since $\nu \leq 1$,
%\begin{equation}
%	\nu_c = \text{min}\left(1, \frac{1}{(D-1)rd}\right)
%	\label{eq:nu_c_ER_model}
%\end{equation} 
%which diminish as $D$ increases: if, on average, $\nu' < \nu$ it is likely to have a diffuse pathogen since $D_c' >  D_c$. 
%Moreover, $\nu_c = 1$ provides that a pathogen dies out whenever, at the beginning of the spreading, the average contacts $D < 1+1/rd$.

\subsection*{The fuse model}
\begin{figure}[t]
	%scale = 0.27, trim = {10cm 0 1cm 0}
	\includegraphics[trim = {2cm 0 1cm 0}, width=\textwidth]{../images/Ch_TheEpidModels_Methodology/NN_Conf_Model_ordp_p0.3_beta0.1_mu0.25.png}
	\centering
	\caption{SD-Phase Transition of a Poissonian-SW Network with $N = 10^{3}, \beta = 0.1, D_c \sim 4.8, \mu = 0.25$. As before, the $D_c$ of the figure differs from the analytical estimation for finite size effects.}
	\label{fig:SD_Threshold_Fuse_Model}
\end{figure}

In this section, it is going to be obtained, for a one-dimensional regular grid with average degree $D$, the critical degree \cite{Thurner::NetBasedExpl} 
\begin{equation}
	D_c = 1 + \frac{2}{\beta d(1+p)}.
	\label{eq:Dc_PSW_network}
\end{equation}
The same result may be used for the poissonian small-world network.

The first step is to introduce an invariant probability function $\rho(x,t) = f(x-vt)$ that grasps the dynamics of the infection front: $\rho(x,t)$ is the probability a node in the $x$ location is infected at time $t$.
In particular, $x$ is the network distance, as the number of in-between nodes from the initial infected node; $t$ is the time passed in days after disease started infecting and $v$ is the traveling velocity of the spreading along the $x$ axis.
A pedagogical representation of $\rho$ could be find in \cite{Thurner::Appendix_NetBasedExpl}: roughly speaking, it has the shape of a $\arctan(x-vt)$ where, on the $x$-axis, there is the 1d-chain on  nodes; while, on the ordinates, it has been translated to fit the $y \in [0,1]$ interval\footnote{The "past" nodes have higher $\rho$ while the future nodes have diminishing probability of being infected.}.   Furthermore, it has been centered to an arbitrary node ($q$), which has $\rho(q) = 1/2$.

To estimate $v \text{ and } \rho(x,t)$, it is taken advantage on $\rho(x,t+1)$ assuming that $v$ is such that all the "non-susceptible nodes" are indeed infected, not recovered. So, the probability of being infected at a following time $t+1$ is
\begin{equation}
	\rho(x, t+1) = \rho(x,t)+(1-\nu)2D \beta \langle \rho \rangle(x,t) (1-\rho(x,t))
	\label{eq:fuse_model_rho(x,t+1)}
\end{equation}  
where $\langle \rho \rangle(x,t)$ is the (local) average probability of being infected at $(x,t)$ over the adjacent neighbors.
In other words, the \autoref{eq:fuse_model_rho(x,t+1)} considers the probability of already being infect; alongside with 2 terms:
\begin{itemize}
	\item $1-\nu$ and $1-\rho$ which are the fraction of infected nodes and the probability of being susceptible;
	\item $\delta:= 2(1-\nu)D \beta$ is the total probability rate of having (at least) a contagion over the $2(1-\nu)D$ infected neighbors.
\end{itemize}
For a one-dimensional front, its radius proceed at a $x=vt$ pace and, defining $\tau$ as the next infection time, the a ratio of the non-susceptible/susceptible nodes may be approximated as 
\begin{equation*}
	r:= \frac{1-\nu}{\nu} = \frac{t+\tau}{t} = 1+\frac{\tau}{t} = 1+v \tau \mathcal{k}.
\end{equation*}
Moreover, defining $\tau$ as the time one single node is infected $v \cdot \tau = 1$ implying that 
\[ R = \frac{1}{2+\mathcal{K}} = \frac{1}{2} \]
since for a flat "fuse model" $\mathcal{K} = 0$. 
The naive expectation is recovered as $\nu \sim 1/2$. 

Inserting the \autoref{eq:fuse_model_rho(x,t+1)} on the invariance condition 
\begin{equation}
	\frac{\partial \rho}{\partial t} = - \frac{\partial x}{\partial t} \frac{\partial \rho}{\partial x} = -v \frac{\partial \rho}{\partial x},
\end{equation}
with the further approximation that $\langle \rho \rangle(x,t) \sim \rho(x,t)$,
\begin{equation}
	\rho(x,t) = \frac{1}{1+e^{\delta(\tau x - t)}}
\end{equation}
with $\delta:= 2(1-\nu)D \beta$. 
For this solution, $\rho(vt,t) = \rho(0,0) = 1/2$ marks the infection front \cite{Thurner::Appendix_NetBasedExpl}. 

The average probability of the nodes on the left of the front being infected is
\begin{equation}
	\hat{\rho} \sim \frac{1}{\nu D} \int_{-\nu D}^{0} dx \rho(x,0) = 1+ \frac{1}{\delta'} \log( \frac{1+e^{-\delta'}}{2} )
	\sim \frac{1}{2} + \frac{1}{8} \delta' + O(\tau^3) 
	\label{eq:rho_hat_fuse_model}
\end{equation}
where it has been exploited that $\delta':=\tau \delta \nu D$ and the last expansion as $\tau<<1$. Thus, $\hat{\rho}$ could also interpreted as the probability of the past infected nodes.

Moreover, $n$ could be obtained considering the situation where a $\nu$ nodes are susceptible but the "past" nodes are not and viceversa: $\nu \hat{\rho} \text{ or } (1-\nu)(1- \hat{\rho})$. 
\\Formally,
\begin{equation}
	n \sim 1+ (D-1)(\nu\hat{\rho}+(1-\nu)(1-\hat{\rho}))
	\sim 1+ (D-1)(\frac{1}{2} - (1-2\nu) \frac{1}{8} \delta')
\end{equation}
which for a flat infection, where $\nu\sim 1/2$, reduces to
\begin{equation}
	n = 1 + \frac{1}{2}(D-1).
\end{equation} 

Ultimately, introducing the $pD$ long-range ("lr") neighbors; while keeping the infection low, $n_{lr} \sim 1$.
\\Since $n$ is a local estimate, removing the $n_{lr}$ contribution,
\begin{equation}
	n = 1 + \frac{1-p}{2}(D-1)
\end{equation}   
and, therefore,
\begin{equation}
	D_c = 1+ \frac{2}{\beta d(1+p)}.
	\label{eq:final_D_c_fuse_network}
\end{equation}

This estimate express the critical degree, i.e. the average number of contact under which the mean-field approximation fails, as a function of $p,\beta,\mu = 1/d$. As a second order approximation in $\tau$ as been exploited in \autoref{eq:rho_hat_fuse_model}, overestimates the number of infected which, in turn, drives to an overestimation of $D_c$. Other sources of divergence from the simulated critical degree may be the finite size effects or deviation from $\mathcal{k} = 0$.   

\section*{Parameters Discussion}

According to \cite{Liu::2021_Review_SContactPattern} (2021), the most relevant studies \footnote{These works are extracted from PubMed, Medline, Embase and Google Scholar} on social contact patterns report $2-5$ average contacts per day during the lockdown, i.e. nearly the household size. This estimate is, roughly, $5$ times less than the pre-COVID-19 ones of $7-29$ average contacts per day. On the other hand, Mossong et Al. (\cite{Mossong:2008_preCOVID-europe_SCP}) (2008), based on $8$ european states, reported a pre-COVID-19 average of $13$ contacts per day, while Leung et Al. \cite{Leung:2017_HKSocialCP} (2017) find an average per day contacts of $8$ individuals for the abitants of Hong Kong. Moreover, the quoted surveys have taken into consideration individuals of different ages, with the further possibility of determine how society could be clustered by age, e.g. elderly are likely in contact with children.
The present thesis does not aim at guessing a balanced average of contacts per day but, anyway, its purpose is to emulate a typical social network. Thus, the main plots are going to be focused on the $D < 30$ regime; while little room is going to be left for $D > 30$ as it can be the case for leisure activities, e.g. exhibitions or discotheques. Since COVID-19 spreads over face-to-face interactions, the latter case is crucial for the re-opening of the crowded activities.

By constrast, the epidemic parameters are chosen independently on the real ones, in order to capture many different scenarios of epidemics. Other way around, transmissibility $\beta$ and recovery $\mu$ depends on many factor and especially on the studied population. Therefore, to have a broad inspection of the capabilities of a network topology it worths consider different combinations of $\beta \textnormal{ and }\mu$. 

In the "Results" chapter, $D_c$ for different simulation is reported but only for poissonian small-world network, it approximates the point where the $SD(C(t))$ start increasing.

%$R_0$ try to modify it with $clust coefficient^{-1} or avg path length^{-1}$, i.e. an average estimate of the "openness" of the network.
\chapter{MISSING TIPS}
\begin{enumerate}
	\item inserisci gli infetti come plot blue dots mec black come nei plots di riferimento
	\item spiega l'immagine dei plots con direttamente un grafico perché xe massa roba
	\item frame to black, shadow legend sim 0.8, if $R_0 \leq 1$ put legend aside e colore più scuro per mf and net totcases; 
	\item scrivi per bene la parte su $R_0$ nuovo che si propone come aggiornamento del vecchio;
	\item plots capitoli precedenti controlla il copyright;
	\item inserisci i plots della JHU;
	\item rifai il $NN Conf Model$ affinché si propaghi in un'unica direzione come il fuse model;
	\item sistema grafici autogenerati;
	\item riscala tutto per il massimo totcases, può essere una buona idea per avere un enanchement della sottiletta in WS-pruned D = 2?
	\item nell'introduzione ci sono ancora i ??
	\item forse anche $R_0$ è mal definito 
	\item WS-pruned: fai rete che a D = 2 ha R0 circa 1.4 così è minor di R0-net = 2 for D = 2.
	\item dì che hai scelto come daily new cases 0 all'inizio
	\item spiega la stella nei plot
	\item R-0c or R-0c-net -- sicuro di usare anche per il mf R-c-net? Perché non ne discende ne niente che R0*s(t) = R-c-net. Quindi, io direi di fissarlo a R-c-net a 1 e fine.
\end{enumerate}
\begin{itemize}	
	\item insert the Koenigsberg graph, the "adjacent definition for two vertexes" and the Theorem 1.1 (FedLor), deg(loop) = 2 as $2L = \sum k$ must hold;
	\item add a Social Network eloquent image, augment the simple network resolution or do them by yourself;
	\item World of War-craft had a serious bug which made epidemiologists devoting to it
	\item Cit: Dario:Ad quid? - a quale scopo ?	Rebus sic stantibus - stando così le cose
	\item Mene: Est modus in rebus
	\item Andre: Ca van san direct
	\item "A gratis"
	\item CHECK K = 0 (AND INSERT A TREE PLOT);
	\item Say "No natural essence in lock-down". So, at the same level, we want to study the topologies to best fit the containment of the disease;
\end{itemize}

\chapter{Results}
\label{ch:Results}
In this chapter, it is going to be discussed the results obtained by the simulations of the heterogeneous SIR model compared with the theoretical framework previously developed.
The networks, in the following, are all composed by $N = 1000$ nodes and the simulations are performed over the Google Colab CPUs of \cite{GoogleColab}.

\section{Regular Graph}
\label{sec:RegularGraph}


Starting from a fully connected graph, i.e. $D = 999$, a pruning scheme is introduced to clarify whether the spreading would have changed by fixing $R_0$, while dividing $D$ and doubling $\beta$: $R_0 = const = (D/k) \cdot (\beta k) / \mu$ where $k$ goes from 1 to the maximum power of $2$ s.t. $D/k > 1$.
Practically, to generate a fully-connected graph it has been used a "Watts-Strogatz" algorithm with $p = 0$. Since this algorithm works only with even $D$, all the uneven $D$ where, arbitrarily, rounded to the nearestest even integer, e.g. $125 \to 124$. In this way, the reaching of the endpoint of $D = 2$ is faster. Moreover, \autoref{eq:lambdac_Rc_netSIR} implies that $R_{c-net} = (1-D^{-1})^{-1}$ decreases when $D$ increases. As expected, the critical basic reproduction number is more likely to be overtaken when a regular lattice has more neighbors.

As said in the \autoref{ch:sir-models}, an epidemic could reach a considerable fraction of the population if $R_0 > R_{c-net} \sim 1$. For the sake of concreteness, let's focus on \autoref{fig:RegLat_p0_R0less1} whose $\mu = 0.8$, while $D \textnormal{ and }\beta$ vary. For brevity only the graphic in the \autoref{fig:RegLat_p0_R0less1} has been generated for this regime, since the others with $D \textnormal{ and } \beta$ are slightly different. 

As reported in \autoref{ch:Methodology}, the following plots display as sup-title the
\begin{itemize}
	\item $R_0 \textnormal{ and } R_{c-net}$;
	\item the previous quantities but rescale by the average path length $\sigma$ whose pedix explicits wheter the network is connected ($C$) or the size of the largest component ($max-cc$);
	\item the Order Parameter and its standar deviation in brackets;
	\item the average degree $D$, the rewiring probability $p$ and the epidemic parameter $\beta \textnormal{ and } \mu$.
\end{itemize}

The graphic shows that for $R_0 < R_{c-net}$ the epidemic remains limited to a small fraction of the population ($1.8 \%$ of the total population); while in the ANMF case, the percentage is comparable, since the epidemic is "weak". Moreover, at these low values of $R_0$ there could be appreciated how the stochasticity plays a relevant role in making the spreading more noisy and short. Indeed, the low-lasting green curves embodies the fact that pathogen has died our at lower time as, due to small "epidemic stregth" $R_0 $, the likelihood of spreading is small.   
As said, fixed $R_0 < R_{c-net}$, as $D$ increases, the curves exhibit a little growth in the total cases as expected: this phenomonon is going to be showed in $R_0 > R_{c-net}$. 
 
\newpage
\begin{figure}[t]
	\includegraphics[width = \textwidth, height = 7cm]{Results/WS_Pruned/p0.0/WS_Pruned_SIR_R0_1_N1000_D2.0_p0.0_beta0.25_mu0.8.png} %../images/
	\centering
	\caption{Network SIR model benchmarked with the AMFN for $R_0 < 1$.}
	\label{fig:RegLat_p0_R0less1}
\end{figure}

Strengthening the epidemic, i.e. chosing $\beta, \mu \textnormal{ s.t. } R_0 = 2.5$, some peculiar behaviors arise. \\Starting from the lowest $D = 6$, \autoref{fig:RegLat_R0maj1_D6} captures the (quasi-)linearity even in the early stage of contagion. Note that $D = 6$, is slightly above the upper bound of $2-5$ lockdown contacts per person \cite{Liu::2021_Review_SContactPattern}. In particular, this network assumes a regular structure of contacts limited in a portion of it. Thus, the number of new daily cases is limited (blue curve) and, so, it is the number of total cases (green one). In addition, in this particular case, only the AMFN could increases until the star-shaped turning point is reached: from that time on the number of cases are going to decrease since $R_0 \cdot s(t) < R_{c-net}$, as the susceptible are no more feeding the pathogen which is going to die out. On the other hand, the network epidemic

\begin{figure}
    \centering
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=.97\textwidth]{Results/WS_Pruned/p0.0/WS_Pruned_SIR_R0_3_N1000_D6.0_p0.0_beta0.083_mu0.2.png} 
        \caption{Generic} \label{fig:RegLat_R0maj1_D6}
    \end{subfigure}
	\vfill
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=.97\textwidth]{Results/WS_Pruned/p0.0/WS_Pruned_SIR_R0_3_N1000_D30.0_p0.0_beta0.017_mu0.2.png} 
        \caption{Competitors} \label{fig:RegLat_R0maj1_D30}
    \end{subfigure}
    \vspace{1cm}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=.97\textwidth]{Results/WS_Pruned/p0.0/WS_Pruned_SIR_R0_3_N1000_D500.0_p0.0_beta0.001_mu0.2.png} 
        \caption{Price regulation} \label{fig:RegLat_R0maj1_D500}
    \end{subfigure}
    \caption{Some general caption of all the figures. In (\autoref{fig:RegLat_R0maj1_D6}) you can see a  green square....}
\end{figure}


\chapter{Conclusions}


\appendix
\chapter{Appendix - Network Models}

\section{Scale-Free Distributions Details}
\label{sec:SFD_details}
In the following part, the discrete approach, assuming $k = 0,1,2\cdots$, is going to be developed; while only the continuum result would be reported. In fact, the discrete approach provides the ready-to-use formulas handle by a calculator which, by construction, performs discrete operations. On the other hand, the continuum approach is useful for analytically support the calculations and could be obtained similarly to the discrete case.
Formally, by exploiting the constraint on probabilities $p_k = C\cdot k^{-\gamma}$ \[\sum_{k=1}^{\infty} p_k = 1,\] it is possible to recover the closed form \cite{barabasi::2016networkbook}
\begin{equation}
	p_k = \frac{1-p_0}{\zeta(\gamma)}k^{-\gamma}.
	\label{eq:p_scalefree}
\end{equation}
where $p_0 = p_{k=0}$ has been separated, since the power-law diverges for $k=0$.

On the another hand, the continuum formalism drives to \[p(k) = (\gamma-1)k_{min}^{\gamma-1}k^{-\gamma},\] since the the smallest value for the power law to hold is not $1$, as in the discrete approach, but the minimum degree for a node $k_{min}$. 

Formally, $k_{min}$ is the degree for which
\begin{equation}
	\int_0^{k_{min}-1} p(k) dk \stackrel{!}{=} 1/N \qquad \textnormal{or} \qquad P(k_{min}-1) \stackrel{!}{=} 1/N.
	\label{eq:kminrel}
\end{equation}

In the next part, the allowed maximum degree $k_{max}$ would be obtained both for an exponential bounded and a power-law distributions. Through this quantitative insight, it would be shown the dependence of $k_{max}$ on $N$, i.e. the size of the network. 

In particular, the exponential distribution for which \(\int_{k_{min}}^{\infty} p(k) dk = 1\) holds, is \[ p_k = \lambda e^{\lambda k_{min}} e^{-\lambda k} \] where, using \autoref{eq:kminrel}, $k_{min} = 1$.
Furthermore, $k_{max}$ (or the "natural cutoff") is defined to be such that the at most one node $i$ could have a degree $k_i$ larger than $k_{max}$. \newline In other words, the cumulative distribution for degrees greater than $k_{max}$ is $1/N$; and, hence, $k_{max}$ is expected to be the size of the bigger hub.

In formulas, 
\begin{equation}
	\int_{k_{max}}^{\infty} p(k) dk = 1/N \implies k_{max} = k_{min} + \frac{\ln(N)}{\lambda}.
	\label{eq:Expkmax}	
\end{equation}
So, $k_{max}$ would not differ much with respect to $k_{min}$ since $\ln(N)$ strongly reduces contribution due to the size of the graph $N$.
For a Poisson distribution, more effort is needed but the same interpretation holds.

On the other hand, applying the same rationale to a scale-free network, 
\begin{equation}
	k_{max} = k_{min}\,N^{\frac{1}{\gamma-1}}.
	\label{eq:SFkmax}
\end{equation}
In this case, there could be order of magnitude of difference between $k_{max}$ and $k_{min}$.
Indeed, as a simple example, WWW forms a graph of on $N \sim 10^5$ documents with $\langle k \rangle \simeq 4.6$ and $\gamma = 2.1$ \cite{barabasi::2016networkbook}. 
By assuming $\lambda = 1 \textnormal{ and recovering that }, k_{min} = 1$ using \autoref{eq:kminrel}, $k_{max} \sim 14$; while $k_{max} = 95.000$ for a scale-free network with $\gamma = 2.1$, e.g. WWW network. This short results reinforces the quoted insight that for the class of "exponentially bounded distributions" the hubs are strongly forbidden since the nodes have comparably the same degree. Instead, the highly connected nodes are naturally arising by increasing $N$ with an underlying power-law distribution.
\label{sec:SFProperties}

Not all the network are scale-free since the hubs are present only if nodes have unlimited degree capacities.
In fact, there could be constraints, such as cost-benefit problem, which limit $k_{max}$ of the bigger node. In this overview, random network may be the best fit, e.g. national highways network, power grid of generators and switches, $\cdots$

With all the previous effort a more quantitative approach to the explanation on the "scale-free" etymology could be ultimate.
More precisely, defining the statistics momentums as
\begin{equation}
	\langle k^n \rangle := \int_{k_{min}}^{k_{max}} k^n\, p(k) dk
\end{equation}
it is possibile to recover the the mean and the variance for $n = 1,2$.
Thus, for a poissonian distribution, the nodes degree $k$ belong to $\langle k \rangle \pm  \langle k \rangle ^ {1/2}$ interval, which drives to the desired $\langle k \rangle$ scale. At the same time, for a power-law distribution holds	
\begin{equation}
	\langle k^n \rangle \sim \frac{k_{max}^{n-\gamma+1}-k_{min}^{n-\gamma+1}}{n-\gamma+1}
	.
\end{equation}
Therefore, knowing that real networks have typically $\gamma \in (2,3)$, e.g. WWW has $\gamma =  2.1$ \cite{barabasi::2016networkbook}
\begin{equation}
	k \in \langle k \rangle \pm \lim_{N \to \infty} \langle k^2 \rangle = \infty.
\end{equation}
where the hidden limit of \( k_{max} \to \infty \) is understood, since $k_{max}$ increases with the system size. 
So, no scale naturally arises.

The result rapidly quoted before is surrounded by three other regimes 
\cite{Cohen:2003_SFUSW}:

\begin{equation}
	\langle d \rangle = 
	\begin{cases}
		constant & \gamma = 2 \\ \\
		\ln(\ln(N)) & \gamma \in (2,3) \\ \\
		\frac{\ln(N)}{\ln(\ln(N))} & \gamma = 3 \\ \\
		\ln(N) & \gamma > 3
	\end{cases}
	\label{eq:USWdistance}
\end{equation}
\newline
\textbf{Anomalous Regime ($\gamma = 2$)}: Due to $k_{max} \sim N$, the network is forced into a "wheel rim configuration", a central hub with spoke nodes. In this regime, the average path length is independent on the size of the graph.
\newline
\textbf{Ultra-Small World Regime ($2 < \gamma < 3$)}: By predicting a $\ln(\ln(N))$ trend of the average distance, \autoref{eq:USWdistance} guides to the concept of "ultra-small world phenomenon" as a slower growth than random network. Indeed, recalling the worldwide social network, \(\ln\ln(N) \sim 3 \text{ while } \ln(N) \sim 22\) more than few times higher than the expected "six degrees of separation".
\newline
\textbf{Critical Point ($\gamma = 3$)}: Looking back on the \autoref{eq:kn_SFnets}, the variance ($n=2$) is finite, marking the passage between the small-world ($\sim \ln(N)$) and the ultra-small world ($\sim \ln(\ln(N))$) classes of graphs.
\newline
\textbf{Small World ($\gamma > 3$)}:
In this regime, the finiteness of the variance is understood, due to the light aggregation that the high-degree nodes could perform in these kind of networks.

Moreover, recalling the root of "ultra-small world" \autoref{eq:SFkmax}, for $N \lesssim 10^2$ the path length distributions overlap in all the reported regimes and, so, converges to a Poisson distribution ($\gamma > 3)$ whose hubs are small to produce the "ultra-small world" phenomenon. In fact, to validate the power-law nature of a distribution, the nodes degree should be separated by 2-3 orders of magnitude. Hence, reverting the \autoref{eq:SFkmax},
\[N = \left(\frac{k_{max}}{k_{min}} \right)^\frac{1}{\gamma -1},\]
which lucidly reproduce $N \sim 10^2$ for $\gamma = 2.1$ (WWW network).

Thus, \(\langle d \rangle\) is changing with $\gamma$, as the smaller it is, the shorter would be the average distance; but also with $N$, as it is the "ab ovo" hypothesis for having big hubs.

A sociological fact has to be remarked when trying to map the "ultra-small" world over social interactions. Indeed, an online experiment which aimed to replicate the "six degrees" concept find that individuals involved in complete chains, reaching their target, were less likely to send a message to a hub than individuals involved in incomplete chains \cite{Dodds:2003_GSonlineNet_6deg}. The reason may be self-imposed, since hubs are perceived as being busy, so contacted only in real need and avoid to concretely complete an "online experiment" challenge. Thus, network is not everything since actual success depends sensitively on individual incentives.


\newpage
\section{Recovering Power-Law Distribution}
\label{App:RecPLD}
Due to a new $m-$stubs node, the degree of an existing node $i$ changes at a rate
\begin{equation}
	\frac{dk_i}{dt} = m \Pi(k_i) = m \frac{k_i}{\sum_j k_j}
	\label{eq:BA_dk_i/dt}
	.
\end{equation}
Hence, for $t >> 1$, 
\begin{equation}
	k_i(t) = m \left(\frac{t}{t_i}\right)^\beta
	\label{eq:BA_modki(t)}
\end{equation}
where $\beta$, alias "dynamical exponent", has the value $\beta = 1/2$ \cite{barabasi::2016networkbook} and $t_i$ is the entering time for the node $i$.

The remarks, coming from the \autoref{eq:BA_modki(t)}, are multiple:
\begin{itemize}
	\item all the nodes follow the same power law, since $\beta$ is independent on the node label $i$. Furthermore, it is independent on $m$ and $m_0$ constructing parameters;
	\item the inverse dependence of $k_i \sim 1/t_i$ is a clear signal of the fact that old nodes are supposed to be larger, also called "first-mover advantage" in business;
	\item both $\beta = 1/2 < 1$ (sublinear growth) and the rate $dk_i/dt \sim 1/\sqrt{tt_i}$ underly a competing scenarios: the existing nodes compete for the $m-$new stubs with a growing bunch of nodes, resulting in a sublinear growth, which drives to a decreasing rate of acquaintances. 
\end{itemize}

To validate the power-law nature of the probability of having a certain node degree, by exploiting \autoref{eq:BA_modki(t)} it is possible note that the nodes $i$ with a degree higher than $k$ are such that \[t_i < t \left(\frac{m}{k} \right) ^{1/\beta}. \] Thus, for $t >> m_0$, the (cumulative) probability of having a degree less than $k$ is \[P(k) = 1 - \left(\frac{m}{k} \right)^{1/\beta}, \] from which the probability of having a degree $k$ could be recovered, by differentiating $P(k)$ and substituting $\beta = 1/2$, $p_k = 2m^2k^{-3}$ \cite{barabasi::2016networkbook}. As desired, the distribution exhibits a long-tail nature with $\gamma = 1/\beta +1 = 3$. Alongside with the BA model simulations, the dependence between $\gamma \textnormal{ and } \beta$ demonstrate the deep relationship among the graph topology and the temporal degree evolution, respectively contained in $\gamma$ and $\beta$ parameters.

Another feature present in \autoref{eq:BA_modki(t)} is that the degree distribution is independent on $t \textnormal{ and } N$ or a "stationary distribution". This result is the one expected, since the model aims at describing the real networks in general; thus, of rather different age and size.

\newpage
\section{The Origins of Preferential Attachment}
\label{App:OriginOfPA}
Since preferential attachment plays a relevant role in guiding the dynamics of a real network, the goal is to recover the same $\Pi(k)$ as the one assumed in the Barabási-Albert model.
It worths to introduce two classes of microscopic processes that generate it naturally \cite{barabasi::2016networkbook}:
\textit{"local (or random) mechanisms"} representing the interplay between random events and some structural properties of the network; and
\textit{"global (or optimized) mechanisms"} which take advantages of a cost-benefit analysis on the whole graph. 
In the following, it would be reviewed the "Link Selection" and "Copying" Models which belongs to the random class; and, finally, the "Optimization" Model of the second one.

\subsection{Local Mechanisms}
\subsubsection{Link Selection}
\label{sec:linear_pa_link_selection}
Assuming growth is at work, i.e. at each event time a new node is added, select randomly an edge and connect the new node to a vertex belonging to that edge.
Thus, the probability $q_k$ that the selected node, already present in the graph, has degree $k$ is 
\begin{equation}
	q_k = \frac{1}{\langle k \rangle} \cdot k p_k
\end{equation}
where the first factor exploiting the normalization constraint $\sum_k q_k = 1$.
Hence, it has the hoped form, being linear in the number of stubs ($k$) and in the frequency of the degree-k nodes ($p_k$). This displays an increasing of the chance of wiring to a degree $k$ node, by enhancing either the number of degrees or the presence of degree-k nodes themselves.

\subsubsection{Copying Model}
The idea is compute the probability of obtaining a connection of a degree-k node by mimicking the phenomenon of authors of a new webpage which copy links from existing webpages on the related topic \cite{Kleinberg:1999_WebAsAGraph}.

By introducing a new node $n$ in the network, as before, the difference stands in the link selection.
More in detail, selecting an existing node $u$ regardless of its degree, e.g. picking an arbitrary web document which topic is related to $n$, there are two possible way of connections:
\begin{itemize}
	\item Random (direct) connection: with a probability $p$, link $n$ to $u$;
	\item Copying (undirect) connection: with likelihood $1-p$, wire $n$ to an neighboring node $v$ of $u$. Reprising the web example, the new webpage $n$, copying the link of $u$, connects with its target $v$.
\end{itemize}

As a constraint to recover a dependence on $k$ of $\Pi(k)$, fix that $v$ should have degree $k$, by considering $p_v = k/2L$.
Hence, 
\begin{equation}
	\Pi(k) = \frac{p}{N} + \frac{k}{2L}(1-p)
	\label{eq:Pik_recovering}
\end{equation}
which is linear in k, yielding the desired preferential attachment.
The power of this approach is its "empirical" root which could be detected from citation networks to the social network of acquaintances.

\subsection{Global Mechanisms}
\subsection{Optimization Mechanism}
The principle of minimization of cost-benefits analysis in an economic market
\cite{Shively:2012_OverviewOfB-CAnalysis} can lead to preferential attachment whether a proper cost function is considered.

For simplicity assume that the vertexes are laying in an unit square, e.g. routers in a square continent. As before, at each event time a new router $i$ is added, while an edge with an existing node $j$ is created according to the cost function \cite{barabasi::2016networkbook}
\[
	C_i = min_{j\in G(V,E)} \left(\delta d_{ij}+h_j\right).
\]
where $d_{ij}$ is the euclidean distance between the vertex $i$ and $j$; while $h_j$ is the number of hops from $j$ to a pre-defined "center" which embodies the best network performance, e.g. net distribution hub. More clearly, $d_{ij}$ and $h_j$ represent the physical distance and "network-based" distance. Their summation describes the cost to be minimized for real results.

Depending on $\delta \textnormal{ and } N$, there are $3$ network topologies:
\begin{itemize}
	\item \textbf{Star Network $(\delta < (1/2)^{1/2})$}: only $\delta d_{ij}$ drives the growth, thus, the new nodes connects with the central one forming a star;
	\item \textbf{Random Network $(\delta \geq N^{1/2})$}: $h_j$ is the minimum quantity to be minimized. So, the new nodes connect to the nearest node;
	\item \textbf{Scale-free Network $(4 \leq \delta \leq N^{1/2})$}: in this intermediate regime scale-free topologies may be recovered as a result of optimization and randomness. In particular, optimization creates a basin of attraction with size $s \sim k_j$ and centered in $j$, within all the appearing nodes $i$ wire with $j$. On the other hand, Randomness is the chance of choosing a specific basin.
\end{itemize}

Thus, linear preferential attachment can come from both rational choice and random actions. Yet, most complex systems are driven by processes that have a bit of both luck or reason.


\bibliographystyle{plain}
\bibliography{../bib/my_bibliography.bib}
\end{document}	

\chapter{The Models}
\subsection{The Network Models}
To this end, we proceeded with the following steps.
At first, we pruned a fully connected graph of $N = 1000$ nodes, by keeping fixed $N$ while having $D$, i.e. the average of nearest neighbors. In parallel, since $R_0 = \frac{\beta \cdot D}{\mu}$, we doubled the $\beta$ \footnote{$\beta \in [N,2]$} parameter to keep the power of the epidemic $R_0$ fixed (\ref{fig:pruning_p0.0}). 

Since the aim is to understand the difference with respect to the "well-mixed" population model, we put in the same plot the number of total infected cases with the epidemics that spreads either on a specific network (in green) or in a "mean-field" fashion (in orange)\footnote{Moreover, the daily infected of the network (in blue) and of the mean-field (in magenta)}. More precisely, the "mean-field" approximation is a "D-neighbors-mean-field" where every node has $D$ nearest neighbors picked at random on the whole set of nodes. In this way, the hypothesis of the "well-mixed" population is recovered, i.e. everyone has the same probability to get in touch with an infected, and the spreading could be modeled with the exact same parameters ($\beta \textnormal{ and } \mu$) as the network epidemics.

Then, we introduced the "rewiring probability" $p$ which enables the distant nodes to connect faster, thus, obtaining a Erdös-Rényi (ER) graph with non-zero $p$. In this case, we could recover the "(pseudo) linearity" of the SIR infection, as it is shown in \ref{fig:pruning_p0.1}, using two similar methods of choosing the spreading parameters $\beta \textnormal{ and } \mu$:
\begin{itemize}
	\item pick a series of $\beta \in [0,1]$ and $\mu \in [0,1]$ coefficients at random;
	\item choosing $\mu \in [0,1]$ and fixing $p=0.1$. Then, as in the previous point, while halving D, the $\beta$ is doubled.
\end{itemize}
For conciseness, in \ref{fig:pruning_p0.1}, it is reported only the epidemics obtained by the "pruning procedure".
\newline
To further test the recovered (pseudo) linearity with respect to the mean-field benchmark, we used a Poissonian "small-world" network which enabled us to taking care of degree heterogeneity (i.e. different social contacts), family-clusters and their overlaps, small-world feature, given the fact that "leisure" activities may connect nodes that are diametrically opposite. Moreover, by changing the number of the average degree $D$ we may pass from light ("exponential spreading" for $D\simeq5$) to severe ("linear spreading" for  $D\simeq2.5$) NPIs measures, i.e. a severe lock-down.
The Poissonian "small-world" network could be realized by imposing, at first, a Poissonian degree sequence on all nodes ("Configuration Model"); then, re-link the nodes to their closest neighboring nodes, i.e. a nearest neighbors rewiring (NNR) procedure is applied. Thus, obtaining a network with poissonian degree distribution and the small-world property.
The (pseudo) linearity obtain with this graph is reported in the \ref{fig:NNR_Conf_Model}.

Furthermore, the work focused on analyzing the network spreading on a fully-connected cavemen graph, where the families cores could be taken into consideration; and also enabling the possibility of a "long-range" connection via the rewiring probability $p = 0.1$ (cfr. \ref{fig:Caveman_Model}). Finally for what concerns the different network models, it has been studied the case of "(pseudo) linear" epidemics on a Barabási-Albert (B-A) Model, i.e. with the presence of hubs (cfr. \ref{fig:Barabási_AlbertSIR}).

Moreover, the critical average degree $D_c$ \footnote{$D_c$ as reported in \cite{Thurner::NetBasedExpl} describes a "first-order-phase transition" between exponential and linear growth of an epidemic} , is going to be recovered for the quoted network topologies. A final up-to-date point, would be to use a Graph Neural Network architecture, based on the previously described scenarios and networks, in order the make epidemics forecasting. For this purpose, a relevant reference, is \cite{Davahli::USA_predicting_COVID19}

\section{The Epidemiological Models}

\section{Goals Outline}

\begin{enumerate}
	\item Assuming that a typical social network has an average number of contact (degree) $D\simeq16.5$(\cite{Liu::2021_Review_SContactPattern}\footnote{A "contact" is defined as per touching or talking at a certain distance. For a spreading of a SARS disease, this open the discussed topic on the validity of the "2 meters" distance policy \cite{Jones::2020_2metersSDIstance}}); while lockdown measures reduce it to the household size ($D\simeq3.5$\cite{Liu::2021_Review_SContactPattern}), our goal is to reproduce the dynamics of an epidemic spreading on a sparser network due to the Non-Pharmaceutical Interventions (NPIs) generally known as lockdown\footnote{For brevity, the word "lockdown" is an alias for all the "lockdown-like" measures undertaken to prevent the diffusion of COVID-19 without requiring Pharmaceutical drug treatment, e.g. confinement (the direct translation of "lockdown") but also contact tracing, higher hygiene, face masks,}. A direct comparison of the obtained curves with the states of Austria and the USA \cite{Thurner::NetBasedExpl} is satisfactory even at a plain-eye inspection. However, the project aims at studying of how the topology of a network could characterize a paradigmatic model as the SIR one. 
    So, no statistical fit has been performed, since a relevant work on the available data has to be performed, e.g. removing out-of-range values, impossible data combinations, and missing values on \href{https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series}{\textit{GitHub John Hopkins University}}\footnote{https://github.com/CSSEGISandData/COVID-19/tree/master/csse\_covid\_19\_data/csse\_covid\_19\_time\_series}. In addition, for the fitting procedure, it would be possible to extend the SIR model in accordance to the other reported compartments, e.g. death (SIRD), exposed (SEIR),$\dots$
	Anyway, these states express the (quasi-)linearity in the cumulative number of cases similarly to the simulated ones.
    Moreover, it has to be pointed out that the previous quoted states are representative of two opposite political policies, undertaken by many others states, in facing the COVID-19 pandemic: an immediate "severe lockdown" (Austria) and "light restriction" on contacts (USA).
	\item As claimed in \cite{Thurner::NetBasedExpl}, we want to find that for any fixed transmission rate, recovery rate and "long-range" probability ($\beta, \, \mu, \, p$) there exists a critical number of social contacts ($D_c$) which describes a first-order phase transition of the disease growth obtained with the SIR model:
	\begin{itemize}
		\item for $D<D_c$, linear growth, i.e. low infection prevalence, occurs;
		\item for $D>D_c$, "classical" exponential growth as predicted by the SIR model.
	\end{itemize}

    The rationale behind is that, since $R_0 := \beta\, D/\mu$, by augmenting the average degree of the underlying network also the $R_0$ is enhanced, resulting in a more powerful spreading.
    \item On the way to obtain $D_c$, the simulated plots show a strongly dependence on $p$, i.e. the probability of a node to have a distant neighbor ("long-range" attachment), which is as crucial as $\beta, \mu \textnormal{ and } D$ in the characterization of an epidemic spreading. Therefore, a new definition of $R_0 := R_0 $ is proposed, which integrate the possibility that same effective parameter but with different long-range attachment (e.g. one local and the other random attached) could drive to different scenarios.
	Finally, by setting our parameters to the empirical estimates of $\beta$ and $\mu$, we are going to estimate $D_c$.
\end{enumerate}

\newpage
\section{Relevant Figures}
\begin{verbatim}
	FIGURES ARE AFTER "\textbf{\end{document}}"
\end{verbatim}


\chapter[Model]{Model}
The main assumptions of the model are:
\begin{itemize}
    \item fixed number of total individuals $N$ connected by a social link;
    \item "undirected" adjacency matrix $A$, where $A_{i,j}=1$ if there's a link between the $i-th$- and $j-th-$node or $A_{i,j}=0$ otherwise;
    \item transmission per day probability $r$ and $d$ days for an infected individual to be recovered, i.e. immune or naturally dead;
    \item average degree $D$ and shortcut (or rewiring) probability $\epsilon$
\end{itemize}

As one may expect, the $r$ and $d$ parameters are in a one-to-one map with to the SIR ones, respectively, for the recovery and transmission rates:
\begin{equation}
    \label{eq::d/r to gamma/beta}
    \gamma=1/d, \quad \beta=rD/N.
\end{equation}

At each time-step (day), the infection curve of positive cases, $P(t)$, is the cumulative sum of the number of new cases $C(t)$.
So, if $D>D_c$ the "well-mixed" (or homogeneous/mean-field) approximation may be applied, with the result that $C(t)$ is associated to $R(t)$ up to a time-shift of $d$, since all the infected are predicted to become recovered after $d$ days.

\chapter[Results]{Results}
\section{Infection Dynamics}
To gain some intuition about the importance of modeling the underlying network, we've made the disease spread for two different values of $D$ but fixing $N_{total}=1000,\,D=8, \, \epsilon=0.1, d=6 \, \textnormal{days},r=0.1$ and $10$ initially infected nodes as seeds ($N_{seeds}=10$). \newline
The results are the following:

\begin{itemize}
    \item In the large $D$ and $\epsilon$ limit, the mean-field condition is fulfilled and the model resembles the compartmental  SIR model. By mapping the parameters as done in \ref{eq::d/r to gamma/beta}, we may compare $P(t)$ from the network model with the compartmental $R(t)$ by shifting it of $-d$ days.
    As a relevant reference for the comparison to the "small $D$" evolution, we've plotted the histogram of the daily cases which shows the typical peak in the early-exponential stages and, then, a decrease towards herd immunity, which for \cite{Thurner::NetBasedExpl} has been reached at the $98\%$ of the overall population. 
    \item By reducing the average degree to $D=3$, we've find a changing in the spreading behavior, since $P(t)$ increases almost linearly for a remarkable timespan. At the end of the infection, nearly $1/5$ of the entire population has been infected which is noticeably smaller than the SIR predicted herd immunity nearly at $80\%$.
\end{itemize}

With this rough perception of the main results by considering the underlying network structure, we've further analyzed the parameter dependence.

\section{Parameter Dependence and Phase Transition}
The aim of this section is to obtain the value of the "critical average degree" $D_c$, such that, for $D<D_c$, the infection curve is (roughly) linear. In order to take care of the two growth trends, we've defined an order parameter 
\begin{equation}
    \label{def::sdc}
    \mathbb{O}:=SD(C(t))
\end{equation}{}
as the standard deviation of the daily new cases, having removed the days without new infected. In this way,     $\mathbb{O}!=0$ signals the presence of a nonlinear increase of the daily new cases and, therefore, of its cumulative sum $P(t)$. Other way around, $\mathbb{O}=0$ corresponds to a linear growth of $P(t)$.


\chapter{Appendix}

\section{Notes on Network book of A.Barabási}

\subsection{Compartmental Models 10.2}
\subsubsection{SI model}
At $(10.1)$ $\beta$ is the probability of 1 spread only. So, $\frac{\beta \, \langle k \rangle S(t)}{N}$ expresses the probability of 1 spread over $\langle k\rangle S(t)/N$ neighbors. \\
$C:=\ln{\frac{i_0}{1-i_0}}$

\subsubsection{SIS model}

In $(10.7)$, define $v_{SIS}:=1-\frac{\mu}{\beta*\langle k\rangle }=1-R_0^{-1}$ the "characteristic velocity". If $v>0$ spread; otherwise it dies out. So, $\tau^{SIS}=\frac{1}{\beta\langle k\rangle v_{SIS}}=\frac{\tau^{SI}}{v_{SIS}}$ In, SI model $v_{SIS}=1$ \cite{barabasi::2016networkbook}
\newline
The endemic fraction of infected is given by $\frac{\beta-\mu}{\beta}$ slides 16 of the LDE course.

\subsection{Epidemics on Nets 10.3}

\subsubsection{SI Model}
$10.16:$ $$\tau_n^{SI}=\frac{\tau_{c}^{SI}}{\frac{\langle k^2 \rangle -\langle k\rangle}{\langle k\rangle^2}}$$
\newline
$10.17:$ is obtained by imposing the initial condition $i(t=0)=i_0$. Anyway, $i_k=i_0+i_0f(t)$

\subsubsection{SIS}:

$$\tau_N^{SIS}=\frac{\tilde{k}^2}{\beta\langle k\rangle-\mu\tilde{k}^2}$$ 
\newline
where $\tilde{k}^2:=\frac{\langle k\rangle^2}{\langle k^2 \rangle }$ and ???? if the network is homogeneous
$\langle k\rangle^2=\langle k^2 \rangle .$
Check better what's the behavior of $\langle k^2 \rangle $ with respect to the considered network.

\subsection{10.B}

If a nets lacks degree of correlation, i.e. $e_{ij}=q_iq_j$, the probability $p_{kk'}$ is independent on k. Thus, $$p_{kk'}=\frac{k'p_k'}{\langle k\rangle}.$$

Typo under $(10.50)$ multiply by $kp_k$ not $k-1)p_k$

\chapter{Results}


\chapter{Conclusion}





\begin{figure}[ht]
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\linewidth, height=8cm]{WS_Pruned_AdjMat_N1000_D14.0_p0.0.png}
        \centering
        \caption{Network information for the regular "D-halving" graph}
        \label{fig:adjmat_pruning_p0.0}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth, height=8cm]{WS_Pruned_SIR_p0_9}
        \caption{(pseudo) linearity of the total number of infected for the "halving-D" method with $p = 0$}
        \label{fig:pruning_p0.0}
    \end{subfigure}
\end{figure}

\newpage
\begin{figure}[ht]
	\begin{subfigure}{0.4\textwidth}
		\includegraphics[width=\textwidth, height=9cm]{WS_Pruned_AdjMat_N1000_D6.0_p0.1.png}
		\centering
		\caption{Network information for the E-R model with $p = 0.1$}
		\label{fig:adjmat_pruning_p0.1}
	\end{subfigure}
	\begin{subfigure}{0.4\textwidth}
		\includegraphics[width=\textwidth, height=8cm]{WS_Pruned_SIR_p0.1_4_2.png}
		\caption{(pseudo) linearity of the total number of infected for the E-R model with $p = 0.1$}
		\label{fig:pruning_p0.1}
	\end{subfigure}
\end{figure}

\newpage
\begin{figure}[ht]
	\begin{subfigure}{0.4\textwidth}
		\includegraphics[width=\textwidth, height=9cm]{NNR_Conf_Model_AdjMat_N1000_D6_p0.0.png}
		\centering
		\caption{Network information for the "Poissonian" small-world network with $p = 0$}
		\label{fig:adjmat_NNR_Conf_Model}
	\end{subfigure}
	\begin{subfigure}{0.4\textwidth}
		\includegraphics[width=\textwidth, height=8cm]{NNR_R0_1-2_p0.0_27.png}
		\caption{(pseudo) linearity for the nearest neighbors rewiring network with $p = 0$}
		\label{fig:NNR_Conf_Model}
	\end{subfigure}
\end{figure}

\newpage
\begin{figure}[ht]
	\begin{subfigure}{0.4\textwidth}
		\includegraphics[width=\textwidth, height=9cm]{Caveman_Model_AdjMat_N1000_D5.0_p0.1.png}
		\centering
		\caption{Network information for the Caveman network model with $p = 0.1$}
		\label{fig:adjmat_Caveman_Model}
	\end{subfigure}
	\begin{subfigure}{0.4\textwidth}
		\includegraphics[width=\textwidth, height=8cm]{CavemanMod_R0_1-3_p0.1_18.png}
		\caption{"(pseudo) linearity" for the Caveman network model with $p = 0.1$}
		\label{fig:Caveman_Model}
	\end{subfigure}
\end{figure}

\newpage
\begin{figure}[ht]
	\begin{subfigure}{0.4\textwidth}
		\includegraphics[width=\textwidth, height=9cm]{B-A_Model_AdjMat_N1000_D4.0_p0.0_m2_N0_2.png}
		\centering
		\caption{Network information for the B-A graph with $p = 0$}
		\label{fig:adjmat_B-A_Model}
	\end{subfigure}
	\begin{subfigure}{0.4\textwidth}
		\includegraphics[width=\textwidth, height=8cm]{B-A_Mod_R0_0-1_p0_6.png}
		\caption{"(pseudo) linearity" for the B-A graph with $p = 0$}
		\label{fig:Barabási_AlbertSIR}
	\end{subfigure}
\end{figure}